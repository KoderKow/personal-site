---
title: Fitting Tree Models
author: Kyle Harris
date: '2019-09-13'
slug: fitting-tree-models
categories:
  - R
  - machine learning
tags:
  - Trees
description: ~
featured_image: ~
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2

---


<div id="TOC">
<ul>
<li><a href="#eda">EDA</a><ul>
<li><a href="#data-cleaning">Data Cleaning</a></li>
<li><a href="#plot-em">Plot Emâ€™!</a></li>
</ul></li>
<li><a href="#modeling">Modeling</a><ul>
<li><a href="#split-the-data">Split the Data</a></li>
<li><a href="#classification-tree">Classification Tree</a></li>
<li><a href="#bagging-random-forests">Bagging &amp; Random Forests</a></li>
<li><a href="#boosted-trees">Boosted Trees</a></li>
</ul></li>
</ul>
</div>

<pre class="r"><code>library(knitr)
library(kableExtra)
library(xgboost)
library(tidyverse)
library(rlang)
library(skimr)
library(ggthemes)
library(kowr)
library(countrycode)
library(rpart)
library(rpart.plot)
library(caret)
library(ModelMetrics)
library(randomForest)
library(ranger)
library(tictoc)</code></pre>
<p>The adult data is obtained from the <a href="https://archive.ics.uci.edu/ml/index.php">UCI Machine Learning Repository</a>. On the site they have the data split into multiple pieces:</p>
<ul>
<li>data: the training data</li>
<li>test: the test data</li>
<li>names: column names</li>
</ul>
<p>We can read the data directly from their website. For readability the URLs will be held within variables.</p>
<pre class="r"><code>data_url &lt;- &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data&quot;
names_url &lt;- &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names&quot;
test_url &lt;- &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test&quot;</code></pre>
<p><hc>read_csv()</hc> will allow us to read in the data from the site, however, we know the data has no column names. Thus, we shall read in the column names first. The data from <hc>names_url</hc> is not in a format for easy R interpritation. After some exploration on the website it looks like a .txt file, so we can use readlines to look at the data and find what we are looking for.</p>
<pre class="r"><code>names_file &lt;- read_lines(names_url)
names_file</code></pre>
<pre class="scroll"><code>##   [1] &quot;| This data was extracted from the census bureau database found at&quot;                                                                                                                                                                                                                                                                                                                                                                             
##   [2] &quot;| http://www.census.gov/ftp/pub/DES/www/welcome.html&quot;                                                                                                                                                                                                                                                                                                                                                                                           
##   [3] &quot;| Donor: Ronny Kohavi and Barry Becker,&quot;                                                                                                                                                                                                                                                                                                                                                                                                        
##   [4] &quot;|        Data Mining and Visualization&quot;                                                                                                                                                                                                                                                                                                                                                                                                         
##   [5] &quot;|        Silicon Graphics.&quot;                                                                                                                                                                                                                                                                                                                                                                                                                     
##   [6] &quot;|        e-mail: ronnyk@sgi.com for questions.&quot;                                                                                                                                                                                                                                                                                                                                                                                                 
##   [7] &quot;| Split into train-test using MLC++ GenCVFiles (2/3, 1/3 random).&quot;                                                                                                                                                                                                                                                                                                                                                                              
##   [8] &quot;| 48842 instances, mix of continuous and discrete    (train=32561, test=16281)&quot;                                                                                                                                                                                                                                                                                                                                                                 
##   [9] &quot;| 45222 if instances with unknown values are removed (train=30162, test=15060)&quot;                                                                                                                                                                                                                                                                                                                                                                 
##  [10] &quot;| Duplicate or conflicting instances : 6&quot;                                                                                                                                                                                                                                                                                                                                                                                                       
##  [11] &quot;| Class probabilities for adult.all file&quot;                                                                                                                                                                                                                                                                                                                                                                                                       
##  [12] &quot;| Probability for the label &#39;&gt;50K&#39;  : 23.93% / 24.78% (without unknowns)&quot;                                                                                                                                                                                                                                                                                                                                                                       
##  [13] &quot;| Probability for the label &#39;&lt;=50K&#39; : 76.07% / 75.22% (without unknowns)&quot;                                                                                                                                                                                                                                                                                                                                                                       
##  [14] &quot;|&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [15] &quot;| Extraction was done by Barry Becker from the 1994 Census database.  A set of&quot;                                                                                                                                                                                                                                                                                                                                                                 
##  [16] &quot;|   reasonably clean records was extracted using the following conditions:&quot;                                                                                                                                                                                                                                                                                                                                                                     
##  [17] &quot;|   ((AAGE&gt;16) &amp;&amp; (AGI&gt;100) &amp;&amp; (AFNLWGT&gt;1)&amp;&amp; (HRSWK&gt;0))&quot;                                                                                                                                                                                                                                                                                                                                                                                        
##  [18] &quot;|&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [19] &quot;| Prediction task is to determine whether a person makes over 50K&quot;                                                                                                                                                                                                                                                                                                                                                                              
##  [20] &quot;| a year.&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                      
##  [21] &quot;|&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [22] &quot;| First cited in:&quot;                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [23] &quot;| @inproceedings{kohavi-nbtree,&quot;                                                                                                                                                                                                                                                                                                                                                                                                                
##  [24] &quot;|    author={Ron Kohavi},&quot;                                                                                                                                                                                                                                                                                                                                                                                                                      
##  [25] &quot;|    title={Scaling Up the Accuracy of Naive-Bayes Classifiers: a&quot;                                                                                                                                                                                                                                                                                                                                                                              
##  [26] &quot;|           Decision-Tree Hybrid},&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [27] &quot;|    booktitle={Proceedings of the Second International Conference on&quot;                                                                                                                                                                                                                                                                                                                                                                          
##  [28] &quot;|               Knowledge Discovery and Data Mining},&quot;                                                                                                                                                                                                                                                                                                                                                                                          
##  [29] &quot;|    year = 1996,&quot;                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [30] &quot;|    pages={to appear}}&quot;                                                                                                                                                                                                                                                                                                                                                                                                                        
##  [31] &quot;|&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [32] &quot;| Error Accuracy reported as follows, after removal of unknowns from&quot;                                                                                                                                                                                                                                                                                                                                                                           
##  [33] &quot;|    train/test sets):&quot;                                                                                                                                                                                                                                                                                                                                                                                                                         
##  [34] &quot;|    C4.5       : 84.46+-0.30&quot;                                                                                                                                                                                                                                                                                                                                                                                                                  
##  [35] &quot;|    Naive-Bayes: 83.88+-0.30&quot;                                                                                                                                                                                                                                                                                                                                                                                                                  
##  [36] &quot;|    NBTree     : 85.90+-0.28&quot;                                                                                                                                                                                                                                                                                                                                                                                                                  
##  [37] &quot;|&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [38] &quot;|&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [39] &quot;| Following algorithms were later run with the following error rates,&quot;                                                                                                                                                                                                                                                                                                                                                                          
##  [40] &quot;|    all after removal of unknowns and using the original train/test split.&quot;                                                                                                                                                                                                                                                                                                                                                                    
##  [41] &quot;|    All these numbers are straight runs using MLC++ with default values.&quot;                                                                                                                                                                                                                                                                                                                                                                      
##  [42] &quot;|&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [43] &quot;|    Algorithm               Error&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [44] &quot;| -- ----------------        -----&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [45] &quot;| 1  C4.5                    15.54&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [46] &quot;| 2  C4.5-auto               14.46&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [47] &quot;| 3  C4.5 rules              14.94&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [48] &quot;| 4  Voted ID3 (0.6)         15.64&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [49] &quot;| 5  Voted ID3 (0.8)         16.47&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [50] &quot;| 6  T2                      16.84&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [51] &quot;| 7  1R                      19.54&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [52] &quot;| 8  NBTree                  14.10&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [53] &quot;| 9  CN2                     16.00&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [54] &quot;| 10 HOODG                   14.82&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [55] &quot;| 11 FSS Naive Bayes         14.05&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [56] &quot;| 12 IDTM (Decision table)   14.46&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [57] &quot;| 13 Naive-Bayes             16.12&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [58] &quot;| 14 Nearest-neighbor (1)    21.42&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [59] &quot;| 15 Nearest-neighbor (3)    20.35&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [60] &quot;| 16 OC1                     15.04&quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [61] &quot;| 17 Pebls                   Crashed.  Unknown why (bounds WERE increased)&quot;                                                                                                                                                                                                                                                                                                                                                                     
##  [62] &quot;|&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [63] &quot;| Conversion of original data as follows:&quot;                                                                                                                                                                                                                                                                                                                                                                                                      
##  [64] &quot;| 1. Discretized agrossincome into two ranges with threshold 50,000.&quot;                                                                                                                                                                                                                                                                                                                                                                           
##  [65] &quot;| 2. Convert U.S. to US to avoid periods.&quot;                                                                                                                                                                                                                                                                                                                                                                                                      
##  [66] &quot;| 3. Convert Unknown to \&quot;?\&quot;&quot;                                                                                                                                                                                                                                                                                                                                                                                                                  
##  [67] &quot;| 4. Run MLC++ GenCVFiles to generate data,test.&quot;                                                                                                                                                                                                                                                                                                                                                                                               
##  [68] &quot;|&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [69] &quot;| Description of fnlwgt (final weight)&quot;                                                                                                                                                                                                                                                                                                                                                                                                         
##  [70] &quot;|&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [71] &quot;| The weights on the CPS files are controlled to independent estimates of the&quot;                                                                                                                                                                                                                                                                                                                                                                  
##  [72] &quot;| civilian noninstitutional population of the US.  These are prepared monthly&quot;                                                                                                                                                                                                                                                                                                                                                                  
##  [73] &quot;| for us by Population Division here at the Census Bureau.  We use 3 sets of&quot;                                                                                                                                                                                                                                                                                                                                                                   
##  [74] &quot;| controls.&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                    
##  [75] &quot;|  These are:&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                  
##  [76] &quot;|          1.  A single cell estimate of the population 16+ for each state.&quot;                                                                                                                                                                                                                                                                                                                                                                    
##  [77] &quot;|          2.  Controls for Hispanic Origin by age and sex.&quot;                                                                                                                                                                                                                                                                                                                                                                                    
##  [78] &quot;|          3.  Controls by Race, age and sex.&quot;                                                                                                                                                                                                                                                                                                                                                                                                  
##  [79] &quot;|&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [80] &quot;| We use all three sets of controls in our weighting program and \&quot;rake\&quot; through&quot;                                                                                                                                                                                                                                                                                                                                                              
##  [81] &quot;| them 6 times so that by the end we come back to all the controls we used.&quot;                                                                                                                                                                                                                                                                                                                                                                    
##  [82] &quot;|&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [83] &quot;| The term estimate refers to population totals derived from CPS by creating&quot;                                                                                                                                                                                                                                                                                                                                                                   
##  [84] &quot;| \&quot;weighted tallies\&quot; of any specified socio-economic characteristics of the&quot;                                                                                                                                                                                                                                                                                                                                                                  
##  [85] &quot;| population.&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                  
##  [86] &quot;|&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [87] &quot;| People with similar demographic characteristics should have&quot;                                                                                                                                                                                                                                                                                                                                                                                  
##  [88] &quot;| similar weights.  There is one important caveat to remember&quot;                                                                                                                                                                                                                                                                                                                                                                                  
##  [89] &quot;| about this statement.  That is that since the CPS sample is&quot;                                                                                                                                                                                                                                                                                                                                                                                  
##  [90] &quot;| actually a collection of 51 state samples, each with its own&quot;                                                                                                                                                                                                                                                                                                                                                                                 
##  [91] &quot;| probability of selection, the statement only applies within&quot;                                                                                                                                                                                                                                                                                                                                                                                  
##  [92] &quot;| state.&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                       
##  [93] &quot;&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                               
##  [94] &quot;&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                               
##  [95] &quot;&gt;50K, &lt;=50K.&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                   
##  [96] &quot;&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                               
##  [97] &quot;age: continuous.&quot;                                                                                                                                                                                                                                                                                                                                                                                                                               
##  [98] &quot;workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.&quot;                                                                                                                                                                                                                                                                                                                              
##  [99] &quot;fnlwgt: continuous.&quot;                                                                                                                                                                                                                                                                                                                                                                                                                            
## [100] &quot;education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.&quot;                                                                                                                                                                                                                                                                              
## [101] &quot;education-num: continuous.&quot;                                                                                                                                                                                                                                                                                                                                                                                                                     
## [102] &quot;marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.&quot;                                                                                                                                                                                                                                                                                                                     
## [103] &quot;occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.&quot;                                                                                                                                                                                                          
## [104] &quot;relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.&quot;                                                                                                                                                                                                                                                                                                                                                              
## [105] &quot;race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.&quot;                                                                                                                                                                                                                                                                                                                                                                             
## [106] &quot;sex: Female, Male.&quot;                                                                                                                                                                                                                                                                                                                                                                                                                             
## [107] &quot;capital-gain: continuous.&quot;                                                                                                                                                                                                                                                                                                                                                                                                                      
## [108] &quot;capital-loss: continuous.&quot;                                                                                                                                                                                                                                                                                                                                                                                                                      
## [109] &quot;hours-per-week: continuous.&quot;                                                                                                                                                                                                                                                                                                                                                                                                                    
## [110] &quot;native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&amp;Tobago, Peru, Hong, Holand-Netherlands.&quot;</code></pre>
<p>Lucky for us the column names are at the end right after an empty string. <hc>names_file</hc> is a character vector so we can use <hc>which()</hc> to seek the pattern we want. Each name/description combination is seperated by a â€œ:â€, we can turn the end of this file in a small data dictionary with two columns; <hc>column_name</hc> and <hc>description</hc>. We can use <hc>read_delim()</hc> to read in the file. We can use the parameters to name the wanted columns, how many lines to skip, and specify what seperates the columns.</p>
<pre class="r"><code>skip_to_here &lt;- which(names_file == &quot;&quot;) %&gt;%
  max()

skip_to_here</code></pre>
<pre><code>## [1] 96</code></pre>
<pre class="r"><code>data_dictionary &lt;- read_delim(
  file = names_url,
  col_names = c(&quot;column_name&quot;, &quot;description&quot;),
  skip = skip_to_here,
  delim = &quot;:&quot;
)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   column_name = col_character(),
##   description = col_character()
## )</code></pre>
<pre class="r"><code>data_dictionary</code></pre>
<pre><code>## # A tibble: 14 x 2
##    column_name    description                                              
##    &lt;chr&gt;          &lt;chr&gt;                                                    
##  1 age            &quot; continuous.&quot;                                           
##  2 workclass      &quot; Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, â€¦
##  3 fnlwgt         &quot; continuous.&quot;                                           
##  4 education      &quot; Bachelors, Some-college, 11th, HS-grad, Prof-school, Aâ€¦
##  5 education-num  &quot; continuous.&quot;                                           
##  6 marital-status &quot; Married-civ-spouse, Divorced, Never-married, Separatedâ€¦
##  7 occupation     &quot; Tech-support, Craft-repair, Other-service, Sales, Execâ€¦
##  8 relationship   &quot; Wife, Own-child, Husband, Not-in-family, Other-relativâ€¦
##  9 race           &quot; White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, â€¦
## 10 sex            &quot; Female, Male.&quot;                                         
## 11 capital-gain   &quot; continuous.&quot;                                           
## 12 capital-loss   &quot; continuous.&quot;                                           
## 13 hours-per-week &quot; continuous.&quot;                                           
## 14 native-country &quot; United-States, Cambodia, England, Puerto-Rico, Canada,â€¦</code></pre>
<p>From information seen on the website about our data, we know there should be 15 rows. Exploring the data in the browser we know the category <em>income</em> is the last column. With that information we will create an additional row to the data dictionary so we have all the columns we need.</p>
<pre class="r"><code>data_dictionary[nrow(data_dictionary) + 1, ] &lt;- c(&quot;income&quot;, &quot;&gt;50K, &lt;=50K&quot;)

data_dictionary</code></pre>
<pre><code>## # A tibble: 15 x 2
##    column_name    description                                              
##  * &lt;chr&gt;          &lt;chr&gt;                                                    
##  1 age            &quot; continuous.&quot;                                           
##  2 workclass      &quot; Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, â€¦
##  3 fnlwgt         &quot; continuous.&quot;                                           
##  4 education      &quot; Bachelors, Some-college, 11th, HS-grad, Prof-school, Aâ€¦
##  5 education-num  &quot; continuous.&quot;                                           
##  6 marital-status &quot; Married-civ-spouse, Divorced, Never-married, Separatedâ€¦
##  7 occupation     &quot; Tech-support, Craft-repair, Other-service, Sales, Execâ€¦
##  8 relationship   &quot; Wife, Own-child, Husband, Not-in-family, Other-relativâ€¦
##  9 race           &quot; White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, â€¦
## 10 sex            &quot; Female, Male.&quot;                                         
## 11 capital-gain   &quot; continuous.&quot;                                           
## 12 capital-loss   &quot; continuous.&quot;                                           
## 13 hours-per-week &quot; continuous.&quot;                                           
## 14 native-country &quot; United-States, Cambodia, England, Puerto-Rico, Canada,â€¦
## 15 income         &gt;50K, &lt;=50K</code></pre>
<p>I am a fan of snake case, so we will replace the â€œ-â€ with &quot;_&quot;. Then we will use the data dictionaryâ€™s <em>column_name</em> feature to label the test/train data we will be reading in. For data exploration and cleaning I will be combining the test and training sets. To ensure an easy split of data when it comes to modeling I will add a <em>group</em> feature so we can later split the full data set with ease.</p>
<pre class="r"><code>data_dictionary &lt;- data_dictionary %&gt;% 
  mutate(column_name = str_replace_all(column_name, &quot;-&quot;, &quot;_&quot;))

train &lt;- read_csv(
  file = data_url,
  col_names = data_dictionary$column_name
  ) %&gt;% 
  mutate(group = &quot;train&quot;)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   age = col_double(),
##   workclass = col_character(),
##   fnlwgt = col_double(),
##   education = col_character(),
##   education_num = col_double(),
##   marital_status = col_character(),
##   occupation = col_character(),
##   relationship = col_character(),
##   race = col_character(),
##   sex = col_character(),
##   capital_gain = col_double(),
##   capital_loss = col_double(),
##   hours_per_week = col_double(),
##   native_country = col_character(),
##   income = col_character()
## )</code></pre>
<p>When looking at the data online we see the first line of the test data has a line stating â€œ1x3 Cross validatorâ€. This will cause an error so we will skip the first line and then combine the two datasets.</p>
<pre class="r"><code>test &lt;- read_csv(
  file = &quot;/Users/kow/Downloads/adult.test&quot;,
  skip = 1,
  col_names = data_dictionary$column_name
) %&gt;% 
  mutate(group = &quot;test&quot;)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   age = col_double(),
##   workclass = col_character(),
##   fnlwgt = col_double(),
##   education = col_character(),
##   education_num = col_double(),
##   marital_status = col_character(),
##   occupation = col_character(),
##   relationship = col_character(),
##   race = col_character(),
##   sex = col_character(),
##   capital_gain = col_double(),
##   capital_loss = col_double(),
##   hours_per_week = col_double(),
##   native_country = col_character(),
##   income = col_character()
## )</code></pre>
<pre class="r"><code>all_data &lt;- bind_rows(train, test)</code></pre>
<div id="eda" class="section level1">
<h1>EDA</h1>
<p><hc>skim()</hc> from the <strong>skimr</strong> package will give us the view <hc>summary()</hc> or <strong>dplyr</strong>â€™s <hc>glimpse()</hc> does, but with additional information.</p>
<pre class="r"><code>skim(all_data)</code></pre>
<pre><code>## Skim summary statistics
##  n obs: 48842 
##  n variables: 16 
## 
## â”€â”€ Variable type:character â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
##        variable missing complete     n min max empty n_unique
##       education       0    48842 48842   3  12     0       16
##           group       0    48842 48842   4   5     0        2
##          income       0    48842 48842   4   6     0        4
##  marital_status       0    48842 48842   7  21     0        7
##  native_country       0    48842 48842   1  26     0       42
##      occupation       0    48842 48842   1  17     0       15
##            race       0    48842 48842   5  18     0        5
##    relationship       0    48842 48842   4  14     0        6
##             sex       0    48842 48842   4   6     0        2
##       workclass       0    48842 48842   1  16     0        9
## 
## â”€â”€ Variable type:numeric â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
##        variable missing complete     n      mean        sd    p0      p25
##             age       0    48842 48842     38.64     13.71    17     28  
##    capital_gain       0    48842 48842   1079.07   7452.02     0      0  
##    capital_loss       0    48842 48842     87.5     403        0      0  
##   education_num       0    48842 48842     10.08      2.57     1      9  
##          fnlwgt       0    48842 48842 189664.13 105604.03 12285 117550.5
##  hours_per_week       0    48842 48842     40.42     12.39     1     40  
##       p50    p75    p100     hist
##      37       48      90 â–‡â–‡â–‡â–†â–ƒâ–‚â–â–
##       0        0   99999 â–‡â–â–â–â–â–â–â–
##       0        0    4356 â–‡â–â–â–â–â–â–â–
##      10       12      16 â–â–â–â–â–‡â–â–ƒâ–
##  178144.5 237642 1490400 â–‡â–…â–â–â–â–â–â–
##      40       45      99 â–â–â–â–‡â–â–â–â–</code></pre>
<div id="data-cleaning" class="section level2">
<h2>Data Cleaning</h2>
<p>I will now go through each variable and collapse character features where available. Hopefully this will increase our accuracy due to possible under-representation of groups in this data. The flow will be counting the data and grouping the factors to the best of my knowledge. Grouping will be with assumptions I make in my nogginâ€™.</p>
<div id="remove-variables" class="section level3">
<h3>Remove Variables</h3>
<p>I am unsure what <em>fnlwgt</em> is exactly and <em>education_num</em> is a numeric version of <em>education</em>.</p>
<pre class="r"><code>all_data &lt;- all_data %&gt;% 
  select(-fnlwgt, -education_num)</code></pre>
</div>
<div id="education" class="section level3">
<h3>Education</h3>
<p>â€œAn ordered factor with levels Preschool &lt; 1st-4th &lt; 5th-6th &lt; 7th-8th &lt; 9th &lt; 10th &lt; 11th &lt; 12th &lt; HS-grad &lt; Prof-school &lt; Assoc-acdm &lt; Assoc-voc &lt; Some-college &lt; Bachelors &lt; Masters &lt; Doctorate.â€</p>
<pre class="r"><code>all_data %&gt;% 
  count(education, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 16 x 2
##    education        n
##    &lt;chr&gt;        &lt;int&gt;
##  1 HS-grad      15784
##  2 Some-college 10878
##  3 Bachelors     8025
##  4 Masters       2657
##  5 Assoc-voc     2061
##  6 11th          1812
##  7 Assoc-acdm    1601
##  8 10th          1389
##  9 7th-8th        955
## 10 Prof-school    834
## 11 9th            756
## 12 12th           657
## 13 Doctorate      594
## 14 5th-6th        509
## 15 1st-4th        247
## 16 Preschool       83</code></pre>
<pre class="r"><code>all_data &lt;- all_data %&gt;% 
  mutate(
    education = fct_collapse(
      education,
      &quot;No Diploma&quot; = c(&quot;Preschool&quot;, &quot;1st-4th&quot;, &quot;5th-6th&quot;, &quot;7th-8th&quot;, paste0(9:12, &quot;th&quot;)),
      &quot;High School&quot; = &quot;HS-grad&quot;,
      &quot;Professional School&quot; = &quot;Prof-school&quot;,
      &quot;Associates&quot; = c(&quot;Assoc-acdm&quot;, &quot;Assoc-voc&quot;), 
      &quot;Some College&quot; = &quot;Some-college&quot;,
      &quot;Bachelors&quot; = &quot;Bachelors&quot;,
      &quot;Graduate&quot; = c(&quot;Doctorate&quot;, &quot;Masters&quot;)
    )
  )

all_data %&gt;% 
  count(education, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 7 x 2
##   education               n
##   &lt;fct&gt;               &lt;int&gt;
## 1 High School         15784
## 2 Some College        10878
## 3 Bachelors            8025
## 4 No Diploma           6408
## 5 Associates           3662
## 6 Graduate             3251
## 7 Professional School   834</code></pre>
<hr />
</div>
<div id="marital-status" class="section level3">
<h3>Marital Status</h3>
<pre class="r"><code>all_data %&gt;% 
  count(marital_status, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 7 x 2
##   marital_status            n
##   &lt;chr&gt;                 &lt;int&gt;
## 1 Married-civ-spouse    22379
## 2 Never-married         16117
## 3 Divorced               6633
## 4 Separated              1530
## 5 Widowed                1518
## 6 Married-spouse-absent   628
## 7 Married-AF-spouse        37</code></pre>
<pre class="r"><code>all_data &lt;- all_data %&gt;% 
  mutate(
    marital_status = fct_collapse(
      marital_status,
      &quot;Not Married&quot; = c(&quot;Divorced&quot;, &quot;Separated&quot;, &quot;Widowed&quot;),
      &quot;Never Married&quot; = &quot;Never-married&quot;,
      &quot;Married&quot; = c(&quot;Married-civ-spouse&quot;, &quot;Married-spouse-absent&quot;, &quot;Married-AF-spouse&quot;)
    )
  )

all_data %&gt;% 
  count(marital_status, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 3 x 2
##   marital_status     n
##   &lt;fct&gt;          &lt;int&gt;
## 1 Married        23044
## 2 Never Married  16117
## 3 Not Married     9681</code></pre>
<hr />
</div>
<div id="income" class="section level3">
<h3>Income</h3>
<pre class="r"><code>all_data %&gt;% 
  count(income, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 4 x 2
##   income     n
##   &lt;chr&gt;  &lt;int&gt;
## 1 &lt;=50K  24720
## 2 &lt;=50K. 12435
## 3 &gt;50K    7841
## 4 &gt;50K.   3846</code></pre>
<pre class="r"><code>all_data &lt;- all_data %&gt;% 
  mutate(income = str_remove_all(income, fixed(&quot;.&quot;)))

all_data %&gt;% 
  count(income, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 2 x 2
##   income     n
##   &lt;chr&gt;  &lt;int&gt;
## 1 &lt;=50K  37155
## 2 &gt;50K   11687</code></pre>
<hr />
</div>
<div id="native-country" class="section level3">
<h3>Native Country</h3>
<pre class="r"><code>all_data %&gt;% 
  count(native_country, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 42 x 2
##    native_country     n
##    &lt;chr&gt;          &lt;int&gt;
##  1 United-States  43832
##  2 Mexico           951
##  3 ?                857
##  4 Philippines      295
##  5 Germany          206
##  6 Puerto-Rico      184
##  7 Canada           182
##  8 El-Salvador      155
##  9 India            151
## 10 Cuba             138
## # â€¦ with 32 more rows</code></pre>
<pre class="r"><code>country_table &lt;- all_data %&gt;% 
  count(native_country, sort = TRUE) %&gt;% 
  mutate(
    code = countrycode::countrycode(native_country,
                                    origin = &quot;country.name&quot;,
                                    destination = &quot;continent&quot;)
  )</code></pre>
<pre><code>## Warning in countrycode::countrycode(native_country, origin = &quot;country.name&quot;, : Some values were not matched unambiguously: ?, Columbia, England, Hong, Scotland, South, Yugoslavia</code></pre>
<pre class="r"><code>country_table</code></pre>
<pre><code>## # A tibble: 42 x 3
##    native_country     n code    
##    &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;   
##  1 United-States  43832 Americas
##  2 Mexico           951 Americas
##  3 ?                857 &lt;NA&gt;    
##  4 Philippines      295 Asia    
##  5 Germany          206 Europe  
##  6 Puerto-Rico      184 Americas
##  7 Canada           182 Americas
##  8 El-Salvador      155 Americas
##  9 India            151 Asia    
## 10 Cuba             138 Americas
## # â€¦ with 32 more rows</code></pre>
<pre class="r"><code>country_table %&gt;% 
  mutate(
    code2 = case_when(
      native_country == &quot;?&quot; ~ &quot;Unknown&quot;,
      native_country == &quot;South&quot; ~ &quot;Unknown&quot;,
      native_country == &quot;England&quot; ~ &quot;Europe&quot;,
      native_country == &quot;Columbia&quot; ~ &quot;Americas&quot;,
      native_country == &quot;Hong&quot; ~ &quot;Asia&quot;,
      native_country == &quot;Yugoslavia&quot; ~ &quot;Europe&quot;,
      native_country == &quot;Scotland&quot; ~ &quot;Europe&quot;,
      TRUE ~ code
    )
  )</code></pre>
<pre><code>## # A tibble: 42 x 4
##    native_country     n code     code2   
##    &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;   
##  1 United-States  43832 Americas Americas
##  2 Mexico           951 Americas Americas
##  3 ?                857 &lt;NA&gt;     Unknown 
##  4 Philippines      295 Asia     Asia    
##  5 Germany          206 Europe   Europe  
##  6 Puerto-Rico      184 Americas Americas
##  7 Canada           182 Americas Americas
##  8 El-Salvador      155 Americas Americas
##  9 India            151 Asia     Asia    
## 10 Cuba             138 Americas Americas
## # â€¦ with 32 more rows</code></pre>
<pre class="r"><code>all_data &lt;- all_data %&gt;% 
  mutate(
    code = countrycode(
      native_country,
      origin = &quot;country.name&quot;,
      destination = &quot;continent&quot;
      ),
    native_continent = case_when(
      native_country == &quot;?&quot; ~ &quot;Unknown&quot;,
      native_country == &quot;South&quot; ~ &quot;Unknown&quot;,
      native_country == &quot;England&quot; ~ &quot;Europe&quot;,
      native_country == &quot;Columbia&quot; ~ &quot;Americas&quot;,
      native_country == &quot;Hong&quot; ~ &quot;Asia&quot;,
      native_country == &quot;Yugoslavia&quot; ~ &quot;Europe&quot;,
      native_country == &quot;Scotland&quot; ~ &quot;Europe&quot;,
      TRUE ~ code
    )
  ) %&gt;% 
  select(-native_country, -code)</code></pre>
<pre><code>## Warning in countrycode(native_country, origin = &quot;country.name&quot;, destination = &quot;continent&quot;): Some values were not matched unambiguously: ?, Columbia, England, Hong, Scotland, South, Yugoslavia</code></pre>
<pre class="r"><code>all_data %&gt;% 
  count(native_continent, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 5 x 2
##   native_continent     n
##   &lt;chr&gt;            &lt;int&gt;
## 1 Americas         46086
## 2 Asia               981
## 3 Unknown            972
## 4 Europe             780
## 5 Oceania             23</code></pre>
<hr />
</div>
<div id="occupation" class="section level3">
<h3>Occupation</h3>
<pre class="r"><code>all_data %&gt;% 
  count(occupation, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 15 x 2
##    occupation            n
##    &lt;chr&gt;             &lt;int&gt;
##  1 Prof-specialty     6172
##  2 Craft-repair       6112
##  3 Exec-managerial    6086
##  4 Adm-clerical       5611
##  5 Sales              5504
##  6 Other-service      4923
##  7 Machine-op-inspct  3022
##  8 ?                  2809
##  9 Transport-moving   2355
## 10 Handlers-cleaners  2072
## 11 Farming-fishing    1490
## 12 Tech-support       1446
## 13 Protective-serv     983
## 14 Priv-house-serv     242
## 15 Armed-Forces         15</code></pre>
<pre class="r"><code>all_data &lt;- all_data %&gt;% 
  mutate(occupation = ifelse(occupation == &quot;?&quot;, &quot;Unknown&quot;, occupation))</code></pre>
<hr />
</div>
<div id="race" class="section level3">
<h3>Race</h3>
<pre class="r"><code>all_data %&gt;% 
  count(race, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 5 x 2
##   race                   n
##   &lt;chr&gt;              &lt;int&gt;
## 1 White              41762
## 2 Black               4685
## 3 Asian-Pac-Islander  1519
## 4 Amer-Indian-Eskimo   470
## 5 Other                406</code></pre>
<hr />
</div>
<div id="relationship" class="section level3">
<h3>Relationship</h3>
<pre class="r"><code>all_data %&gt;% 
  count(relationship, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 6 x 2
##   relationship       n
##   &lt;chr&gt;          &lt;int&gt;
## 1 Husband        19716
## 2 Not-in-family  12583
## 3 Own-child       7581
## 4 Unmarried       5125
## 5 Wife            2331
## 6 Other-relative  1506</code></pre>
<hr />
</div>
<div id="sex" class="section level3">
<h3>Sex</h3>
<pre class="r"><code>all_data %&gt;% 
  count(sex, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 2 x 2
##   sex        n
##   &lt;chr&gt;  &lt;int&gt;
## 1 Male   32650
## 2 Female 16192</code></pre>
<hr />
</div>
<div id="workclass" class="section level3">
<h3>Workclass</h3>
<pre class="r"><code>all_data %&gt;% 
  count(workclass, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 9 x 2
##   workclass            n
##   &lt;chr&gt;            &lt;int&gt;
## 1 Private          33906
## 2 Self-emp-not-inc  3862
## 3 Local-gov         3136
## 4 ?                 2799
## 5 State-gov         1981
## 6 Self-emp-inc      1695
## 7 Federal-gov       1432
## 8 Without-pay         21
## 9 Never-worked        10</code></pre>
<pre class="r"><code>all_data %&gt;% 
  count(workclass, sort = TRUE) %&gt;% 
  mutate(
    workclass2 = fct_collapse(
      workclass,
      &quot;Private&quot; = &quot;Private&quot;,
      &quot;Government&quot; = c(&quot;Local-gov&quot;, &quot;State-gov&quot;, &quot;Federal-gov&quot;),
      &quot;Self-Employed&quot; = c(&quot;Self-emp-not-inc&quot;, &quot;Self-emp-inc&quot;),
      &quot;Unknown&quot; = &quot;?&quot;,
      &quot;No Income&quot; = c(&quot;Without-pay&quot;, &quot;Never-worked&quot;)
    )
  )</code></pre>
<pre><code>## # A tibble: 9 x 3
##   workclass            n workclass2   
##   &lt;chr&gt;            &lt;int&gt; &lt;fct&gt;        
## 1 Private          33906 Private      
## 2 Self-emp-not-inc  3862 Self-Employed
## 3 Local-gov         3136 Government   
## 4 ?                 2799 Unknown      
## 5 State-gov         1981 Government   
## 6 Self-emp-inc      1695 Self-Employed
## 7 Federal-gov       1432 Government   
## 8 Without-pay         21 No Income    
## 9 Never-worked        10 No Income</code></pre>
<pre class="r"><code>all_data &lt;- all_data %&gt;% 
  mutate(
    workclass = fct_collapse(
      workclass,
      &quot;Private&quot; = &quot;Private&quot;,
      &quot;Government&quot; = c(&quot;Local-gov&quot;, &quot;State-gov&quot;, &quot;Federal-gov&quot;),
      &quot;Self-Employed&quot; = c(&quot;Self-emp-not-inc&quot;, &quot;Self-emp-inc&quot;),
      &quot;Unknown&quot; = &quot;?&quot;,
      &quot;No Income&quot; = c(&quot;Without-pay&quot;, &quot;Never-worked&quot;)
    )
  )</code></pre>
<hr />
</div>
</div>
<div id="plot-em" class="section level2">
<h2>Plot Emâ€™!</h2>
<p>I made a <em>mapping</em> function to use with <em>purrr</em>â€™s <hc>map()</hc> that will automate the graphing of all variables against our <span class="math inline">\(y\)</span> variable, in this case <span class="math inline">\(y = income\)</span>. I will not go over this in detail, if there is any questions or you would like to discuss this or any future code contact me on social media (found on the homepage).</p>
<pre class="r"><code>plot_em &lt;- as_mapper(~ {
  main_var &lt;- parse_expr(.y)
  if (is.numeric(.x)) {
    p &lt;- ggplot(all_data, aes(x = income, y = !!main_var)) +
      geom_boxplot() +
      labs(x = .y)
    
    p %&gt;% snake_to()
    
  } else {
    bar_data &lt;- all_data %&gt;% 
      count(income, !!main_var, name = &quot;count&quot;) %&gt;% 
      group_by(income) %&gt;% 
      mutate(proportion = count / sum(count)) %&gt;% 
      ungroup() %&gt;% 
      arrange(desc(proportion)) %&gt;% 
      mutate(
        !!main_var := factor(!!main_var, levels = unique(!!main_var)),
        income = factor(income, levels = unique(income))
        )
    
    p &lt;- ggplot(
      data = bar_data,
      mapping = aes(x = income, y = proportion, fill = !!main_var)
      ) +
      geom_col() +
      labs(x = .y)
    
    p %&gt;% snake_to()
  }
})

all_plots &lt;- map2(all_data, names(all_data), plot_em)</code></pre>
<p><hc>all_plots</hc> holds all plots in a <em>list</em>. We will go through one by one so we can have a deeper understanding of our data. We could use a graphing function such as <hc>pairs()</hc> or <hc>ggpairs()</hc>, however, I really wanted to look at all variables one by one closely and I got to practice some dplyr programming ;)</p>
<pre class="r"><code>all_plots[[1]]</code></pre>
<p><img src="/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>On average there are more people that have an income of <em>&gt;50k</em> who are older.</p>
<pre class="r"><code>all_plots[[2]]</code></pre>
<p><img src="/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>Higher Proportion per Outcome (<span class="math inline">\(y = income\)</span>):</p>
<ul>
<li>Private: <em>&lt;=50k</em></li>
<li>Government: <em>&gt;50k</em></li>
<li>Self-Employed: <em>&gt;50k</em></li>
<li>Unknown: <em>&lt;=50k</em></li>
</ul>
<pre class="r"><code>all_plots[[3]]</code></pre>
<p><img src="/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>Higher Proportion per Outcome (<span class="math inline">\(y = income\)</span>):</p>
<ul>
<li>High School: <em>&lt;=50k</em></li>
<li>Bachelors: <em>&gt;50k</em></li>
<li>Some College: <em>&lt;=50k</em></li>
<li>No Diploma: <em>&lt;=50k</em></li>
<li>Graduate: <em>&gt;50k</em></li>
<li>Associates: <em>About Even</em></li>
<li>Professional School: <em>&gt;50k</em></li>
</ul>
<pre class="r"><code>all_plots[[4]]</code></pre>
<p><img src="/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>Higher Proportion per Outcome (<span class="math inline">\(y = income\)</span>):</p>
<ul>
<li>Married: <em>&gt;50k</em></li>
<li>Never Married: <em>&lt;=50k</em></li>
<li>Not Married: <em>&lt;=50k</em></li>
</ul>
<pre class="r"><code>all_plots[[5]]</code></pre>
<p><img src="/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>There are a lot of different occupations so I will point out the obvious points.</p>
<p>Higher Proportion per Outcome (<span class="math inline">\(y = income\)</span>):</p>
<ul>
<li>Exec-managerial: <em>&gt;50k</em></li>
<li>Prof-specialty: <em>&gt;50k</em></li>
<li>Adm-clerical: <em>&lt;=50k</em></li>
<li>Sales: <em>&gt;50k</em></li>
<li>Machine-op-inspct: <em>&lt;=50k</em></li>
<li>Unknown: <em>&lt;=50k</em></li>
</ul>
<pre class="r"><code>all_plots[[6]]</code></pre>
<p><img src="/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>Higher Proportion per Outcome (<span class="math inline">\(y = income\)</span>):</p>
<ul>
<li>Husband: <em>&gt;50k</em></li>
<li>Not-in-family: <em>&lt;=50k</em></li>
<li>Own-child: <em>&lt;=50k</em></li>
<li>Unmarried: <em>&lt;=50k</em></li>
<li>Wife: <em>&gt;50k</em></li>
<li>Other-relative: <em>&lt;=50k</em></li>
</ul>
<pre class="r"><code>all_plots[[7]]</code></pre>
<p><img src="/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>Higher Proportion per Outcome (<span class="math inline">\(y = income\)</span>):</p>
<ul>
<li>White: <em>&gt;50k</em></li>
<li>Black: <em>&lt;=50k</em></li>
<li>Asian-Pac-Islander: <em>&gt;50k</em></li>
<li>Amer-Indian-Eskimo: <em>&lt;=50k</em></li>
<li>Other: <em>&lt;=50k</em></li>
</ul>
<pre class="r"><code>all_plots[[8]]</code></pre>
<p><img src="/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p>Higher Proportion per Outcome (<span class="math inline">\(y = income\)</span>):</p>
<ul>
<li>Male: <em>&gt;50k</em></li>
<li>Female: <em>&lt;=50k</em></li>
</ul>
<pre class="r"><code>all_plots[[9]]</code></pre>
<p><img src="/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<p>No comment.</p>
<pre class="r"><code>all_plots[[10]]</code></pre>
<p><img src="/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<p>Also no comment.</p>
<pre class="r"><code>all_plots[[11]]</code></pre>
<p><img src="/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<p>We can see the average between line is around 40 for both <em>&lt;=50k</em> and <em>&gt;50k</em>. It appears on average more people in the <em>&gt;50k</em> group work more hours.</p>
<pre class="r"><code>all_plots[[14]]</code></pre>
<p><img src="/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<p>Mostly equal, Americas has a high N so it is hard to see a signal.</p>
</div>
</div>
<div id="modeling" class="section level1">
<h1>Modeling</h1>
<div id="split-the-data" class="section level2">
<h2>Split the Data</h2>
<pre class="r"><code>train &lt;- all_data %&gt;% 
  filter(group == &quot;train&quot;) %&gt;% 
  select(-group)

test &lt;- all_data %&gt;% 
  filter(group == &quot;test&quot;) %&gt;% 
  select(-group)</code></pre>
</div>
<div id="classification-tree" class="section level2">
<h2>Classification Tree</h2>
<p>A classification tree uses a decision tree to take features (columns) of our data and comes up with a final value, in our case this the final value is a label. Decision trees can also handle numerical outputs. Generally this method is reffered to as CART, <strong>C</strong>lassifican <strong>A</strong>nd <strong>R</strong>egression <strong>T</strong>rees.</p>
<p>A decision tree has a hierarchrical structure:</p>
<ul>
<li>Top: Root Node. This is where all of the data from our features begin their journey</li>
<li>Bottom: Leaf Nodes. This is the finish line of the tree. Once the data reaches this point we obtain the final value. Either a numeric <span class="math inline">\(y\)</span> value or a label for <span class="math inline">\(y\)</span>.</li>
<li>Middle: Internal Nodes. These are the nodes between the root and leaf nodes.</li>
</ul>
<p>Advantages of Decision Trees:</p>
<ul>
<li>Easy to interprit
<ul>
<li>If you can read a flow chart you can read a decision tree</li>
</ul></li>
<li>Training and prediction flows are easy to explain</li>
<li>Easier to explain than a linear model</li>
<li>Following the path of the tree allows for full explanation of the data</li>
<li>Easy to interprit and visualize</li>
<li>Handles categorical and numerical data with ease
<ul>
<li>No dummy data for categorical data</li>
<li>No need to normalize or transform numeric data</li>
</ul></li>
<li>Missing data? No problem!
<ul>
<li>One method to handle missing data is when going down the branch and the value is NA for that feature it will randomly choose left or right and continue onward</li>
<li>Another method involves going down both branches at the split with missing data and when the leaf nodes are reached you average the leafs values for the final prediction</li>
</ul></li>
<li>Robust to outliers</li>
<li>Requires little data prep</li>
<li>Can model non-linearity in the data</li>
<li>Trains quickly on large data sets</li>
</ul>
<p>Disadvantages of Decision Trees:</p>
<ul>
<li>Large trees can be hard to interpret</li>
<li>Trees can have high variance
<ul>
<li>Causes model performance to be poor</li>
</ul></li>
<li>Trees overfit easily</li>
</ul>
<div id="model-selection" class="section level3">
<h3>Model Selection</h3>
<p>We will use grid search for hyperperameter searching. Hyper perameters are the different knobs and settings we can tune to get the best results from our data. Grid search is an exhaustive and iterative search through a manually defined set of model hyperperameters.</p>
<p>The two parameters we will be iterating over are <hc>minsplit</hc> and <hc>maxdepth</hc>.</p>
<ul>
<li><hc>minsplit</hc>: Minumum number of data points requeired to attempt a split</li>
<li><hc>maxdepth</hc>: Maximum depth of our tree</li>
</ul>
<p>The goal of the grid search is to train model per row of the grid and then evaluate which model is best. The best model, in this instance, is accuracy of correct predictions. Below we will use some <em>tidy</em> methods for training the models. This brilliant flow is thanks to the post located <a href="https://drsimonj.svbtle.com/grid-search-in-the-tidyverse">here</a>!</p>
<pre class="r"><code>grid_search &lt;- list(
  minsplit = seq(10, 45, 15),
  maxdepth = c(1, 5, 10, 25)
) %&gt;% 
  cross_df()

grid_search</code></pre>
<pre><code>## # A tibble: 12 x 2
##    minsplit maxdepth
##       &lt;dbl&gt;    &lt;dbl&gt;
##  1       10        1
##  2       25        1
##  3       40        1
##  4       10        5
##  5       25        5
##  6       40        5
##  7       10       10
##  8       25       10
##  9       40       10
## 10       10       25
## 11       25       25
## 12       40       25</code></pre>
<p>Now that we have our grid we can train a model per row. We expect there to be 12 models at the end. We can define our own function to simply pass the grid to the <hc>control</hc> parameter of <hc>rpart()</hc> using <hc>â€¦</hc>. We will use mutate to add a model per row to the <hc>grid_search</hc> data frame,</p>
<pre class="r"><code>dt_mod &lt;- function(...) {
  rpart(
    formula = income ~ .,
    data = train,
    control = rpart.control(...)
    )
}

grid_search &lt;- grid_search %&gt;% 
  mutate(fit = pmap(grid_search, dt_mod))

grid_search</code></pre>
<pre><code>## # A tibble: 12 x 3
##    minsplit maxdepth fit    
##       &lt;dbl&gt;    &lt;dbl&gt; &lt;list&gt; 
##  1       10        1 &lt;rpart&gt;
##  2       25        1 &lt;rpart&gt;
##  3       40        1 &lt;rpart&gt;
##  4       10        5 &lt;rpart&gt;
##  5       25        5 &lt;rpart&gt;
##  6       40        5 &lt;rpart&gt;
##  7       10       10 &lt;rpart&gt;
##  8       25       10 &lt;rpart&gt;
##  9       40       10 &lt;rpart&gt;
## 10       10       25 &lt;rpart&gt;
## 11       25       25 &lt;rpart&gt;
## 12       40       25 &lt;rpart&gt;</code></pre>
<p>The <hc>fit</hc> column now holds all 12 of our models. If we print the model column we can see the normal output of an <em>rpart</em> model for all 12 models.</p>
<pre class="r"><code>grid_search$fit</code></pre>
<pre class="scroll"><code>## [[1]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 32561 7841 &lt;=50K (0.7591904 0.2408096) *
## 
## [[2]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 32561 7841 &lt;=50K (0.7591904 0.2408096) *
## 
## [[3]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 32561 7841 &lt;=50K (0.7591904 0.2408096) *
## 
## [[4]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&lt; 7073.5 17482  872 &lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&gt;=7073.5 318   12 &gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&lt; 5095.5 9807 2944 &lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&gt;=5095.5 522   10 &gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &gt;50K (0.27639892 0.72360108) *
## 
## [[5]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&lt; 7073.5 17482  872 &lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&gt;=7073.5 318   12 &gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&lt; 5095.5 9807 2944 &lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&gt;=5095.5 522   10 &gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &gt;50K (0.27639892 0.72360108) *
## 
## [[6]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&lt; 7073.5 17482  872 &lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&gt;=7073.5 318   12 &gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&lt; 5095.5 9807 2944 &lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&gt;=5095.5 522   10 &gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &gt;50K (0.27639892 0.72360108) *
## 
## [[7]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&lt; 7073.5 17482  872 &lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&gt;=7073.5 318   12 &gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&lt; 5095.5 9807 2944 &lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&gt;=5095.5 522   10 &gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &gt;50K (0.27639892 0.72360108) *
## 
## [[8]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&lt; 7073.5 17482  872 &lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&gt;=7073.5 318   12 &gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&lt; 5095.5 9807 2944 &lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&gt;=5095.5 522   10 &gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &gt;50K (0.27639892 0.72360108) *
## 
## [[9]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&lt; 7073.5 17482  872 &lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&gt;=7073.5 318   12 &gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&lt; 5095.5 9807 2944 &lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&gt;=5095.5 522   10 &gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &gt;50K (0.27639892 0.72360108) *
## 
## [[10]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&lt; 7073.5 17482  872 &lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&gt;=7073.5 318   12 &gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&lt; 5095.5 9807 2944 &lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&gt;=5095.5 522   10 &gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &gt;50K (0.27639892 0.72360108) *
## 
## [[11]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&lt; 7073.5 17482  872 &lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&gt;=7073.5 318   12 &gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&lt; 5095.5 9807 2944 &lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&gt;=5095.5 522   10 &gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &gt;50K (0.27639892 0.72360108) *
## 
## [[12]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&lt; 7073.5 17482  872 &lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&gt;=7073.5 318   12 &gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&lt; 5095.5 9807 2944 &lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&gt;=5095.5 522   10 &gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &gt;50K (0.27639892 0.72360108) *</code></pre>
</div>
<div id="model-evaluation-and-tuning" class="section level3">
<h3>Model Evaluation and Tuning</h3>
<p>Following the flow from the post linked above we can easily make make predictions on the test data and check the accuracy. We will seperate the independant variables and the dependent variable from the test data to compute the accuracy per model. We will add the accuracy per model to the <hc>grid_search</hc> data using <hc>mutate</hc> and arrange to show the best and most simple model. Simplicity is always preferred!</p>
<pre class="r"><code>compute_accuracy &lt;- function(fit, test_features, test_labels) {
  predicted &lt;- predict(
    fit,
    test_features,
    type = &quot;class&quot;
    )
  
  mean(predicted == test_labels)
}

test_features &lt;- test %&gt;%
  select(-income)
test_labels   &lt;- test %&gt;% 
  pull(income)

grid_search &lt;- grid_search %&gt;%
  mutate(
    test_accuracy = map_dbl(
      fit,
      compute_accuracy,
      test_features,
      test_labels)
    ) %&gt;%
  arrange(desc(test_accuracy), minsplit, maxdepth)
  
grid_search</code></pre>
<pre><code>## # A tibble: 12 x 4
##    minsplit maxdepth fit     test_accuracy
##       &lt;dbl&gt;    &lt;dbl&gt; &lt;list&gt;          &lt;dbl&gt;
##  1       10        5 &lt;rpart&gt;         0.845
##  2       10       10 &lt;rpart&gt;         0.845
##  3       10       25 &lt;rpart&gt;         0.845
##  4       25        5 &lt;rpart&gt;         0.845
##  5       25       10 &lt;rpart&gt;         0.845
##  6       25       25 &lt;rpart&gt;         0.845
##  7       40        5 &lt;rpart&gt;         0.845
##  8       40       10 &lt;rpart&gt;         0.845
##  9       40       25 &lt;rpart&gt;         0.845
## 10       10        1 &lt;rpart&gt;         0.764
## 11       25        1 &lt;rpart&gt;         0.764
## 12       40        1 &lt;rpart&gt;         0.764</code></pre>
<p>We can see the best and most simplistic model has a <hc>minsplit</hc> of 10 and a <hc>maxdepth</hc> of 5. A lot of these models appear to split and look at the data in the same way due to all of them having the same accuracy. 84% is a pretty great result for such a simple model! But is it as good as it seems? We will revisit this number at the end of this section. Now we can use the <strong>rpart.plot</strong> library to produce a nice visualize of the model.</p>
<pre class="r"><code>best_rpart &lt;- grid_search %&gt;% 
  pull(fit) %&gt;% 
  .[[1]]

rpart.plot(
  x = best_rpart,
  yesno = 2,
  type = 0,
  extra = 0
  )</code></pre>
<p><img src="/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>Using this flow we can take a person and their information and make a prediction on how much income they make. This is the power of a tree, we can interpreit the model and how it works every step of the way with ease!</p>
<p>There are a few more things we can do to our model. <strong>Pruning</strong> can possibly reduce the size of the tree (simple is ideal!) without losing and predictive power on classifications. We can do this by looking at the <hc>cp</hc>, aka <em>complexity parameter</em>.</p>
<p>CP is a penalty term that helps control tree size. The smaller the CP the more complex a tree will be. The <hc>rpart()</hc> function computes the 10-fold cross validation error of the model over various values for CP and stores the results in a table inside the model. We can plot the cross validation error across different values of CP using <hc>plotcp()</hc>.</p>
<pre class="r"><code>plotcp(best_rpart)</code></pre>
<p><img src="/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<p>Here we can quickly get an estimate for the omptimal value of CP. To retreieve the optimal CP value we can look at the CP table stored in the model and look where the <em>xerror</em> is minimized.</p>
<pre class="r"><code>best_rpart$cptable</code></pre>
<pre><code>##           CP nsplit rel error    xerror        xstd
## 1 0.12638694      0 1.0000000 1.0000000 0.009839876
## 2 0.06402245      2 0.7472261 0.7472261 0.008840225
## 3 0.03749522      3 0.6832037 0.6832037 0.008532119
## 4 0.01000000      4 0.6457085 0.6457085 0.008339388</code></pre>
<pre class="r"><code>opt_index &lt;- which.min(best_rpart$cptable[, &quot;xerror&quot;])
cp_opt &lt;- best_rpart$cptable[opt_index, &quot;CP&quot;]
cp_opt</code></pre>
<pre><code>## [1] 0.01</code></pre>
<p>With this value we can use <hc>prune()</hc> to possibly trim our model. <hc>prune()</hc> will return the optimized model.</p>
<pre class="r"><code>best_rpart_opt &lt;- prune(
  tree = best_rpart,
  cp = cp_opt
)</code></pre>
<p>Did this change anything?</p>
<pre class="r"><code>rpart.plot(
  x = best_rpart_opt,
  yesno = 2,
  type = 0,
  extra = 0
  )</code></pre>
<p><img src="/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<p>It appears in this instance, pruning did not change the visual of our model. Now onto the last part of of the evaluation is to look at a confusion matrix. A confusion matrix will show us a more detailed breakdown of correct and incorrect classifications for each class. We will use the <strong>caret</strong> package to produce the matrix.</p>
<pre class="r"><code>class_prediction &lt;- predict(
  object = best_rpart_opt,
  newdata = test,
  type = &quot;class&quot;
)

caret::confusionMatrix(
  data = class_prediction,
  reference = as.factor(test$income)
)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction &lt;=50K  &gt;50K
##      &lt;=50K 11805  1901
##      &gt;50K    630  1945
##                                           
##                Accuracy : 0.8445          
##                  95% CI : (0.8389, 0.8501)
##     No Information Rate : 0.7638          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.5137          
##                                           
##  Mcnemar&#39;s Test P-Value : &lt; 2.2e-16       
##                                           
##             Sensitivity : 0.9493          
##             Specificity : 0.5057          
##          Pos Pred Value : 0.8613          
##          Neg Pred Value : 0.7553          
##              Prevalence : 0.7638          
##          Detection Rate : 0.7251          
##    Detection Prevalence : 0.8418          
##       Balanced Accuracy : 0.7275          
##                                           
##        &#39;Positive&#39; Class : &lt;=50K           
## </code></pre>
<p>There is a lot of output to this function. We will focus on the 2x2 table, accuracy, sensitivity, and specificity.</p>
<ul>
<li><strong>Accuracy</strong>: Correct Prediction %</li>
<li><strong>Sensitivity</strong>: True Positive Rate
<ul>
<li>As noted on the last line of the output, the â€œpositiveâ€ factor is <em>&lt;=50k</em>. So the decision tree get 95% of the predictions correct when they are positive. Sounds great, right?</li>
</ul></li>
<li><strong>Specificity</strong>: True Negative Rate
<ul>
<li>The decision tree only has a 50% accuracy when it comes to incomes <em>&gt;50k</em>. What gives? Lets look at the count in each group. It appears the tree model labels a strong majority of predictions as <em>&lt;=50k</em>. Since most of the data belongs to this group and three likes to predict this label, it will have a high accuracy overall. This is why it is important to look at sensitivity and specificity. Our tree is not as great once we look at accuracy per label!</li>
</ul></li>
</ul>
</div>
<div id="other-accuracy-metrics-wip" class="section level3">
<h3>Other Accuracy Metrics [WIP]</h3>
<p>If accuracy is deceiving at times, what can we use? Great question person reading this. We can use AUCâ€¦</p>
</div>
</div>
<div id="bagging-random-forests" class="section level2">
<h2>Bagging &amp; Random Forests</h2>
<p>The drawbacks of decision trees is the high variability. A small change in the data, introduction to new data, or changing the test / training groups can change the model drastically. How can we approach these issues drawbacks? Enter bagging. :)</p>
<p>Bagged trees averages many trees to reduce the variance. Combing multiple models like this, in this instance it is multiple decision trees, is called an <strong>ensemble model</strong>. Another issue bagging helps with is overfitting.</p>
<p>Bagging is an ensemble method and the term <em>bagging</em> is shorthand for <strong>B</strong>ootstrap <strong>AGG</strong>regat<strong>ING</strong>. Bagging uses <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrap sampling</a> and aggregates the individual models by averaging. Bootstrap sampling means sampling rows at random from the training dataset with replacement. Bagging also starts with all available features of the data.</p>
<center>
<figure>
<img src="https://i.imgur.com/JYBMwak.png" />
<figcaption>
<a href="https://www.datacamp.com/courses/machine-learning-with-tree-based-models-in-r">Image Source</a>
</figcaption>
</figure>
</center>
<p>With replacement means there is a chance the bootstrap sample will have the same observation more than once. This can also lead some rows be absent. This allows us to have â€œnewâ€ data. By doing this we can fit many different, yet similar, models.</p>
<div id="bagging-steps" class="section level3">
<h3>Bagging Steps:</h3>
<p><strong>Step 1</strong>: Draw <span class="math inline">\(B\)</span> samples with replacement from the original training set, where B is a number less than or equal to the <span class="math inline">\(N\)</span> ($N = total training rows $). A common choice for <span class="math inline">\(B\)</span> is <span class="math inline">\(\frac{N}{2}\)</span>.</p>
<p><strong>Step 2</strong>: Train a decision tree on the newly created bootstrapped sample.</p>
<p><strong>Step 3</strong>: Repeat steps 1 and 2 multiple times. 10, 50, 100, 1000, etc. Typically, the more trees the better.</p>
</div>
<div id="predicting-with-bagging" class="section level3">
<h3>Predicting with Bagging</h3>
<p>If we have 1,000 bootstrapped trees that makes up our ensemble model, each bootstrap tree may have different terminal nodes compared to the other. To generate a prediction with this model, the model will make a prediction with all 1,000 trees and then average the predictions together to end up with the final prediciton. Due to bagging averaging the predictions, this will lower the variability and lead to a better performing model. I found a picture that repsents a decision trees prediction versus a bagging ensemble method. Hint: the decision tree is on the left ;)</p>
<p><a href="https://cdn.imgbin.com/17/0/7/imgbin-the-wisdom-of-crowds-wisdom-of-the-crowd-crowdsourcing-others-0Ehm17am7cWP7TMhUGkA7wXjv.jpg" class="uri">https://cdn.imgbin.com/17/0/7/imgbin-the-wisdom-of-crowds-wisdom-of-the-crowd-crowdsourcing-others-0Ehm17am7cWP7TMhUGkA7wXjv.jpg</a>
<img src="https://cdn.imgbin.com/17/0/7/imgbin-the-wisdom-of-crowds-wisdom-of-the-crowd-crowdsourcing-others-0Ehm17am7cWP7TMhUGkA7wXjv.jpg" /></p>
</div>
<div id="what-about-random-forests" class="section level3">
<h3>What about Random Forests?</h3>
<p>The only difference between bagging and random forest is that random forests will use a random subset of the datas features to build the models while bagging uses all features. This can lead to random forest removing a random feature and possibly finding an pattern that was not noticable while using all features. This parameter in functions to fit the models is called <hc>mtry</hc>. When <hc>mtry</hc> is equal to the count of independant variables then it is bagging. Anything less than that is random forest. We will fit an example model using the randomForest package for demonstration and when we need to use grid search for parameter selection we will use the <strong>ranger</strong> package (ranger is written in C# and is much faster when scaling up). The model will be fit using the default parameters, aside from mtry for bagging, and to go over the output of the model.</p>
<p>To demonstrate the time difference between fitting models between randomForest and ranger I will use the package <strong>tictoc</strong> to measure the amount of time it takes to run the code and compare them when we transfer to ranger. Note: <code>randomForest()</code> requires character columns to be of the factor data type. We will transfrom this into a different data set so we dont change our original data.</p>
<pre class="r"><code>bagging &lt;- ncol(train) - 1 # Do not include the Y variable
train_rf &lt;- train %&gt;% 
  mutate_if(is.character, as.factor)

tic()
rf_example &lt;- randomForest(
  formula = income ~ .,
  data = train_rf,
  mtry = bagging,
  ntree = 1000
)
toc()</code></pre>
<pre><code>## 94.728 sec elapsed</code></pre>
<pre class="r"><code>tic()
ranger_example &lt;- ranger(
  formula = income ~ .,
  data = train_rf,
  mtry = bagging,
  num.trees = 1000,
  verbose = FALSE
)
toc()</code></pre>
<pre><code>## 67.176 sec elapsed</code></pre>
<p>For 1,000 trees it saves us almost 1 minute! To explore random forests and bagging, the <code>randomForest()</code> function has nice built in functions for visualizing the model. When we want to find an optimized model we will grid search with <strong>ranger</strong>.</p>
<div id="randomforest-output" class="section level4">
<h4>randomForest Output</h4>
<pre class="r"><code>rf_example</code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = income ~ ., data = train_rf, mtry = bagging,      ntree = 1000) 
##                Type of random forest: classification
##                      Number of trees: 1000
## No. of variables tried at each split: 12
## 
##         OOB estimate of  error rate: 15.35%
## Confusion matrix:
##       &lt;=50K &gt;50K class.error
## &lt;=50K 22677 2043  0.08264563
## &gt;50K   2956 4885  0.37699273</code></pre>
<p>When we <em>print</em> the model we can see the original call to create the data, the type of random forest, tree count, number of variables per split, OOB estimate, and a confusion matrix. The OOB estimate takes the prediction for each tree. For example, we trained 1,000 trees. The OOB</p>
<p>We can see the error rate as the tree count increases using the <hc>plot()</hc> function.</p>
<pre class="r"><code>plot(rf_example)</code></pre>
<p><img src="/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-55-1.png" width="672" /></p>
<p>We can see that around 175 trees the error stops decreasing. This meaning we used too many trees, which translate to too much time spent waiting in terms of computation time. This means we can train a model with a much smaller amount of trees. This will be covered when we do a grid search, but it is nice to visual a model and see how many trees optimizes the error.</p>
</div>
</div>
<div id="random-forest-grid-search" class="section level3">
<h3>Random Forest Grid Search</h3>
<p>Just like the decision tree we will grid search for the best random forest model. The two parameters we will be tuning are <em>mtry</em>, total variables to use, and <em>num.tree</em>, the tree count. The choice for <em>mtry</em> is a standard way of checking when I was in college. Due to what we saw in the first random forest model, we shouldnt need anywhere near 1000 trees, but it doesnâ€™t hurt to check. Remember, more trees = more computationally expensive for us. By us I mean my computer :) So I will only check a few tree values. Idealy these models will be trained when there is idle time and/or on a computer that can handle training a lot of large models.</p>
<pre class="r"><code># bagging = total number of independent variables
rf_grid_search &lt;- list(
  mtry = unique(
    ceiling(
      c(
        bagging,
        sqrt(bagging),
        bagging / 2)
      )
    ),
  num.trees = c(100, 300, 500)
) %&gt;% 
  cross_df() %&gt;% 
  arrange(mtry, num.trees)

rf_grid_search</code></pre>
<pre><code>## # A tibble: 9 x 2
##    mtry num.trees
##   &lt;dbl&gt;     &lt;dbl&gt;
## 1     4       100
## 2     4       300
## 3     4       500
## 4     6       100
## 5     6       300
## 6     6       500
## 7    12       100
## 8    12       300
## 9    12       500</code></pre>
</div>
<div id="rf-model-fitting" class="section level3">
<h3>RF Model Fitting</h3>
<p>Again we will create a model function. This time we will use <strong>ranger</strong> to fit our RF models. The pmap handles passing in the parameter using â€¦! The other method we could go with is a for loop, however, this is shorter and elegant.</p>
<pre class="r"><code>rf_mod &lt;- function(...) {
  ranger(income ~ ., data = train, ...)
}

rf_grid_search &lt;- rf_grid_search %&gt;% 
  mutate(fit = pmap(rf_grid_search, rf_mod))</code></pre>
<pre><code>## Growing trees.. Progress: 97%. Estimated remaining time: 1 seconds.</code></pre>
<pre class="r"><code>rf_grid_search</code></pre>
<pre><code>## # A tibble: 9 x 3
##    mtry num.trees fit     
##   &lt;dbl&gt;     &lt;dbl&gt; &lt;list&gt;  
## 1     4       100 &lt;ranger&gt;
## 2     4       300 &lt;ranger&gt;
## 3     4       500 &lt;ranger&gt;
## 4     6       100 &lt;ranger&gt;
## 5     6       300 &lt;ranger&gt;
## 6     6       500 &lt;ranger&gt;
## 7    12       100 &lt;ranger&gt;
## 8    12       300 &lt;ranger&gt;
## 9    12       500 &lt;ranger&gt;</code></pre>
<p>We now have 9 ranger models. We will look at accuracy. To reiterate, in a real life setting it may be more applicable to use AUC. Below, notice how <hc>predicted</hc> needs to be set up. Before when we used predict it returned a vector. <strong>ranger</strong>â€™s predict function returns a list object with <strong>predictions</strong> being one of the list elements. We can use the <hc>pluck()</hc> function from <strong>purrr</strong> to grab the named element from the list.</p>
<pre class="r"><code>compute_accuracy &lt;- function(fit, test_features, test_labels) {
  predicted &lt;- predict(fit, test_features) %&gt;% 
    pluck(predictions)
  
  mean(predicted == test_labels)
}

test_features &lt;- test %&gt;% select(-income)
test_labels   &lt;- test$income

rf_grid_search &lt;- rf_grid_search %&gt;%
  mutate(
    test_accuracy = map_dbl(
      fit,
      compute_accuracy,
      test_features,
      test_labels
      )
    ) %&gt;%
  arrange(
    desc(test_accuracy),
    mtry, 
    num.trees
    )
  
rf_grid_search</code></pre>
<pre><code>## # A tibble: 9 x 4
##    mtry num.trees fit      test_accuracy
##   &lt;dbl&gt;     &lt;dbl&gt; &lt;list&gt;           &lt;dbl&gt;
## 1     4       300 &lt;ranger&gt;         0.862
## 2     4       100 &lt;ranger&gt;         0.861
## 3     4       500 &lt;ranger&gt;         0.861
## 4     6       300 &lt;ranger&gt;         0.855
## 5     6       500 &lt;ranger&gt;         0.854
## 6     6       100 &lt;ranger&gt;         0.854
## 7    12       100 &lt;ranger&gt;         0.846
## 8    12       500 &lt;ranger&gt;         0.846
## 9    12       300 &lt;ranger&gt;         0.845</code></pre>
</div>
<div id="best-model-evaluation" class="section level3">
<h3>Best Model Evaluation</h3>
<p>In terms of overall accuracy the best model has a <em>mtry</em> value of 4 and uses a tree count of 300. Much like what we did with the rpart model we can make a confusion matrix to check our best models performance.</p>
<pre class="r"><code>best_rf_mod &lt;- rf_grid_search %&gt;% 
  pull(fit) %&gt;% 
  .[[1]]

best_rf_mod</code></pre>
<pre><code>## Ranger result
## 
## Call:
##  ranger(income ~ ., data = train, ...) 
## 
## Type:                             Classification 
## Number of trees:                  300 
## Sample size:                      32561 
## Number of independent variables:  12 
## Mtry:                             4 
## Target node size:                 1 
## Variable importance mode:         none 
## Splitrule:                        gini 
## OOB prediction error:             13.89 %</code></pre>
<pre class="r"><code>features &lt;- test %&gt;%
  select(-income)
labels   &lt;- test %&gt;% 
  pull(income)

class_prediction &lt;- predict(
  best_rf_mod,
  test_features
) %&gt;% 
  pluck(predictions)

caret::confusionMatrix(
  data = class_prediction,
  reference = as.factor(labels)
)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction &lt;=50K  &gt;50K
##      &lt;=50K 11619  1423
##      &gt;50K    816  2423
##                                           
##                Accuracy : 0.8625          
##                  95% CI : (0.8571, 0.8677)
##     No Information Rate : 0.7638          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.5969          
##                                           
##  Mcnemar&#39;s Test P-Value : &lt; 2.2e-16       
##                                           
##             Sensitivity : 0.9344          
##             Specificity : 0.6300          
##          Pos Pred Value : 0.8909          
##          Neg Pred Value : 0.7481          
##              Prevalence : 0.7638          
##          Detection Rate : 0.7137          
##    Detection Prevalence : 0.8011          
##       Balanced Accuracy : 0.7822          
##                                           
##        &#39;Positive&#39; Class : &lt;=50K           
## </code></pre>
<p>Our previous simple model had:
- <strong>Accuracy</strong>: 84.45%
- <strong>Sensitivity</strong>: 94.93%
- <strong>Specificity</strong>: 50.57%</p>
<p>Our best random forest model does better in every category besides sensitivity, where it dropp by ~ 1%. The RF model was able to catch a lot more (13% more!) people who made more than $50,000! Fantastic! This is the power of the ensemble method of random forest, it will build multiple trees with different data and use different features to find different patterns that are used in the final outcome! I find that so awesome.</p>
<p>Now there is still one more tree method to go! This next method isnâ€™t too different from the RF methods. Onto boosted trees!</p>
</div>
</div>
<div id="boosted-trees" class="section level2">
<h2>Boosted Trees</h2>
<p>Work in progress..</p>
</div>
</div>
