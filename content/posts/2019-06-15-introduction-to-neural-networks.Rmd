---
title: Introduction to Neural Networks
author: Kyle Harris
date: '2019-06-15'
slug: introduction-to-neural-networks
categories:
  - R
  - machine learning
tags: []
description: ~
featured_image: ~
---

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">

# Follow Me On

<a href="https://twitter.com/KoderKow"> <i class='fab fa-twitter fa-3x'></i></a> <a href="https://www.instagram.com/koderkow/"> <i class='fab fa-instagram fa-3x'></i></a> <a href="https://github.com/KoderKow/"><i class='fab fa-github fa-3x'></i></a> <a href="https://koderkow.rbind.io"><i class='fas fa-globe fa-3x'></i></a> 

# Overview

- [What Are Neural Networks?]
- [Structure of a Neuron]
- [Visual Representation of Neural Networks]
- [Layers of a Neural Network]
- [Feedforward and Feedback Neural Networks]
- [Neural Network Application - The Faraway Way]
  - [Examine the Estimated Weights]
  - [Drawbacks of a Neural Network Model]
  - [Weight Interpretation]
  - [Improving the Fit]
  - [Demonstration Wrap-Up]
  - [Final Model Fit]
- [Conclusion]

# Goal

This post was first built as a Xaringan presentation for the final in the Contemporary Regression course at IUPUI (Indiana University Purdue University Indianapolis) which is part of the amazing Health Data Science program. I enjoyed making the presentation (link below) so much that I wanted to transfer the information I gathered into a blog post. I hope this serves as a launching point for those who have not had an opportunity to work with basic neural networks.
  
Slides available here: **http://bit.ly/intro-to-neural-networks**

# What Are Neural Networks?

A Neural network, or Artificial Neural Network, is a set of algorithms, modeled loosely after the human brain to help recognize patterns. The brain has about $1.5 \times 10^{10}$ neurons each with 10 to 104 connections called synapses. The speed of messages between neurons is about 100 m/sec which is much slower than CPU speed. The human brain's fastest reaction time is around 100 ms. A neuron computation time is 1â€“10 ms. Computation (to no surprise) is 10 times faster! That is just for one simple task! 

The original idea behind neural networks was to use a computer-based model of the human brain. We can recognize people in fractions of a second, but computers find this task difficult. So why not make the software more like the human brain? The brain model of connected neurons, first suggested by [McCulloch and Pitts (1943)](http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html), is too simplistic given more recent research.

As with artificial intelligence and the sentient takeover, the promise of neural networks is not matched by the reality of their performance. At least for now...

<center>
  <figure>
    <img class="lazyload" src="https://img.buzzfeed.com/buzzfeed-static/static/2015-04/1/17/enhanced/webdr07/anigif_enhanced-29933-1427925503-3.gif" width=40% />
    <figcaption><a href="https://www.buzzfeed.com/norbertobriceno/01101010101001001">Image Source</a></figcaption>
  </figure>
</center>

Neural networks have various purposes such as biological models, hardware implementation for adaptive control and many more! We are interested in the data analysis application of neural network; classification, clustering methods, regression methods.
      
# Structure of a Neuron

<center>
  <figure>
    <img class="lazyload" src="https://3c1703fe8d.site.internapcdn.net/newman/csz/news/800/2018/2-whyareneuron.jpg" width=80% />
    <figcaption><a href="https://medicalxpress.com/news/2018-07-neuron-axons-spindly-theyre-optimizing.html">Image Source</a></figcaption>
  </figure>
</center>

- *Dendrites* receive signals
- *Cell body* sums up the inputs of the signals to generate output
- *Axon terminals* is the final output

# Visual Representation of Neural Networks

<center>
  <figure>
    <img class="lazyload" src="https://cdn-images-1.medium.com/max/1600/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg" width=40% />
    <figcaption><a href="https://cdn-images-1.medium.com/max/1600/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg">Image Source</a></figcaption>
    </figure>
</center>

Here we can see how a neural network resembles a neuron. Neural networks are collections of thousands of these simple processing units that together perform useful computations.

- **Inputs $x_1, x_2, \dots, x_n$**: independent variables
- **Weights $w_1, w_2, \dots, w_n$:** learns the weights from the data
- **Bias $b$:** the intercept
- **Activation Function $f$:** defines the output of the neuron
  - *Identity Function:* linear fit
  - *Sigmoid Function:* logistic fit, where $y$ is binary
  - *ReLu (rectified linear fit):* linear fit, outputs 0 for negative values

# Layers of a Neural Network

<center>
  <img class="lazyload" src="https://i.stack.imgur.com/Kc50L.jpg" width = 40%/>
</center>

**Input Layer:** the raw data, think of each "node" as a variable in our data
**Hidden Layer:** this is where the "black magic" happens

- Each layer is focused on learning about the data
- We can think about each layer is learning about an aspect of the data
- Larger and more complex data may require multiple hidden layers

**Output Layer:** the final output. This is generally a single output of the input(s)

# Feedforward and Feedback Neural Networks

<figure>
    <img class="lazyload" src="https://cdn-images-1.medium.com/max/1600/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg" width=40% />
    <figcaption><a href="https://cdn-images-1.medium.com/max/1600/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg">Image Source</a></figcaption>
    </figure>

<div class="column-left">
<h3>Feedforward</h3>
- Signal goes from input to output
- No loops
<br><br>
<center>
<img class="lazyload" src="https://thumbs.gfycat.com/EnviousNiftyCorydorascatfish-size_restricted.gif" width="450px" height="250px"/>
</center>
</div>

<div class="column-right">
<h3>Feedback</h3>
- The neural network is recursive
- Data loops; goes in both directions
<br><br>
<center>
<img class="lazyload" src="https://thumbs.gfycat.com/MiniatureDependentCob-size_restricted.gif" width="450px" height="250px"/>
</center>
</div>
<br>

[Image Source](https://www.youtube.com/watch?v=aircAruvnKk)

# Neural Network Application - The Faraway Way

```{r, message=FALSE}
# libaries
library(nnet)
library(tidyverse)
library(ggthemes)
library(glue)
library(plotly)
library(kowr)
data(ozone, package = "faraway")
```

We will start with three variables from the [ozone data set from the faraway package](https://cran.r-project.org/web/packages/faraway/faraway.pdf) for demonstrative purposes. We fit a feed-forward neural network with one hidden layer containing two units with a linear output unit:

```{r, results="hidden", message=FALSE, error=FALSE}
set.seed(2019)
nnet_model <- nnet(
  formula = O3 ~ temp + ibh + ibt,
  data = ozone,
  size = 2,
  linout = TRUE,
  trace = FALSE
  )
```

- `nnet()` fits a single-hidden-layer neural network
- `formula = O3 ~ temp + ibh + ibt`: formula interface
- `data = ozone`: data where the formula variables reside
- `size = 2`: number of neurons in the hidden layer (this can be optimized)
- `linout = TRUE`: tells the model that it will have lienar output units
- `trace = FALSE`: hides the printed out optimization information

# Neural Network Application

If you repeat this, your result may differ slightly because of the random starting point of the algorithm, but you will likely get a similar result.

```{r, echo=FALSE}
rss_value <- round(nnet_model$value, 2)
glue("RSS Value: {rss_value}")
```

The RSS of 21099.4 is almost equal to $\sum_i(y_i - \hat{y})^2$, so the fit is not any better than the null model.

```{r}
sum((ozone$O3 - mean(ozone$O3))^2)
```

The problem lies with the initial selection of weights. It is hard to do this well when the variables have very different scales.

```{r}
scale_ozone <- scale(ozone)
```

Due to the random starting point, the algorithm uses it may not actually converge. We will fit the model 100 times and pick the one that has the lowest RSS. In theory, this will choose a random starting point that leads to the true minimum.

```{r, results="hide"}
set.seed(2019)

## fit 100 nn models
results <- 1:100 %>%
  map(~nnet(
    formula = O3 ~ temp + ibh + ibt,
    data = scale_ozone,
    size = 2,
    linout = TRUE,
    trace = FALSE))

## get the index of the model with the lowest RSS
best_model_index <- results %>%
  map_dbl(~.x$value) %>%
  which.min()

## select best model
best_nn <- results[[best_model_index]]
```

<center>
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center"}
p <- results %>%
  map_dbl(~.x$value) %>% 
  enframe(name = NULL) %>% 
  ggplot(aes(value)) +
    geom_histogram(fill = "red", color = "black", alpha = 0.7) +
    theme_gdocs() +
    labs(x = "RSS", y = "Count", title = "Distribution of RSS from 100 Model Fits")

ggplotly(p) %>% 
  clear_plotly_options()
```
</center>

```{r}
best_nn
```

```{r, echo=FALSE}
glue("Best RSS Value: {round(best_nn$value, 4)}")
```

Our `best_nn` model has 11 parameters or weights (The parameters are shown below). For each of the parameters, there is an optimization that occurs. The surface optimization problem has multiple peaks and valleys. The model can converge on one of these minimums. This is why we run our model 100 times to test out multiple random starting points for our model, to hopefully find the global minimum!

# Examine the Estimated Weights

```{r}
summary(best_nn)
```

<div class="column-left">
- $b$: bias
- $i_x$: input where $x$ is the index of the variable
- $h_y$: hidden neuron where $y$ is the index of the hidden neuron
- $o$: output
- $i_1 \rightarrow h_1$: refers to the link between input 1 and the first hidden neuron
- $b \rightarrow o$: is a one skip-layer connection, from the bias to the output
<br><br><br><br><br>
</div>

<div class="column-right">
<center>
<figure>
<img class="lazyload" src="https://memegenerator.net/img/instances/68631115/neural-networks-so-hot-right-now.jpg" width = 40%/>
<figcaption><a href="https://memegenerator.net/">Image Source</a></figcaption>
</figure>
</center>
<br><br><br>
</div>

# Drawbacks of a Neural Network Model

- Parameters are uninterpretable
- Not based on a probability model that expresses the structure and variation
  - No standard errors
  - Some inference is possible with bootstrapping
- We can get an $R^2$ estimate:
  ```{r}
1 - best_nn$value / sum((scale_ozone[, 1] - mean(scale_ozone[, 1]))^2)
```

This is similar to the additive model fit for these predictors that Faraway fits in previous chapters of his [book](https://www.amazon.com/Extending-Linear-Model-Generalized-Nonparametric/dp/149872096X).

# Weight Interpretation

```{r, echo=FALSE, fig.height=3, fig.align="center"}
par(mfrow=c(1,3))
ozmeans <- colMeans(ozone)
ozscales <- apply(ozone,2,sd)
xx <- expand.grid(temp=seq(-3,3,0.1),ibh=0,ibt=0)
plot(xx$temp*ozscales["temp"]+ozmeans["temp"], predict(best_nn,new=xx)*ozscales["O3"]+ozmeans["O3"], xlab="Temp", ylab="O3", type="l")
xx <- expand.grid(temp=0,ibh=seq(-3,3,0.1),ibt=0)
plot(xx$ibh*ozscales["ibh"]+ozmeans["ibh"], predict(best_nn,new=xx)*ozscales["O3"]+ozmeans["O3"], xlab="IBH", ylab="O3", type="l")
xx <- expand.grid(temp=0,ibh=0,ibt=seq(-3,3,0.1))
plot(xx$ibt*ozscales["ibt"]+ozmeans["ibt"], predict(best_nn,new=xx)*ozscales["O3"]+ozmeans["O3"],xlab="IBT",ylab="O3", type="l")
```

Although the neural network weights may be difficult to interpret, we can get some sense of the effect of the predictors by observing the marginal effect of changes in one or more predictor as other predictors are held fixed. Here, we vary each predictor individually while keeping the other predictors fixed at their mean values. Because the data has been centered and scaled for the neural network fitting, we need to restore the original scales. As seen in the plots there are large discontinuities in the lines plots. This does not follow the linear trend we are expecting. Looking back at the weights of `summary(best_nn)` we can see that some weights have extremely large values despite the scaling of the data, $i_2 \rightarrow h_2 = 63.10$. This means there is a lot of variability in this neuron. This is analogous to the collinearity problem in linear regression. The neural network is choosing these large values to optimize the fit.

# Improving the Fit

We can use a penalty function, as with smoothing splines, to obtain a more stable fit. Instead of minimizing MSE, we minimize: $MSE + \lambda \sum\limits_{i} w_i^2$. We can introduce a *weight decay* to our neural network, this is a similar approach we take with ridge regression. Lets set $\lambda = 0.001$ and create 100 models again.

```{r, eval=FALSE}
set.seed(2019)
## fit 100 nn models
results_decay <- 1:100 %>%
  map(~nnet(
    formula = O3 ~ temp + ibh + ibt,
    data = scale_ozone,
    size = 2,
    linout = TRUE,
    trace = FALSE,
    `decay = 0.001`))

## get the index of the model with the lowest RSS
best_decay_index <- results_decay %>%
  map_dbl(~.x$value) %>%
  which.min()

## select best model
best_decay <- results[[best_decay_index]]
```

```{r, echo=FALSE}
set.seed(2019)
## fit 100 nn models
results_decay <- 1:100 %>%
  map(~nnet(O3 ~ temp + ibh + ibt, scale_ozone, size = 2, linout = TRUE, trace = FALSE, decay = 0.001))
## get the index of the model with the lowest RSS
best_decay_index <- results_decay %>%
  map_dbl(~.x$value) %>%
  which.min()
## select best model
best_decay <- results[[best_decay_index]]
```

```{r, eval=FALSE}
best_decay$value
```

```{r, echo=FALSE}
round(best_decay$value, 4)
```

Our previous value was `r round(best_nn$value, 4)`, our new RSS is a little bit higher. This is expected because we are sacrificing some of the fit for a more stable result.

<div class="column-left">
```{r, echo=FALSE, fig.height=4, fig.align="center"}
par(mfrow=c(1,3))
ozmeans <- colMeans(ozone)
ozscales <- apply(ozone,2,sd)
xx <- expand.grid(temp=seq(-3,3,0.1),ibh=0,ibt=0)
plot(xx$temp*ozscales["temp"]+ozmeans["temp"], predict(best_decay,new=xx)*ozscales["O3"]+ozmeans["O3"], xlab="Temp", ylab="O3", type="l")
xx <- expand.grid(temp=0,ibh=seq(-3,3,0.1),ibt=0)
plot(xx$ibh*ozscales["ibh"]+ozmeans["ibh"], predict(best_decay,new=xx)*ozscales["O3"]+ozmeans["O3"], xlab="IBH", ylab="O3", type="l")
xx <- expand.grid(temp=0,ibh=0,ibt=seq(-3,3,0.1))
plot(xx$ibt*ozscales["ibt"]+ozmeans["ibt"], predict(best_decay,new=xx)*ozscales["O3"]+ozmeans["O3"],xlab="IBT",ylab="O3", type="l")
```
</div>

<div class="column-right">
<center>
  <figure>
    <img class="lazyload" src="https://media1.tenor.com/images/154e8427624e163c030970a795b6f169/tenor.gif?itemid=5143620" />
    <figcaption><a href="https://tenor.com/">Image Source</a></figcaption>
  </figure>
</center>
<br><br>
</div>

# Demonstration Wrap-Up

```{r}
summary(best_decay)
```

The weights of the second row are not as extreme now. There is not a way to assess the significance of any of the variables. Neural networks do have interactions built in and these can be observed by the method we used before by varying two variables in our model at a time.

# Final Model Fit

```{r}
set.seed(2019)

## fit 100 nn models
results <- 1:100 %>%
  map(~nnet(
    formula = O3 ~ .,
    data = scale_ozone,
    size = 4,
    linout = TRUE,
    trace = FALSE))

## get the index of the model with the lowest RSS
best_model_index <- results_decay %>%
  map_dbl(~.x$value) %>%
  which.min()

## select best model
best_model <- results[[best_model_index]]
best_model
```

```{r}
summary(best_model)
```

$R^2$ estimate:

```{r}
1 - best_model$value / sum((scale_ozone[,1] - mean(scale_ozone))^2)
```

# Conclusion

- Neural networks cannot be used for inference
- Flexible, Easy to fit large complex data
- Can be easily overfit
- Truly a "black box", plots only give a rough idea of what is happening with our data
- Lacks the diagnostics, model selection, and theory
- Initially developed to address real-life issues, not statistical issues
- "Neural networks can outperform their statistical competitors for some problems provided they are carefully used. However, one should not be fooled by the evocative name, as neural networks are just another tool in the box." (Faraway, 2016)

# Thanks!

#### Slides
- **Slides**: *http://bit.ly/intro-to-neural-networks*
- **Source Code**: *https://github.com/KoderKow/intro-to-neural-networks*
- **All Posts Source Code**: *https://github.com/KoderKow/personal-site/tree/master/content/posts*

#### References
- [McCulloch and Pitts (1943)](http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html)
- [DataCamp: Neural Network Models](https://www.datacamp.com/community/tutorials/neural-network-models-r)
- [Stack Overflow](https://stackoverflow.com/questions/35345191/what-is-a-layer-in-a-neural-network)
- [Faraway: Extending the Linear Model with R](https://www.amazon.com/Extending-Linear-Model-Generalized-Nonparametric/dp/149872096X)
- [R Package Used for Slides: Xaringan](https://github.com/yihui/xaringan)