<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Kyle Harris</title>
    <link>/posts/</link>
    <description>Recent content in Posts on Kyle Harris</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 04 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tesseract OCR Version 4.1.0 Parameters</title>
      <link>/2019/2019-12-04-tesseract-ocr-version-4-1-0-parameters/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/2019-12-04-tesseract-ocr-version-4-1-0-parameters/</guid>
      <description>Overview During a project my team was having a hard time tracking down an up to date parameters list for Tesseract OCR. The documentation mentions to use tesseract –print-parameters (two hyphens in front of print) to get the available 600+ options. Personally, I use R at my job so sifting through the terminal and trying to go through a poorly formatted (this may be due to my unfamiliarity to the terminal) list in the terminal can be a headache.</description>
    </item>
    
    <item>
      <title>Fitting Tree Models</title>
      <link>/2019/2019-09-13-fitting-tree-models/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/2019-09-13-fitting-tree-models/</guid>
      <description>EDA Data Cleaning Plot Em’!  Modeling Split the Data Classification Tree Bagging &amp;amp; Random Forests Boosted Trees    library(knitr) library(kableExtra) library(xgboost) library(tidyverse) library(rlang) library(skimr) library(ggthemes) library(kowr) library(countrycode) library(rpart) library(rpart.plot) library(caret) library(ModelMetrics) library(randomForest) library(ranger) library(tictoc) The adult data is obtained from the UCI Machine Learning Repository. On the site they have the data split into multiple pieces:
 data: the training data test: the test data names: column names  We can read the data directly from their website.</description>
    </item>
    
    <item>
      <title>Introduction to Neural Networks</title>
      <link>/2019/2019-06-15-introduction-to-neural-networks/</link>
      <pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/2019-06-15-introduction-to-neural-networks/</guid>
      <description>Follow Me On    
 Overview  What Are Neural Networks? Structure of a Neuron Visual Representation of Neural Networks Layers of a Neural Network Feedforward and Feedback Neural Networks Neural Network Application - The Faraway Way  Examine the Estimated Weights Drawbacks of a Neural Network Model Weight Interpretation Improving the Fit Demonstration Wrap-Up Final Model Fit  Conclusion   Goal This post was first built as a Xaringan presentation for the final in the Contemporary Regression course at IUPUI (Indiana University Purdue University Indianapolis) which is part of the amazing Health Data Science program.</description>
    </item>
    
    <item>
      <title>Indy Civic Food Security Hackathon</title>
      <link>/2018/2018-07-03-indy-civic-food-security-hackathon/</link>
      <pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/2018-07-03-indy-civic-food-security-hackathon/</guid>
      <description>Follow Me On    
 Indy Civic Food Security Hackathon This will be a short post (Hopefully a header image soon. Ha!). This is more of a reference to show what my friends and I from Health Data Science were able to build in 24 hours at the Indy Civic Food Security Hackathon! Link to their site is below.
Our App was made with Rshiny to help show nearby food options for those in need.</description>
    </item>
    
    <item>
      <title>Making the Transcribing Process a Bit Simpler</title>
      <link>/2018/2018-06-10-making-the-transcribing-process-a-bit-simpler/</link>
      <pubDate>Sun, 10 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/2018-06-10-making-the-transcribing-process-a-bit-simpler/</guid>
      <description>Follow Me On    
 Intro For the past year I have had the pleasure of working with the data visualization team at IUPUI. I approached Professor Reda looking to explore the realm of research. He gave me the opportunity to sit in their team meetings for the spring semester. As fall came around I joined the team.
For those who have worked in research, you may have crossed paths with user testing and the art of transcription.</description>
    </item>
    
    <item>
      <title>Accidental Drug Related Deaths</title>
      <link>/2018/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut/</link>
      <pubDate>Sun, 27 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut/</guid>
      <description>Follow Me On    
 Intro The opioid epidemic is a nationwide issue for the United States. Among the states Connecticut takes second place in terms of deaths from opioids in the years 2014-2015. As the years go on death rates are doubling from the abuse of opioids. Heroin overdoses continue to rise and now fentanyl is joining its ranks. Fentanyl has been getting mixed with heroin to increase its potency and in return it increases the chance of overdosing (Rondinone, 2017).</description>
    </item>
    
    <item>
      <title>Web Scraping World of Warcraft Weapons</title>
      <link>/2018/2018-05-24-web-scraping-world-of-warcraft-weapons/</link>
      <pubDate>Thu, 24 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/2018-05-24-web-scraping-world-of-warcraft-weapons/</guid>
      <description>Follow Me On    
 Intro The main goal of this project was to learn how to use the R libraries rvest and RSelenium. The web scraping would be taking place on the website wowhead.com. There is about 10,000 rows of data to collect.
Learning Points  Navigate the DOM to capture desired element values Used R’s RSelenium and rvest library to capture data Construct a R script to automate the complete web scraping process Made a data frame with captured data to save to a csv file for data analysis   Web Scraping Process The web scraper goes to the wowhead.</description>
    </item>
    
  </channel>
</rss>