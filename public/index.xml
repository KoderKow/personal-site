<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kyle Harris on Kyle Harris</title>
    <link>/</link>
    <description>Recent content in Kyle Harris on Kyle Harris</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 04 Dec 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Tesseract OCR Version 4.1.0 Parameters</title>
      <link>/2019/2019-12-04-tesseract-ocr-version-4-1-0-parameters/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/2019-12-04-tesseract-ocr-version-4-1-0-parameters/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/datatables-css/datatables-crosstalk.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/datatables-binding/datatables.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/dt-core/css/jquery.dataTables.min.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;/rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/dt-core/js/jquery.dataTables.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/crosstalk/css/crosstalk.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/crosstalk/js/crosstalk.min.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;During a project my team was having a hard time tracking down an up to date parameters list for &lt;a href=&#34;https://github.com/tesseract-ocr/tesseract&#34;&gt;Tesseract OCR&lt;/a&gt;. The documentation mentions to use &lt;hc&gt;tesseract –print-parameters&lt;/hc&gt; to get the available &lt;em&gt;600+&lt;/em&gt; options. Personally, I use R at my job so sifting through the terminal and trying to go through a poorly formatted (this may be due to my unfamiliarity to the terminal) list in the terminal can be a headache. I have taken the current parameters list for &lt;strong&gt;Tesseract OCR 4.1.0&lt;/strong&gt; and made it available on this page and in JSON format in hopes it will help people explore the available options.&lt;/p&gt;
&lt;!-- Below the table there will will be this table is JSON for those wanting the raw data. --&gt;
&lt;/div&gt;
&lt;div id=&#34;cleaning-process&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Cleaning Process&lt;/h1&gt;
&lt;p&gt;I used RStudio’s find/replace functionality to clean up the text I copied from the terminal using the &lt;hc&gt;tesseract –print-parameters&lt;/hc&gt; command. The &lt;em&gt;Description&lt;/em&gt; column has apostrophe’s &lt;strong&gt;removed&lt;/strong&gt; due to the major conflicts it was causing me during data cleaning. There are some single and double quotes in the &lt;em&gt;Value&lt;/em&gt; column, these were intially removed and then re-added. I have triple-checked to ensure they are correct since &lt;em&gt;Value&lt;/em&gt; is important when looking at the default value of these parameters.&lt;/p&gt;
&lt;p&gt;Thank you Tesseract OCR dev team for this fantastic piece of software and making it open source! :)&lt;/p&gt;
&lt;div id=&#34;json-source&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;JSON Source&lt;/h2&gt;
&lt;p&gt;If you are interested in the data used in the table, &lt;a href=&#34;https://raw.githubusercontent.com/KoderKow/stuff/master/data/tp_json.json&#34;&gt;here&lt;/a&gt; is a link to the raw data on &lt;a href=&#34;https://github.com/KoderKow/stuff/blob/master/data/tp_json.json&#34;&gt;my GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;parameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Parameters&lt;/h1&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;allow_blob_division&#34;,&#34;ambigs_debug_level&#34;,&#34;applybox_debug&#34;,&#34;applybox_exposure_pattern&#34;,&#34;applybox_learn_chars_and_char_frags_mode&#34;,&#34;applybox_learn_ngrams_mode&#34;,&#34;applybox_page&#34;,&#34;assume_fixed_pitch_char_segment&#34;,&#34;bidi_debug&#34;,&#34;bland_unrej&#34;,&#34;certainty_scale&#34;,&#34;certainty_scale&#34;,&#34;chop_center_knob&#34;,&#34;chop_centered_maxwidth&#34;,&#34;chop_debug&#34;,&#34;chop_enable&#34;,&#34;chop_good_split&#34;,&#34;chop_inside_angle&#34;,&#34;chop_min_outline_area&#34;,&#34;chop_min_outline_points&#34;,&#34;chop_new_seam_pile&#34;,&#34;chop_ok_split&#34;,&#34;chop_overlap_knob&#34;,&#34;chop_same_distance&#34;,&#34;chop_seam_pile_size&#34;,&#34;chop_sharpness_knob&#34;,&#34;chop_split_dist_knob&#34;,&#34;chop_split_length&#34;,&#34;chop_vertical_creep&#34;,&#34;chop_width_change_knob&#34;,&#34;chop_x_y_weight&#34;,&#34;chs_leading_punct&#34;,&#34;chs_trailing_punct1&#34;,&#34;chs_trailing_punct2&#34;,&#34;classify_adapt_feature_threshold&#34;,&#34;classify_adapt_proto_threshold&#34;,&#34;classify_adapted_pruning_factor&#34;,&#34;classify_adapted_pruning_threshold&#34;,&#34;classify_bln_numeric_mode&#34;,&#34;classify_char_norm_range&#34;,&#34;classify_character_fragments_garbage_certainty_threshold&#34;,&#34;classify_class_pruner_multiplier&#34;,&#34;classify_class_pruner_threshold&#34;,&#34;classify_cp_angle_pad_loose&#34;,&#34;classify_cp_angle_pad_medium&#34;,&#34;classify_cp_angle_pad_tight&#34;,&#34;classify_cp_end_pad_loose&#34;,&#34;classify_cp_end_pad_medium&#34;,&#34;classify_cp_end_pad_tight&#34;,&#34;classify_cp_side_pad_loose&#34;,&#34;classify_cp_side_pad_medium&#34;,&#34;classify_cp_side_pad_tight&#34;,&#34;classify_debug_character_fragments&#34;,&#34;classify_debug_level&#34;,&#34;classify_enable_adaptive_debugger&#34;,&#34;classify_enable_adaptive_matcher&#34;,&#34;classify_enable_learning&#34;,&#34;classify_font_name&#34;,&#34;classify_learn_debug_str&#34;,&#34;classify_learning_debug_level&#34;,&#34;classify_max_certainty_margin&#34;,&#34;classify_max_norm_scale_x&#34;,&#34;classify_max_norm_scale_y&#34;,&#34;classify_max_rating_ratio&#34;,&#34;classify_max_slope&#34;,&#34;classify_min_norm_scale_x&#34;,&#34;classify_min_norm_scale_y&#34;,&#34;classify_min_slope&#34;,&#34;classify_misfit_junk_penalty&#34;,&#34;classify_nonlinear_norm&#34;,&#34;classify_norm_adj_curl&#34;,&#34;classify_norm_adj_midpoint&#34;,&#34;classify_norm_method&#34;,&#34;classify_num_cp_levels&#34;,&#34;classify_pico_feature_length&#34;,&#34;classify_pp_angle_pad&#34;,&#34;classify_pp_end_pad&#34;,&#34;classify_pp_side_pad&#34;,&#34;classify_save_adapted_templates&#34;,&#34;classify_use_pre_adapted_templates&#34;,&#34;conflict_set_I_l_1&#34;,&#34;crunch_accept_ok&#34;,&#34;crunch_debug&#34;,&#34;crunch_del_cert&#34;,&#34;crunch_del_high_word&#34;,&#34;crunch_del_low_word&#34;,&#34;crunch_del_max_ht&#34;,&#34;crunch_del_min_ht&#34;,&#34;crunch_del_min_width&#34;,&#34;crunch_del_rating&#34;,&#34;crunch_early_convert_bad_unlv_chs&#34;,&#34;crunch_early_merge_tess_fails&#34;,&#34;crunch_include_numerals&#34;,&#34;crunch_leave_accept_strings&#34;,&#34;crunch_leave_lc_strings&#34;,&#34;crunch_leave_ok_strings&#34;,&#34;crunch_leave_uc_strings&#34;,&#34;crunch_long_repetitions&#34;,&#34;crunch_poor_garbage_cert&#34;,&#34;crunch_poor_garbage_rate&#34;,&#34;crunch_pot_garbage&#34;,&#34;crunch_pot_indicators&#34;,&#34;crunch_pot_poor_cert&#34;,&#34;crunch_pot_poor_rate&#34;,&#34;crunch_rating_max&#34;,&#34;crunch_small_outlines_size&#34;,&#34;crunch_terrible_garbage&#34;,&#34;crunch_terrible_rating&#34;,&#34;dawg_debug_level&#34;,&#34;debug_acceptable_wds&#34;,&#34;debug_file&#34;,&#34;debug_fix_space_level&#34;,&#34;debug_noise_removal&#34;,&#34;debug_x_ht_level&#34;,&#34;devanagari_split_debugimage&#34;,&#34;devanagari_split_debuglevel&#34;,&#34;disable_character_fragments&#34;,&#34;doc_dict_certainty_threshold&#34;,&#34;doc_dict_pending_threshold&#34;,&#34;docqual_excuse_outline_errs&#34;,&#34;dotproduct&#34;,&#34;edges_boxarea&#34;,&#34;edges_childarea&#34;,&#34;edges_children_count_limit&#34;,&#34;edges_children_fix&#34;,&#34;edges_children_per_grandchild&#34;,&#34;edges_debug&#34;,&#34;edges_max_children_layers&#34;,&#34;edges_max_children_per_outline&#34;,&#34;edges_min_nonhole&#34;,&#34;edges_patharea_ratio&#34;,&#34;edges_use_new_outline_complexity&#34;,&#34;editor_dbwin_height&#34;,&#34;editor_dbwin_name&#34;,&#34;editor_dbwin_width&#34;,&#34;editor_dbwin_xpos&#34;,&#34;editor_dbwin_ypos&#34;,&#34;editor_debug_config_file&#34;,&#34;editor_image_blob_bb_color&#34;,&#34;editor_image_menuheight&#34;,&#34;editor_image_text_color&#34;,&#34;editor_image_win_name&#34;,&#34;editor_image_word_bb_color&#34;,&#34;editor_image_xpos&#34;,&#34;editor_image_ypos&#34;,&#34;editor_word_height&#34;,&#34;editor_word_name&#34;,&#34;editor_word_width&#34;,&#34;editor_word_xpos&#34;,&#34;editor_word_ypos&#34;,&#34;enable_noise_removal&#34;,&#34;equationdetect_save_bi_image&#34;,&#34;equationdetect_save_merged_image&#34;,&#34;equationdetect_save_seed_image&#34;,&#34;equationdetect_save_spt_image&#34;,&#34;file_type&#34;,&#34;fixsp_done_mode&#34;,&#34;fixsp_non_noise_limit&#34;,&#34;fixsp_small_outlines_size&#34;,&#34;force_word_assoc&#34;,&#34;fragments_debug&#34;,&#34;fragments_guide_chopper&#34;,&#34;fx_debugfile&#34;,&#34;gapmap_big_gaps&#34;,&#34;gapmap_debug&#34;,&#34;gapmap_no_isolated_quanta&#34;,&#34;gapmap_use_ends&#34;,&#34;hocr_char_boxes&#34;,&#34;hocr_font_info&#34;,&#34;hyphen_debug_level&#34;,&#34;interactive_display_mode&#34;,&#34;jpg_quality&#34;,&#34;language_model_debug_level&#34;,&#34;language_model_min_compound_length&#34;,&#34;language_model_ngram_nonmatch_score&#34;,&#34;language_model_ngram_on&#34;,&#34;language_model_ngram_order&#34;,&#34;language_model_ngram_rating_factor&#34;,&#34;language_model_ngram_scale_factor&#34;,&#34;language_model_ngram_small_prob&#34;,&#34;language_model_ngram_space_delimited_language&#34;,&#34;language_model_ngram_use_only_first_uft8_step&#34;,&#34;language_model_penalty_case&#34;,&#34;language_model_penalty_chartype&#34;,&#34;language_model_penalty_font&#34;,&#34;language_model_penalty_increment&#34;,&#34;language_model_penalty_non_dict_word&#34;,&#34;language_model_penalty_non_freq_dict_word&#34;,&#34;language_model_penalty_punc&#34;,&#34;language_model_penalty_script&#34;,&#34;language_model_penalty_spacing&#34;,&#34;language_model_use_sigmoidal_certainty&#34;,&#34;language_model_viterbi_list_max_num_prunable&#34;,&#34;language_model_viterbi_list_max_size&#34;,&#34;load_bigram_dawg&#34;,&#34;load_freq_dawg&#34;,&#34;load_number_dawg&#34;,&#34;load_punc_dawg&#34;,&#34;load_system_dawg&#34;,&#34;load_unambig_dawg&#34;,&#34;lstm_choice_mode&#34;,&#34;lstm_use_matrix&#34;,&#34;matcher_avg_noise_size&#34;,&#34;matcher_bad_match_pad&#34;,&#34;matcher_clustering_max_angle_delta&#34;,&#34;matcher_debug_flags&#34;,&#34;matcher_debug_level&#34;,&#34;matcher_debug_separate_windows&#34;,&#34;matcher_good_threshold&#34;,&#34;matcher_min_examples_for_prototyping&#34;,&#34;matcher_perfect_threshold&#34;,&#34;matcher_rating_margin&#34;,&#34;matcher_reliable_adaptive_result&#34;,&#34;matcher_sufficient_examples_for_prototyping&#34;,&#34;max_permuter_attempts&#34;,&#34;max_viterbi_list_size&#34;,&#34;merge_fragments_in_matrix&#34;,&#34;min_characters_to_try&#34;,&#34;min_orientation_margin&#34;,&#34;min_sane_x_ht_pixels&#34;,&#34;multilang_debug_level&#34;,&#34;noise_cert_basechar&#34;,&#34;noise_cert_disjoint&#34;,&#34;noise_cert_factor&#34;,&#34;noise_cert_punc&#34;,&#34;noise_maxperblob&#34;,&#34;noise_maxperword&#34;,&#34;numeric_punctuation&#34;,&#34;ocr_devanagari_split_strategy&#34;,&#34;ok_repeated_ch_non_alphanum_wds&#34;,&#34;oldbl_corrfix&#34;,&#34;oldbl_dot_error_size&#34;,&#34;oldbl_holed_losscount&#34;,&#34;oldbl_xhfix&#34;,&#34;oldbl_xhfract&#34;,&#34;outlines_2&#34;,&#34;outlines_odd&#34;,&#34;output_ambig_words_file&#34;,&#34;page_separator&#34;,&#34;pageseg_devanagari_split_strategy&#34;,&#34;paragraph_debug_level&#34;,&#34;paragraph_text_based&#34;,&#34;pitsync_fake_depth&#34;,&#34;pitsync_joined_edge&#34;,&#34;pitsync_linear_version&#34;,&#34;pitsync_offset_freecut_fraction&#34;,&#34;poly_allow_detailed_fx&#34;,&#34;poly_debug&#34;,&#34;poly_wide_objects_better&#34;,&#34;preserve_interword_spaces&#34;,&#34;prioritize_division&#34;,&#34;quality_blob_pc&#34;,&#34;quality_char_pc&#34;,&#34;quality_min_initial_alphas_reqd&#34;,&#34;quality_outline_pc&#34;,&#34;quality_rej_pc&#34;,&#34;quality_rowrej_pc&#34;,&#34;rating_scale&#34;,&#34;rej_1Il_trust_permuter_type&#34;,&#34;rej_1Il_use_dict_word&#34;,&#34;rej_alphas_in_number_perm&#34;,&#34;rej_trust_doc_dawg&#34;,&#34;rej_use_good_perm&#34;,&#34;rej_use_sensible_wd&#34;,&#34;rej_use_tess_accepted&#34;,&#34;rej_use_tess_blanks&#34;,&#34;rej_whole_of_mostly_reject_word_fract&#34;,&#34;repair_unchopped_blobs&#34;,&#34;save_alt_choices&#34;,&#34;save_doc_words&#34;,&#34;segment_adjust_debug&#34;,&#34;segment_nonalphabetic_script&#34;,&#34;segment_penalty_dict_case_bad&#34;,&#34;segment_penalty_dict_case_ok&#34;,&#34;segment_penalty_dict_frequent_word&#34;,&#34;segment_penalty_dict_nonword&#34;,&#34;segment_penalty_garbage&#34;,&#34;segsearch_debug_level&#34;,&#34;segsearch_max_char_wh_ratio&#34;,&#34;segsearch_max_futile_classifications&#34;,&#34;segsearch_max_pain_points&#34;,&#34;speckle_large_max_size&#34;,&#34;speckle_rating_penalty&#34;,&#34;stopper_allowable_character_badness&#34;,&#34;stopper_certainty_per_char&#34;,&#34;stopper_debug_level&#34;,&#34;stopper_no_acceptable_choices&#34;,&#34;stopper_nondict_certainty_base&#34;,&#34;stopper_phase2_certainty_rejection_offset&#34;,&#34;stopper_smallword_size&#34;,&#34;stream_filelist&#34;,&#34;subscript_max_y_top&#34;,&#34;superscript_bettered_certainty&#34;,&#34;superscript_debug&#34;,&#34;superscript_min_y_bottom&#34;,&#34;superscript_scaledown_ratio&#34;,&#34;superscript_worse_certainty&#34;,&#34;suspect_accept_rating&#34;,&#34;suspect_constrain_1Il&#34;,&#34;suspect_level&#34;,&#34;suspect_rating_per_ch&#34;,&#34;suspect_short_words&#34;,&#34;suspect_space_level&#34;,&#34;tess_bn_matching&#34;,&#34;tess_cn_matching&#34;,&#34;tessedit_adaption_debug&#34;,&#34;tessedit_ambigs_training&#34;,&#34;tessedit_bigram_debug&#34;,&#34;tessedit_certainty_threshold&#34;,&#34;tessedit_char_blacklist&#34;,&#34;tessedit_char_unblacklist&#34;,&#34;tessedit_char_whitelist&#34;,&#34;tessedit_class_miss_scale&#34;,&#34;tessedit_consistent_reps&#34;,&#34;tessedit_create_alto&#34;,&#34;tessedit_create_boxfile&#34;,&#34;tessedit_create_hocr&#34;,&#34;tessedit_create_lstmbox&#34;,&#34;tessedit_create_pdf&#34;,&#34;tessedit_create_tsv&#34;,&#34;tessedit_create_txt&#34;,&#34;tessedit_create_wordstrbox&#34;,&#34;tessedit_debug_block_rejection&#34;,&#34;tessedit_debug_doc_rejection&#34;,&#34;tessedit_debug_fonts&#34;,&#34;tessedit_debug_quality_metrics&#34;,&#34;tessedit_display_outwords&#34;,&#34;tessedit_dont_blkrej_good_wds&#34;,&#34;tessedit_dont_rowrej_good_wds&#34;,&#34;tessedit_dump_choices&#34;,&#34;tessedit_dump_pageseg_images&#34;,&#34;tessedit_enable_bigram_correction&#34;,&#34;tessedit_enable_dict_correction&#34;,&#34;tessedit_enable_doc_dict&#34;,&#34;tessedit_fix_fuzzy_spaces&#34;,&#34;tessedit_fix_hyphens&#34;,&#34;tessedit_flip_0O&#34;,&#34;tessedit_good_doc_still_rowrej_wd&#34;,&#34;tessedit_good_quality_unrej&#34;,&#34;tessedit_image_border&#34;,&#34;tessedit_init_config_only&#34;,&#34;tessedit_load_sublangs&#34;,&#34;tessedit_lower_flip_hyphen&#34;,&#34;tessedit_make_boxes_from_boxes&#34;,&#34;tessedit_matcher_log&#34;,&#34;tessedit_minimal_rej_pass1&#34;,&#34;tessedit_minimal_rejection&#34;,&#34;tessedit_ocr_engine_mode&#34;,&#34;tessedit_override_permuter&#34;,&#34;tessedit_page_number&#34;,&#34;tessedit_pageseg_mode&#34;,&#34;tessedit_parallelize&#34;,&#34;tessedit_prefer_joined_punct&#34;,&#34;tessedit_preserve_blk_rej_perfect_wds&#34;,&#34;tessedit_preserve_min_wd_len&#34;,&#34;tessedit_preserve_row_rej_perfect_wds&#34;,&#34;tessedit_redo_xheight&#34;,&#34;tessedit_reject_bad_qual_wds&#34;,&#34;tessedit_reject_block_percent&#34;,&#34;tessedit_reject_doc_percent&#34;,&#34;tessedit_reject_mode&#34;,&#34;tessedit_reject_row_percent&#34;,&#34;tessedit_rejection_debug&#34;,&#34;tessedit_resegment_from_boxes&#34;,&#34;tessedit_resegment_from_line_boxes&#34;,&#34;tessedit_row_rej_good_docs&#34;,&#34;tessedit_tess_adaption_mode&#34;,&#34;tessedit_test_adaption&#34;,&#34;tessedit_test_adaption_mode&#34;,&#34;tessedit_timing_debug&#34;,&#34;tessedit_train_from_boxes&#34;,&#34;tessedit_train_line_recognizer&#34;,&#34;tessedit_truncate_wordchoice_log&#34;,&#34;tessedit_unrej_any_wd&#34;,&#34;tessedit_upper_flip_hyphen&#34;,&#34;tessedit_use_primary_params_model&#34;,&#34;tessedit_use_reject_spaces&#34;,&#34;tessedit_whole_wd_rej_row_percent&#34;,&#34;tessedit_word_for_word&#34;,&#34;tessedit_write_block_separators&#34;,&#34;tessedit_write_images&#34;,&#34;tessedit_write_params_to_file&#34;,&#34;tessedit_write_rep_codes&#34;,&#34;tessedit_write_unlv&#34;,&#34;tessedit_zero_kelvin_rejection&#34;,&#34;tessedit_zero_rejection&#34;,&#34;test_pt&#34;,&#34;test_pt_x&#34;,&#34;test_pt_y&#34;,&#34;textonly_pdf&#34;,&#34;textord_all_prop&#34;,&#34;textord_ascheight_mode_fraction&#34;,&#34;textord_ascx_ratio_max&#34;,&#34;textord_ascx_ratio_min&#34;,&#34;textord_balance_factor&#34;,&#34;textord_baseline_debug&#34;,&#34;textord_biased_skewcalc&#34;,&#34;textord_blob_size_bigile&#34;,&#34;textord_blob_size_smallile&#34;,&#34;textord_blockndoc_fixed&#34;,&#34;textord_blocksall_fixed&#34;,&#34;textord_blocksall_prop&#34;,&#34;textord_blocksall_testing&#34;,&#34;textord_blshift_maxshift&#34;,&#34;textord_blshift_xfraction&#34;,&#34;textord_chop_width&#34;,&#34;textord_chopper_test&#34;,&#34;textord_debug_baselines&#34;,&#34;textord_debug_blob&#34;,&#34;textord_debug_block&#34;,&#34;textord_debug_bugs&#34;,&#34;textord_debug_pitch_metric&#34;,&#34;textord_debug_pitch_test&#34;,&#34;textord_debug_printable&#34;,&#34;textord_debug_tabfind&#34;,&#34;textord_debug_xheights&#34;,&#34;textord_descheight_mode_fraction&#34;,&#34;textord_descx_ratio_max&#34;,&#34;textord_descx_ratio_min&#34;,&#34;textord_disable_pitch_test&#34;,&#34;textord_dotmatrix_gap&#34;,&#34;textord_equation_detect&#34;,&#34;textord_excess_blobsize&#34;,&#34;textord_expansion_factor&#34;,&#34;textord_fast_pitch_test&#34;,&#34;textord_fix_makerow_bug&#34;,&#34;textord_fix_xheight_bug&#34;,&#34;textord_force_make_prop_words&#34;,&#34;textord_fp_chop_error&#34;,&#34;textord_fp_chop_snap&#34;,&#34;textord_fp_chopping&#34;,&#34;textord_fp_min_width&#34;,&#34;textord_fpiqr_ratio&#34;,&#34;textord_heavy_nr&#34;,&#34;textord_initialasc_ile&#34;,&#34;textord_initialx_ile&#34;,&#34;textord_interpolating_skew&#34;,&#34;textord_linespace_iqrlimit&#34;,&#34;textord_lms_line_trials&#34;,&#34;textord_max_blob_overlaps&#34;,&#34;textord_max_noise_size&#34;,&#34;textord_max_pitch_iqr&#34;,&#34;textord_min_blob_height_fraction&#34;,&#34;textord_min_blobs_in_row&#34;,&#34;textord_min_linesize&#34;,&#34;textord_min_xheight&#34;,&#34;textord_minxh&#34;,&#34;textord_new_initial_xheight&#34;,&#34;textord_no_rejects&#34;,&#34;textord_noise_area_ratio&#34;,&#34;textord_noise_debug&#34;,&#34;textord_noise_hfract&#34;,&#34;textord_noise_normratio&#34;,&#34;textord_noise_rejrows&#34;,&#34;textord_noise_rejwords&#34;,&#34;textord_noise_rowratio&#34;,&#34;textord_noise_sizefraction&#34;,&#34;textord_noise_sizelimit&#34;,&#34;textord_noise_sncount&#34;,&#34;textord_noise_sxfract&#34;,&#34;textord_noise_syfract&#34;,&#34;textord_noise_translimit&#34;,&#34;textord_occupancy_threshold&#34;,&#34;textord_ocropus_mode&#34;,&#34;textord_old_baselines&#34;,&#34;textord_old_xheight&#34;,&#34;textord_oldbl_debug&#34;,&#34;textord_oldbl_jumplimit&#34;,&#34;textord_oldbl_merge_parts&#34;,&#34;textord_oldbl_paradef&#34;,&#34;textord_oldbl_split_splines&#34;,&#34;textord_overlap_x&#34;,&#34;textord_parallel_baselines&#34;,&#34;textord_pitch_cheat&#34;,&#34;textord_pitch_range&#34;,&#34;textord_pitch_rowsimilarity&#34;,&#34;textord_pitch_scalebigwords&#34;,&#34;textord_projection_scale&#34;,&#34;textord_really_old_xheight&#34;,&#34;textord_restore_underlines&#34;,&#34;textord_show_blobs&#34;,&#34;textord_show_boxes&#34;,&#34;textord_show_expanded_rows&#34;,&#34;textord_show_final_blobs&#34;,&#34;textord_show_final_rows&#34;,&#34;textord_show_fixed_cuts&#34;,&#34;textord_show_fixed_words&#34;,&#34;textord_show_initial_rows&#34;,&#34;textord_show_initial_words&#34;,&#34;textord_show_new_words&#34;,&#34;textord_show_page_cuts&#34;,&#34;textord_show_parallel_rows&#34;,&#34;textord_show_row_cuts&#34;,&#34;textord_show_tables&#34;,&#34;textord_single_height_mode&#34;,&#34;textord_skew_ile&#34;,&#34;textord_skew_lag&#34;,&#34;textord_skewsmooth_offset&#34;,&#34;textord_skewsmooth_offset2&#34;,&#34;textord_space_size_is_variable&#34;,&#34;textord_spacesize_ratiofp&#34;,&#34;textord_spacesize_ratioprop&#34;,&#34;textord_spline_medianwin&#34;,&#34;textord_spline_minblobs&#34;,&#34;textord_spline_outlier_fraction&#34;,&#34;textord_spline_shift_fraction&#34;,&#34;textord_straight_baselines&#34;,&#34;textord_tabfind_aligned_gap_fraction&#34;,&#34;textord_tabfind_find_tables&#34;,&#34;textord_tabfind_force_vertical_text&#34;,&#34;textord_tabfind_only_strokewidths&#34;,&#34;textord_tabfind_show_blocks&#34;,&#34;textord_tabfind_show_color_fit&#34;,&#34;textord_tabfind_show_columns&#34;,&#34;textord_tabfind_show_finaltabs&#34;,&#34;textord_tabfind_show_images&#34;,&#34;textord_tabfind_show_initial_partitions&#34;,&#34;textord_tabfind_show_initialtabs&#34;,&#34;textord_tabfind_show_partitions&#34;,&#34;textord_tabfind_show_reject_blobs&#34;,&#34;textord_tabfind_show_strokewidths&#34;,&#34;textord_tabfind_show_vlines&#34;,&#34;textord_tabfind_vertical_text&#34;,&#34;textord_tabfind_vertical_text_ratio&#34;,&#34;textord_tablefind_recognize_tables&#34;,&#34;textord_tablefind_show_mark&#34;,&#34;textord_tablefind_show_stats&#34;,&#34;textord_tabvector_vertical_box_ratio&#34;,&#34;textord_tabvector_vertical_gap_fraction&#34;,&#34;textord_test_landscape&#34;,&#34;textord_test_mode&#34;,&#34;textord_test_x&#34;,&#34;textord_test_y&#34;,&#34;textord_testregion_bottom&#34;,&#34;textord_testregion_left&#34;,&#34;textord_testregion_right&#34;,&#34;textord_testregion_top&#34;,&#34;textord_underline_offset&#34;,&#34;textord_underline_threshold&#34;,&#34;textord_underline_width&#34;,&#34;textord_use_cjk_fp_model&#34;,&#34;textord_width_limit&#34;,&#34;textord_width_smooth_factor&#34;,&#34;textord_words_def_fixed&#34;,&#34;textord_words_def_prop&#34;,&#34;textord_words_default_maxspace&#34;,&#34;textord_words_default_minspace&#34;,&#34;textord_words_default_nonspace&#34;,&#34;textord_words_definite_spread&#34;,&#34;textord_words_initial_lower&#34;,&#34;textord_words_initial_upper&#34;,&#34;textord_words_maxspace&#34;,&#34;textord_words_min_minspace&#34;,&#34;textord_words_minlarge&#34;,&#34;textord_words_pitchsd_threshold&#34;,&#34;textord_words_veto_power&#34;,&#34;textord_words_width_ile&#34;,&#34;textord_wordstats_smooth_factor&#34;,&#34;textord_xheight_error_margin&#34;,&#34;textord_xheight_mode_fraction&#34;,&#34;tosp_all_flips_fuzzy&#34;,&#34;tosp_block_use_cert_spaces&#34;,&#34;tosp_debug_level&#34;,&#34;tosp_dont_fool_with_small_kerns&#34;,&#34;tosp_enough_small_gaps&#34;,&#34;tosp_enough_space_samples_for_median&#34;,&#34;tosp_few_samples&#34;,&#34;tosp_flip_caution&#34;,&#34;tosp_flip_fuzz_kn_to_sp&#34;,&#34;tosp_flip_fuzz_sp_to_kn&#34;,&#34;tosp_fuzzy_kn_fraction&#34;,&#34;tosp_fuzzy_sp_fraction&#34;,&#34;tosp_fuzzy_space_factor&#34;,&#34;tosp_fuzzy_space_factor1&#34;,&#34;tosp_fuzzy_space_factor2&#34;,&#34;tosp_gap_factor&#34;,&#34;tosp_ignore_big_gaps&#34;,&#34;tosp_ignore_very_big_gaps&#34;,&#34;tosp_improve_thresh&#34;,&#34;tosp_init_guess_kn_mult&#34;,&#34;tosp_init_guess_xht_mult&#34;,&#34;tosp_kern_gap_factor1&#34;,&#34;tosp_kern_gap_factor2&#34;,&#34;tosp_kern_gap_factor3&#34;,&#34;tosp_large_kerning&#34;,&#34;tosp_max_sane_kn_thresh&#34;,&#34;tosp_min_sane_kn_sp&#34;,&#34;tosp_narrow_aspect_ratio&#34;,&#34;tosp_narrow_blobs_not_cert&#34;,&#34;tosp_narrow_fraction&#34;,&#34;tosp_near_lh_edge&#34;,&#34;tosp_old_sp_kn_th_factor&#34;,&#34;tosp_old_to_method&#34;,&#34;tosp_only_small_gaps_for_kern&#34;,&#34;tosp_only_use_prop_rows&#34;,&#34;tosp_only_use_xht_gaps&#34;,&#34;tosp_pass_wide_fuzz_sp_to_context&#34;,&#34;tosp_recovery_isolated_row_stats&#34;,&#34;tosp_redo_kern_limit&#34;,&#34;tosp_rep_space&#34;,&#34;tosp_row_use_cert_spaces&#34;,&#34;tosp_row_use_cert_spaces1&#34;,&#34;tosp_rule_9_test_punct&#34;,&#34;tosp_sanity_method&#34;,&#34;tosp_short_row&#34;,&#34;tosp_silly_kn_sp_gap&#34;,&#34;tosp_stats_use_xht_gaps&#34;,&#34;tosp_table_fuzzy_kn_sp_ratio&#34;,&#34;tosp_table_kn_sp_ratio&#34;,&#34;tosp_table_xht_sp_ratio&#34;,&#34;tosp_threshold_bias1&#34;,&#34;tosp_threshold_bias2&#34;,&#34;tosp_use_pre_chopping&#34;,&#34;tosp_use_xht_gaps&#34;,&#34;tosp_wide_aspect_ratio&#34;,&#34;tosp_wide_fraction&#34;,&#34;unlv_tilde_crunching&#34;,&#34;unrecognised_char&#34;,&#34;use_ambigs_for_adaption&#34;,&#34;use_definite_ambigs_for_classifier&#34;,&#34;use_only_first_uft8_step&#34;,&#34;user_defined_dpi&#34;,&#34;user_patterns_file&#34;,&#34;user_patterns_suffix&#34;,&#34;user_words_file&#34;,&#34;user_words_suffix&#34;,&#34;word_to_debug&#34;,&#34;word_to_debug_lengths&#34;,&#34;wordrec_blob_pause&#34;,&#34;wordrec_debug_blamer&#34;,&#34;wordrec_debug_level&#34;,&#34;wordrec_display_all_blobs&#34;,&#34;wordrec_display_all_words&#34;,&#34;wordrec_display_segmentations&#34;,&#34;wordrec_display_splits&#34;,&#34;wordrec_enable_assoc&#34;,&#34;wordrec_max_join_chunks&#34;,&#34;wordrec_no_block&#34;,&#34;wordrec_run_blamer&#34;,&#34;wordrec_skip_no_truth_words&#34;,&#34;wordrec_worst_state&#34;,&#34;words_default_fixed_limit&#34;,&#34;words_default_fixed_space&#34;,&#34;words_default_prop_nonspace&#34;,&#34;words_initial_lower&#34;,&#34;words_initial_upper&#34;,&#34;x_ht_acceptance_tolerance&#34;,&#34;x_ht_min_change&#34;,&#34;xheight_penalty_inconsistent&#34;,&#34;xheight_penalty_subscripts&#34;],[&#34;1&#34;,&#34;0&#34;,&#34;1&#34;,&#34;.exp&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;20&#34;,&#34;20&#34;,&#34;0.15&#34;,&#34;90&#34;,&#34;0&#34;,&#34;1&#34;,&#34;50&#34;,&#34;-50&#34;,&#34;2000&#34;,&#34;6&#34;,&#34;1&#34;,&#34;100&#34;,&#34;0.9&#34;,&#34;2&#34;,&#34;150&#34;,&#34;0.06&#34;,&#34;0.5&#34;,&#34;10000&#34;,&#34;0&#34;,&#34;5&#34;,&#34;3&#34;,&#34;(&#39;`\&#34;&#34;,&#34;).,;:?!&#34;,&#34;(&#39;`\&#34;&#34;,&#34;230&#34;,&#34;230&#34;,&#34;2.5&#34;,&#34;-1&#34;,&#34;0&#34;,&#34;0.2&#34;,&#34;-3&#34;,&#34;15&#34;,&#34;229&#34;,&#34;45&#34;,&#34;20&#34;,&#34;10&#34;,&#34;0.5&#34;,&#34;0.5&#34;,&#34;0.5&#34;,&#34;2.5&#34;,&#34;1.2&#34;,&#34;0.6&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;1&#34;,&#34;1&#34;,&#34;UnknownFont&#34;,null,&#34;0&#34;,&#34;5.5&#34;,&#34;0.325&#34;,&#34;0.325&#34;,&#34;1.5&#34;,&#34;2.41421&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0.414214&#34;,&#34;0&#34;,&#34;0&#34;,&#34;2&#34;,&#34;32&#34;,&#34;1&#34;,&#34;3&#34;,&#34;0.05&#34;,&#34;45&#34;,&#34;0.5&#34;,&#34;2.5&#34;,&#34;0&#34;,&#34;0&#34;,&#34;Il1[]&#34;,&#34;1&#34;,&#34;0&#34;,&#34;-10&#34;,&#34;1.5&#34;,&#34;0.5&#34;,&#34;3&#34;,&#34;0.7&#34;,&#34;3&#34;,&#34;60&#34;,&#34;0&#34;,&#34;1&#34;,&#34;0&#34;,&#34;0&#34;,&#34;4&#34;,&#34;1&#34;,&#34;4&#34;,&#34;3&#34;,&#34;-9&#34;,&#34;60&#34;,&#34;1&#34;,&#34;1&#34;,&#34;-8&#34;,&#34;40&#34;,&#34;10&#34;,&#34;0.6&#34;,&#34;1&#34;,&#34;80&#34;,&#34;0&#34;,&#34;0&#34;,null,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;1&#34;,&#34;-2.25&#34;,&#34;0&#34;,&#34;0&#34;,null,&#34;0.875&#34;,&#34;0.5&#34;,&#34;45&#34;,&#34;0&#34;,&#34;10&#34;,&#34;0&#34;,&#34;5&#34;,&#34;10&#34;,&#34;12&#34;,&#34;40&#34;,&#34;0&#34;,&#34;24&#34;,&#34;EditorDBWin&#34;,&#34;80&#34;,&#34;50&#34;,&#34;500&#34;,null,&#34;4&#34;,&#34;50&#34;,&#34;2&#34;,&#34;EditorImage&#34;,&#34;7&#34;,&#34;590&#34;,&#34;10&#34;,&#34;240&#34;,&#34;BlnWords&#34;,&#34;655&#34;,&#34;60&#34;,&#34;510&#34;,&#34;1&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;.tif&#34;,&#34;1&#34;,&#34;1&#34;,&#34;0.28&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,null,&#34;1.75&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;85&#34;,&#34;0&#34;,&#34;3&#34;,&#34;-40&#34;,&#34;0&#34;,&#34;8&#34;,&#34;16&#34;,&#34;0.03&#34;,&#34;1e-06&#34;,&#34;1&#34;,&#34;0&#34;,&#34;0.1&#34;,&#34;0.3&#34;,&#34;0&#34;,&#34;0.01&#34;,&#34;0.15&#34;,&#34;0.1&#34;,&#34;0.2&#34;,&#34;0.5&#34;,&#34;0.05&#34;,&#34;0&#34;,&#34;10&#34;,&#34;500&#34;,&#34;1&#34;,&#34;1&#34;,&#34;1&#34;,&#34;1&#34;,&#34;1&#34;,&#34;1&#34;,&#34;0&#34;,&#34;1&#34;,&#34;12&#34;,&#34;0.15&#34;,&#34;0.015&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0.125&#34;,&#34;3&#34;,&#34;0.02&#34;,&#34;0.1&#34;,&#34;0&#34;,&#34;5&#34;,&#34;10000&#34;,&#34;10&#34;,&#34;1&#34;,&#34;50&#34;,&#34;7&#34;,&#34;8&#34;,&#34;0&#34;,&#34;-8&#34;,&#34;-1&#34;,&#34;0.375&#34;,&#34;-3&#34;,&#34;8&#34;,&#34;16&#34;,&#34;.&#34;,&#34;0&#34;,&#34;-?*=&#34;,&#34;1&#34;,&#34;1.26&#34;,&#34;10&#34;,&#34;0&#34;,&#34;0.4&#34;,&#34;ij!?%\&#34;:;&#34;,&#34;%|&#34;,null,null,&#34;0&#34;,&#34;0&#34;,&#34;1&#34;,&#34;1&#34;,&#34;0.75&#34;,&#34;6&#34;,&#34;0.25&#34;,&#34;0&#34;,&#34;0&#34;,&#34;1&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0.95&#34;,&#34;2&#34;,&#34;1&#34;,&#34;0.08&#34;,&#34;1.1&#34;,&#34;1.5&#34;,&#34;1&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;1&#34;,&#34;0&#34;,&#34;1&#34;,&#34;1&#34;,&#34;0.85&#34;,&#34;1&#34;,&#34;1&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;1.3125&#34;,&#34;1.1&#34;,&#34;1&#34;,&#34;1.25&#34;,&#34;1.5&#34;,&#34;0&#34;,&#34;2&#34;,&#34;20&#34;,&#34;2000&#34;,&#34;0.3&#34;,&#34;10&#34;,&#34;3&#34;,&#34;-0.5&#34;,&#34;0&#34;,&#34;0&#34;,&#34;-2.5&#34;,&#34;1&#34;,&#34;2&#34;,&#34;0&#34;,&#34;0.5&#34;,&#34;0.97&#34;,&#34;0&#34;,&#34;0.3&#34;,&#34;0.4&#34;,&#34;2&#34;,&#34;-999.9&#34;,&#34;0&#34;,&#34;99&#34;,&#34;999.9&#34;,&#34;2&#34;,&#34;100&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;-2.25&#34;,null,null,null,&#34;0.00390625&#34;,&#34;1&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;1&#34;,&#34;0&#34;,&#34;1&#34;,&#34;1&#34;,&#34;1&#34;,&#34;1&#34;,&#34;1.1&#34;,&#34;1&#34;,&#34;2&#34;,&#34;0&#34;,null,&#34;1.5&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;1&#34;,&#34;1&#34;,&#34;-1&#34;,&#34;6&#34;,&#34;0&#34;,&#34;0&#34;,&#34;1&#34;,&#34;2&#34;,&#34;1&#34;,&#34;1&#34;,&#34;1&#34;,&#34;45&#34;,&#34;65&#34;,&#34;0&#34;,&#34;40&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;1&#34;,&#34;39&#34;,&#34;0&#34;,&#34;3&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;10&#34;,&#34;0&#34;,&#34;1.8&#34;,&#34;0&#34;,&#34;1&#34;,&#34;70&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,null,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;100000&#34;,&#34;100000&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0.08&#34;,&#34;1.8&#34;,&#34;1.25&#34;,&#34;1&#34;,&#34;0&#34;,&#34;1&#34;,&#34;95&#34;,&#34;20&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;9.99&#34;,&#34;1.5&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0.08&#34;,&#34;0.6&#34;,&#34;0.25&#34;,&#34;0&#34;,&#34;3&#34;,&#34;0&#34;,&#34;1.3&#34;,&#34;1&#34;,&#34;0&#34;,&#34;1&#34;,&#34;1&#34;,&#34;0&#34;,&#34;2&#34;,&#34;0.5&#34;,&#34;1&#34;,&#34;0.5&#34;,&#34;1.5&#34;,&#34;0&#34;,&#34;0.9&#34;,&#34;0.75&#34;,&#34;1&#34;,&#34;0.2&#34;,&#34;12&#34;,&#34;4&#34;,&#34;7&#34;,&#34;0.2&#34;,&#34;0.75&#34;,&#34;4&#34;,&#34;1.25&#34;,&#34;10&#34;,&#34;0.25&#34;,&#34;1&#34;,&#34;0&#34;,&#34;0.7&#34;,&#34;0&#34;,&#34;0.015625&#34;,&#34;2&#34;,&#34;1&#34;,&#34;1&#34;,&#34;6&#34;,&#34;10&#34;,&#34;0.5&#34;,&#34;1&#34;,&#34;0.4&#34;,&#34;0.2&#34;,&#34;16&#34;,&#34;0.4&#34;,&#34;0&#34;,&#34;1&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0.15&#34;,&#34;1&#34;,&#34;1&#34;,&#34;1&#34;,&#34;0.375&#34;,&#34;1&#34;,&#34;0&#34;,&#34;2&#34;,&#34;0.08&#34;,&#34;0&#34;,&#34;0.2&#34;,&#34;0&#34;,&#34;1&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0.5&#34;,&#34;0.02&#34;,&#34;4&#34;,&#34;1&#34;,&#34;0&#34;,&#34;2.8&#34;,&#34;2&#34;,&#34;6&#34;,&#34;8&#34;,&#34;0.1&#34;,&#34;0.02&#34;,&#34;0&#34;,&#34;0.75&#34;,&#34;1&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;1&#34;,&#34;0.5&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0.5&#34;,&#34;0.5&#34;,&#34;0&#34;,&#34;0&#34;,&#34;-2147483647&#34;,&#34;-2147483647&#34;,&#34;2147483647&#34;,&#34;-1&#34;,&#34;2147483647&#34;,&#34;-1&#34;,&#34;0.1&#34;,&#34;0.5&#34;,&#34;2&#34;,&#34;0&#34;,&#34;8&#34;,&#34;0.1&#34;,&#34;0.016&#34;,&#34;0.09&#34;,&#34;3.5&#34;,&#34;0.6&#34;,&#34;0.2&#34;,&#34;0.3&#34;,&#34;0.25&#34;,&#34;0.15&#34;,&#34;4&#34;,&#34;0.3&#34;,&#34;0.75&#34;,&#34;0.04&#34;,&#34;5&#34;,&#34;0.4&#34;,&#34;0.05&#34;,&#34;0.1&#34;,&#34;0.4&#34;,&#34;0&#34;,&#34;1&#34;,&#34;0&#34;,&#34;-1&#34;,&#34;0.65&#34;,&#34;3&#34;,&#34;40&#34;,&#34;0&#34;,&#34;1&#34;,&#34;1&#34;,&#34;0.5&#34;,&#34;0.5&#34;,&#34;0.6&#34;,&#34;0.5&#34;,&#34;0.72&#34;,&#34;0.83&#34;,&#34;-1&#34;,&#34;3.5&#34;,&#34;0&#34;,&#34;2.2&#34;,&#34;0.28&#34;,&#34;2&#34;,&#34;1.3&#34;,&#34;2.5&#34;,&#34;0.19&#34;,&#34;5&#34;,&#34;1.5&#34;,&#34;0.48&#34;,&#34;1&#34;,&#34;0.3&#34;,&#34;0&#34;,&#34;2&#34;,&#34;0&#34;,&#34;0&#34;,&#34;1&#34;,&#34;0&#34;,&#34;0.75&#34;,&#34;1&#34;,&#34;10&#34;,&#34;1.6&#34;,&#34;1&#34;,&#34;1&#34;,&#34;0&#34;,&#34;1&#34;,&#34;20&#34;,&#34;0.2&#34;,&#34;1&#34;,&#34;3&#34;,&#34;2.25&#34;,&#34;0.33&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;1&#34;,&#34;0&#34;,&#34;0.52&#34;,&#34;0&#34;,&#34;|&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,null,null,null,null,null,null,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;1&#34;,&#34;4&#34;,&#34;0&#34;,&#34;0&#34;,&#34;0&#34;,&#34;1&#34;,&#34;0.6&#34;,&#34;0.75&#34;,&#34;0.25&#34;,&#34;0.5&#34;,&#34;0.15&#34;,&#34;8&#34;,&#34;8&#34;,&#34;0.25&#34;,&#34;0.125&#34;],[&#34;Use divisible blobs chopping&#34;,&#34;Debug level for unichar ambiguities&#34;,&#34;Debug level&#34;,&#34;Exposure value follows this pattern in the image filename. The name of the image files are expected to be in the form [lang].[fontname].exp[num].tif&#34;,&#34;Learn both character fragments (as is done in the special low exposure mode) as well as unfragmented characters.&#34;,&#34;Each bounding box is assumed to contain ngrams. Only learn the ngrams whose outlines overlap horizontally.&#34;,&#34;Page number to apply boxes from&#34;,&#34;include fixed-pitch heuristics in char segmentation&#34;,&#34;Debug level for BiDi&#34;,&#34;unrej potential with no checks&#34;,&#34;Certainty scaling factor&#34;,&#34;Certainty scaling factor&#34;,&#34;Split center adjustment&#34;,&#34;Width of (smaller) chopped blobs above which we dont care that a chop is not near the center.&#34;,&#34;Chop debug&#34;,&#34;Chop enable&#34;,&#34;Good split limit&#34;,&#34;Min Inside Angle Bend&#34;,&#34;Min Outline Area&#34;,&#34;Min Number of Points on Outline&#34;,&#34;Use new seam_pile&#34;,&#34;OK split limit&#34;,&#34;Split overlap adjustment&#34;,&#34;Same distance&#34;,&#34;Max number of seams in seam_pile&#34;,&#34;Split sharpness adjustment&#34;,&#34;Split length adjustment&#34;,&#34;Split Length&#34;,&#34;Vertical creep&#34;,&#34;Width change adjustment&#34;,&#34;X / Y,length weight&#34;,&#34;Leading punctuation&#34;,&#34;;:?! 1st Trailing punctuation&#34;,&#34;2nd Trailing punctuation&#34;,&#34;Threshold for good features during adaptive 0-255&#34;,&#34;Threshold for good protos during adaptive 0-255&#34;,&#34;Prune poor adapted results this much worse than best result&#34;,&#34;Threshold at which classify_adapted_pruning_factor starts&#34;,&#34;Assume the input is numbers [0-9].&#34;,&#34;Character Normalization Range ...&#34;,&#34;Exclude fragments that do not look like whole characters from training and adaption&#34;,&#34;Class Pruner Multiplier 0-255:,classify_cp_cutoff_strength,7,Class Pruner CutoffStrength:,classify_integer_matcher_multiplier,10,Integer Matcher Multiplier,0-255:,il1_adaption_test,0,Dont adapt to i/I at beginning of word&#34;,&#34;Class Pruner Threshold 0-255&#34;,&#34;Class Pruner Angle Pad Loose&#34;,&#34;Class Pruner Angle Pad Medium&#34;,&#34;CLass Pruner Angle Pad Tight&#34;,&#34;Class Pruner End Pad Loose&#34;,&#34;Class Pruner End Pad Medium&#34;,&#34;Class Pruner End Pad Tight&#34;,&#34;Class Pruner Side Pad Loose&#34;,&#34;Class Pruner Side Pad Medium&#34;,&#34;Class Pruner Side Pad Tight&#34;,&#34;Bring up graphical debugging windows for fragments training&#34;,&#34;Classify debug level&#34;,&#34;Enable match debugger&#34;,&#34;Enable adaptive classifier&#34;,&#34;Enable adaptive classifier&#34;,&#34;Default font name to be used in training&#34;,&#34;Class str to debug learning&#34;,&#34;Learning Debug Level:,matcher_permanent_classes_min,1,Min # of permanent classes&#34;,&#34;Veto difference between classifier certainties&#34;,&#34;Max char x-norm scale ...&#34;,&#34;Max char y-norm scale ...&#34;,&#34;Veto ratio between classifier ratings&#34;,&#34;Slope above which lines are called vertical&#34;,&#34;Min char x-norm scale ...&#34;,&#34;Min char y-norm scale ...&#34;,&#34;Slope below which lines are called horizontal&#34;,&#34;Penalty to apply when a non-alnum is vertically out of its expected textline position&#34;,&#34;Non-linear stroke-density normalization&#34;,&#34;Norm adjust curl ...&#34;,&#34;Norm adjust midpoint ...&#34;,&#34;Normalization Method,...&#34;,&#34;Number of Class Pruner Levels&#34;,&#34;Pico Feature Length&#34;,&#34;Proto Pruner Angle Pad&#34;,&#34;Proto Prune End Pad&#34;,&#34;Proto Pruner Side Pad&#34;,&#34;Save adapted templates to a file&#34;,&#34;Use pre-adapted classifier templates&#34;,&#34;Il1 conflict set&#34;,&#34;Use acceptability in okstring&#34;,&#34;As it says&#34;,&#34;POTENTIAL crunch cert lt this&#34;,&#34;Del if word gt xht x this above bl&#34;,&#34;Del if word gt xht x this below bl&#34;,&#34;Del if word ht gt xht x this&#34;,&#34;Del if word ht lt xht x this&#34;,&#34;Del if word width lt xht x this&#34;,&#34;POTENTIAL crunch rating lt this&#34;,&#34;Take out ~^ early?&#34;,&#34;Before word crunch?&#34;,&#34;Fiddle alpha figures&#34;,&#34;Dont pot crunch sensible strings&#34;,&#34;Dont crunch words with long lower case strings&#34;,&#34;Dont touch sensible strings&#34;,&#34;Dont crunch words with long lower case strings&#34;,&#34;Crunch words with long repetitions&#34;,&#34;crunch garbage cert lt this&#34;,&#34;crunch garbage rating lt this&#34;,&#34;POTENTIAL crunch garbage&#34;,&#34;How many potential indicators needed&#34;,&#34;POTENTIAL crunch cert lt this&#34;,&#34;POTENTIAL crunch rating lt this&#34;,&#34;For adj length in rating per ch&#34;,&#34;Small if lt xht x this&#34;,&#34;As it says&#34;,&#34;crunch rating lt this&#34;,&#34;Set to 1 for general debug info, to 2 for more details, to 3 to see all the debug messages&#34;,&#34;Dump word pass/fail chk&#34;,&#34;File to send tprintf output to&#34;,&#34;Contextual fixspace debug&#34;,&#34;Debug reassignment of small outlines&#34;,&#34;Reestimate debug&#34;,&#34;Whether to create a debug image for split shiro-rekha process.&#34;,&#34;Debug level for split shiro-rekha process.&#34;,&#34;Do not include character fragments in the results of the classifier&#34;,&#34;Worst certainty for words that can be inserted into the document dictionary&#34;,&#34;Worst certainty for using pending dictionary&#34;,&#34;Allow outline errs in unrejection?&#34;,&#34;generic Function used for calculation of dot product&#34;,&#34;Min area fraction of grandchild for box&#34;,&#34;Min area fraction of child outline&#34;,&#34;Max holes allowed in blob&#34;,&#34;Remove boxy parents of char-like children&#34;,&#34;Importance ratio for chucking outlines&#34;,&#34;turn on debugging for this module&#34;,&#34;Max layers of nested children inside a character outline&#34;,&#34;Max number of children inside a character outline&#34;,&#34;Min pixels for potential char in box&#34;,&#34;Max lensq/area for acceptable child outline&#34;,&#34;Use the new outline complexity module&#34;,&#34;Editor debug window height&#34;,&#34;Editor debug window name&#34;,&#34;Editor debug window width&#34;,&#34;Editor debug window X Pos&#34;,&#34;Editor debug window Y Pos&#34;,&#34;Config file to apply to single words&#34;,&#34;Blob bounding box colour&#34;,&#34;Add to image height for menu bar&#34;,&#34;Correct text colour&#34;,&#34;Editor image window name&#34;,&#34;Word bounding box colour&#34;,&#34;Editor image X Pos&#34;,&#34;Editor image Y Pos&#34;,&#34;Word window height&#34;,&#34;BL normalized word window&#34;,&#34;Word window width&#34;,&#34;Word window X Pos&#34;,&#34;Word window Y Pos&#34;,&#34;Remove and conditionally reassign small outlines when they confuse layout analysis, determining diacritics vs noise&#34;,&#34;Save input bi image&#34;,&#34;Save the merged image&#34;,&#34;Save the seed image&#34;,&#34;Save special character image&#34;,&#34;Filename extension&#34;,&#34;What constitues done for spacing&#34;,&#34;How many non-noise blbs either side?&#34;,&#34;Small if lt xht x this&#34;,&#34;force associator to run regardless of what enable_assoc is. This is used for CJK where component grouping is necessary.&#34;,&#34;Debug character fragments&#34;,&#34;Use information from fragments to guide chopping process&#34;,&#34;FXDebug Name of debugfile&#34;,&#34;xht multiplier&#34;,&#34;Say which blocks have tables&#34;,&#34;Ensure gaps not less than 2quanta wide&#34;,&#34;Use large space at start and end of rows&#34;,&#34;Add coordinates for each character to hocr output&#34;,&#34;Add font info to hocr output&#34;,&#34;Debug level for hyphenated words.&#34;,&#34;Run interactively?&#34;,&#34;Set JPEG quality level&#34;,&#34;Language model debug level&#34;,&#34;Minimum length of compound words&#34;,&#34;Average classifier score of a non-matching unichar.&#34;,&#34;Turn on/off the use of character ngram model&#34;,&#34;Maximum order of the character ngram model&#34;,&#34;Factor to bring log-probs into the same range as ratings when multiplied by outline length&#34;,&#34;Strength of the character ngram model relative to the character classifier&#34;,&#34;To avoid overly small denominators use this as the floor of the probability returned by the ngram model.&#34;,&#34;Words are delimited by space&#34;,&#34;Use only the first UTF8 step of the given string when computing log probabilities.&#34;,&#34;Penalty for inconsistent case&#34;,&#34;Penalty for inconsistent character type&#34;,&#34;Penalty for inconsistent font&#34;,&#34;Penalty increment&#34;,&#34;Penalty for non-dictionary words&#34;,&#34;Penalty for words not in the frequent word dictionary&#34;,&#34;Penalty for inconsistent punctuation&#34;,&#34;Penalty for inconsistent script&#34;,&#34;Penalty for inconsistent spacing&#34;,&#34;Use sigmoidal score for certainty&#34;,&#34;Maximum number of prunable (those for which PrunablePath() is true) entries in each viterbi list recorded in BLOB_CHOICEs&#34;,&#34;Maximum size of viterbi lists recorded in BLOB_CHOICEs&#34;,&#34;Load dawg with special word bigrams.&#34;,&#34;Load frequent word dawg.&#34;,&#34;Load dawg with number patterns.&#34;,&#34;Load dawg with punctuation patterns.&#34;,&#34;Load system word dawg.&#34;,&#34;Load unambiguous word dawg.&#34;,&#34;Allows to include alternative symbols choices in the hOCR output. Valid input values are 0, 1, 2 and 3. 0 is the default value. With 1 the alternative symbol choices per timestep are included. With 2 the alternative symbol choices are accumulated per character.&#34;,&#34;Use ratings matrix/beam search with lstm&#34;,&#34;Avg. noise blob length&#34;,&#34;Bad Match Pad (0-1)&#34;,&#34;Maximum angle delta for prototype clustering&#34;,&#34;Matcher Debug Flags&#34;,&#34;Matcher Debug Level&#34;,&#34;Use two different windows for debugging the matching: One for the protos and one for the features.&#34;,&#34;Good Match (0-1)&#34;,&#34;Reliable Config Threshold&#34;,&#34;Perfect Match (0-1)&#34;,&#34;New template margin (0-1)&#34;,&#34;Great Match (0-1)&#34;,&#34;Enable adaption even if the ambiguities have not been seen&#34;,&#34;Maximum number of different character choices to consider during permutation. This limit is especially useful when user patterns are specified, since overly generic patterns can result in dawg search exploring an overly large number of options.&#34;,&#34;Maximum size of viterbi list.&#34;,&#34;Merge the fragments in the ratings matrix and delete them after merging&#34;,&#34;Specify minimum characters to try during OSD&#34;,&#34;Min acceptable orientation margin&#34;,&#34;Reject any x-ht lt or eq than this&#34;,&#34;Print multilang debug info.&#34;,&#34;Hingepoint for base char certainty&#34;,&#34;Hingepoint for disjoint certainty&#34;,&#34;Scaling on certainty diff from Hingepoint&#34;,&#34;Threshold for new punc char certainty&#34;,&#34;Max diacritics to apply to a blob&#34;,&#34;Max diacritics to apply to a word&#34;,&#34;Punct. chs expected WITHIN numbers&#34;,&#34;Whether to use the top-line splitting process for Devanagari documents while performing ocr.&#34;,&#34;Allow NN to unrej&#34;,&#34;Improve correlation of heights&#34;,&#34;Max aspect ratio of a dot&#34;,&#34;Max lost before fallback line used&#34;,&#34;Fix bug in modes threshold for xheights&#34;,&#34;Fraction of est allowed in calc&#34;,&#34;Non standard number of outlines&#34;,&#34;Non standard number of outlines&#34;,&#34;Output file for ambiguities found in the dictionary&#34;,&#34;Page separator (default is form feed control character)&#34;,&#34;Whether to use the top-line splitting process for Devanagari documents while performing page-segmentation.&#34;,&#34;Print paragraph debug info.&#34;,&#34;Run paragraph detection on the post-text-recognition (more accurate)&#34;,&#34;Max advance fake generation&#34;,&#34;Dist inside big blob for chopping&#34;,&#34;Use new fast algorithm&#34;,&#34;Fraction of cut for free cuts&#34;,&#34;Allow feature extractors to see the original outline&#34;,&#34;Debug old poly&#34;,&#34;More accurate approx on wide things&#34;,&#34;Preserve multiple interword spaces&#34;,&#34;Prioritize blob division over chopping&#34;,&#34;good_quality_doc gte good blobs limit&#34;,&#34;good_quality_doc gte good char limit&#34;,&#34;alphas in a good word&#34;,&#34;good_quality_doc lte outline error limit&#34;,&#34;good_quality_doc lte rejection limit&#34;,&#34;good_quality_doc gte good char limit&#34;,&#34;Rating scaling factor&#34;,&#34;Dont double check&#34;,&#34;Use dictword test&#34;,&#34;Extend permuter check&#34;,&#34;Use DOC dawg in 11l conf. detector&#34;,&#34;Individual rejection control&#34;,&#34;Extend permuter check&#34;,&#34;Individual rejection control&#34;,&#34;Individual rejection control&#34;,&#34;if &amp;gt;this fract&#34;,&#34;Fix blobs that arent chopped&#34;,&#34;Save alternative paths found during chopping and segmentation search&#34;,&#34;Save Document Words&#34;,&#34;Segmentation adjustment debug&#34;,&#34;Dont use any alphabetic-specific tricks. Set to true in the traineddata config file for scripts that are cursive or inherently fixed-pitch&#34;,&#34;Default score multiplier for word matches, which may have case issues (lower is better).&#34;,&#34;Score multiplier for word matches that have good case (lower is better).&#34;,&#34;Score multiplier for word matches which have good case and are frequent in the given language (lower is better).&#34;,&#34;Score multiplier for glyph fragment segmentations which do not match a dictionary word (lower is better).&#34;,&#34;Score multiplier for poorly cased strings that are not in the dictionary and generally look like garbage (lower is better).&#34;,&#34;SegSearch debug level&#34;,&#34;Maximum character width-to-height ratio&#34;,&#34;Maximum number of pain point classifications per chunk that did not result in finding a better word choice.&#34;,&#34;Maximum number of pain points stored in the queue&#34;,&#34;Max large speckle size&#34;,&#34;Penalty to add to worst rating for noise&#34;,&#34;Max certaintly variation allowed in a word (in sigma)&#34;,&#34;Certainty to add for each dict char above small word size.&#34;,&#34;Stopper debug level&#34;,&#34;Make AcceptableChoice() always return false. Useful when there is a need to explore all segmentations&#34;,&#34;Certainty threshold for non-dict words&#34;,&#34;Reject certainty offset&#34;,&#34;Size of dict word to be treated as non-dict word&#34;,&#34;Stream a filelist from stdin&#34;,&#34;Maximum top of a character measured as a multiple of x-height above the baseline for us to reconsider whether its a subscript.&#34;,&#34;What reduction in badness do we think sufficient to choose a superscript over what wed thought.,For example, a value of 0.6 means we want to reduce badness of certainty by at least 40%&#34;,&#34;Debug level for sub &amp;amp; superscript fixer&#34;,&#34;Minimum bottom of a character measured as a multiple of x-height above the baseline for us to reconsider whether its a superscript.&#34;,&#34;A superscript scaled down more than this is unbelievably small.,For example, 0.3 means we expect the font size to be no smaller than 30% of the text line font size.&#34;,&#34;How many times worse certainty does a superscript position glyph need to be for us to try classifying it as a char with a different baseline?&#34;,&#34;Accept good rating limit&#34;,&#34;UNLV keep 1Il chars rejected&#34;,&#34;Suspect marker level&#34;,&#34;Dont touch bad rating limit&#34;,&#34;Dont suspect dict wds longer than this&#34;,&#34;Min suspect level for rejecting spaces&#34;,&#34;Baseline Normalized Matching&#34;,&#34;Character Normalized Matching&#34;,&#34;Generate and print debug information for adaption&#34;,&#34;Perform training for ambiguities&#34;,&#34;Amount of debug output for bigram correction.&#34;,&#34;Good blob limit&#34;,&#34;Blacklist of chars not to recognize&#34;,&#34;List of chars to override tessedit_char_blacklist&#34;,&#34;Whitelist of chars to recognize&#34;,&#34;Scale factor for features not used&#34;,&#34;Force all rep chars the same&#34;,&#34;Write .xml ALTO file&#34;,&#34;Output text with boxes&#34;,&#34;Write .html hOCR output file&#34;,&#34;Write .box file for LSTM training&#34;,&#34;Write .pdf output file&#34;,&#34;Write .tsv output file&#34;,&#34;Write .txt output file&#34;,&#34;Write WordStr format .box output file&#34;,&#34;Block and Row stats&#34;,&#34;Page stats&#34;,&#34;Output font info per char&#34;,&#34;Output data to debug file&#34;,&#34;Draw output words&#34;,&#34;Use word segmentation quality metric&#34;,&#34;Use word segmentation quality metric&#34;,&#34;Dump char choices&#34;,&#34;Dump intermediate images made during page segmentation&#34;,&#34;Enable correction based on the word bigram dictionary.&#34;,&#34;Enable single word correction based on the dictionary.&#34;,&#34;Add words to the document dictionary&#34;,&#34;Try to improve fuzzy spaces&#34;,&#34;Crunch double hyphens?&#34;,&#34;Contextual 0O O0 flips&#34;,&#34;rej good doc wd if more than this fraction rejected&#34;,&#34;Reduce rejection on good docs&#34;,&#34;Rej blbs near image edge limit&#34;,&#34;Only initialize with the config file. Useful if the instance is not going to be used for OCR but say only for layout analysis.&#34;,&#34;List of languages to load with this one&#34;,&#34;Aspect ratio dot/hyphen test&#34;,&#34;Generate more boxes from boxed chars&#34;,&#34;Log matcher activity&#34;,&#34;Do minimal rejection on pass 1 output&#34;,&#34;Only reject tess failures&#34;,&#34;Which OCR engine(s) to run (Tesseract, LSTM, both). Defaults to loading and running the most accurate available.&#34;,&#34;According to dict_word&#34;,&#34;-1 -&amp;gt; All pages , else specific page to process&#34;,&#34;Page seg mode: 0=osd only, 1=auto+osd, 2=auto_only, 3=auto, 4=column, 5=block_vert, 6=block, 7=line, 8=word, 9=word_circle, 10=char,11=sparse_text, 12=sparse_text+osd, 13=raw_line (Values from PageSegMode enum in publictypes.h)&#34;,&#34;Run in parallel where possible&#34;,&#34;Reward punctuation joins&#34;,&#34;Only rej partially rejected words in block rejection&#34;,&#34;Only preserve wds longer than this&#34;,&#34;Only rej partially rejected words in row rejection&#34;,&#34;Check/Correct x-height&#34;,&#34;Reject all bad quality wds&#34;,&#34;%rej allowed before rej whole block&#34;,&#34;%rej allowed before rej whole doc&#34;,&#34;Rejection algorithm&#34;,&#34;%rej allowed before rej whole row&#34;,&#34;Adaption debug&#34;,&#34;Take segmentation and labeling from box file&#34;,&#34;Conversion of word/line box file to char box file&#34;,&#34;Apply row rejection to good docs&#34;,&#34;Adaptation decision algorithm for tess&#34;,&#34;Test adaption criteria&#34;,&#34;Adaptation decision algorithm for tess&#34;,&#34;Print timing stats&#34;,&#34;Generate training data from boxed chars&#34;,&#34;Break input into lines and remap boxes if present&#34;,&#34;Max words to keep in list&#34;,&#34;Dont bother with word plausibility&#34;,&#34;Aspect ratio dot/hyphen test&#34;,&#34;In multilingual mode use params model of the primary language&#34;,&#34;Reject spaces?&#34;,&#34;Number of row rejects in whole word rejects which prevents whole row rejection&#34;,&#34;Make output have exactly one word per WERD&#34;,&#34;Write block separators in output&#34;,&#34;Capture the image from the IPE&#34;,&#34;Write all parameters to the given file.&#34;,&#34;Write repetition char code&#34;,&#34;Write .unlv output file&#34;,&#34;Dont reject ANYTHING AT ALL&#34;,&#34;Dont reject ANYTHING&#34;,&#34;Test for point&#34;,&#34;xcoord&#34;,&#34;ycoord&#34;,&#34;Create PDF with only one invisible text layer&#34;,&#34;All doc is proportial text&#34;,&#34;Min pile height to make ascheight&#34;,&#34;Max cap/xheight&#34;,&#34;Min cap/xheight&#34;,&#34;Ding rate for unbalanced char cells&#34;,&#34;Baseline debug level&#34;,&#34;Bias skew estimates with line length&#34;,&#34;Percentile for large blobs&#34;,&#34;Percentile for small blobs&#34;,&#34;Attempt whole doc/block fixed pitch&#34;,&#34;Moan about prop blocks&#34;,&#34;Moan about fixed pitch blocks&#34;,&#34;Dump stats when moaning&#34;,&#34;Max baseline shift&#34;,&#34;Min size of baseline shift&#34;,&#34;Max width before chopping&#34;,&#34;Chopper is being tested.&#34;,&#34;Debug baseline generation&#34;,&#34;Print test blob information&#34;,&#34;Block to do debug on&#34;,&#34;Turn on output related to bugs in tab finding&#34;,&#34;Write full metric stuff&#34;,&#34;Debug on fixed pitch test&#34;,&#34;Make debug windows printable&#34;,&#34;Debug tab finding&#34;,&#34;Test xheight algorithms&#34;,&#34;Min pile height to make descheight&#34;,&#34;Max desc/xheight&#34;,&#34;Min desc/xheight&#34;,&#34;Turn off dp fixed pitch algorithm&#34;,&#34;Max pixel gap for broken pixed pitch&#34;,&#34;Turn on equation detector&#34;,&#34;New row made if blob makes row this big&#34;,&#34;Factor to expand rows by in expand_rows&#34;,&#34;Do even faster pitch algorithm&#34;,&#34;Prevent multiple baselines&#34;,&#34;Use spline baseline&#34;,&#34;Force proportional word segmentation on all rows&#34;,&#34;Max allowed bending of chop cells&#34;,&#34;Max distance of chop pt from vertex&#34;,&#34;Do fixed pitch chopping&#34;,&#34;Min width of decent blobs&#34;,&#34;Pitch IQR/Gap IQR threshold&#34;,&#34;Vigorously remove noise&#34;,&#34;Ile of sizes for xheight guess&#34;,&#34;Ile of sizes for xheight guess&#34;,&#34;Interpolate across gaps&#34;,&#34;Max iqr/median for linespace&#34;,&#34;Number of linew fits to do&#34;,&#34;Max number of blobs a big blob can overlap&#34;,&#34;Pixel size of noise&#34;,&#34;Xh fraction noise in pitch&#34;,&#34;Min blob height/top to include blob top into xheight stats&#34;,&#34;Min blobs before gradient counted&#34;,&#34;* blob height for initial linesize&#34;,&#34;Min credible pixel xheight&#34;,&#34;fraction of linesize for min xheight&#34;,&#34;Use test xheight mechanism&#34;,&#34;Dont remove noise blobs&#34;,&#34;Fraction of bounding box for noise&#34;,&#34;Debug row garbage detector&#34;,&#34;Height fraction to discard outlines as speckle noise&#34;,&#34;Dot to norm ratio for deletion&#34;,&#34;Reject noise-like rows&#34;,&#34;Reject noise-like words&#34;,&#34;Dot to norm ratio for deletion&#34;,&#34;Fraction of size for maxima&#34;,&#34;Fraction of x for big t count&#34;,&#34;super norm blobs to save row&#34;,&#34;xh fract width error for norm blobs&#34;,&#34;xh fract height error for norm blobs&#34;,&#34;Transitions for normal blob&#34;,&#34;Fraction of neighbourhood&#34;,&#34;Make baselines for ocropus&#34;,&#34;Use old baseline algorithm&#34;,&#34;Use old xheight algorithm&#34;,&#34;Debug old baseline generation&#34;,&#34;X fraction for new partition&#34;,&#34;Merge suspect partitions&#34;,&#34;Use para default mechanism&#34;,&#34;Split stepped splines&#34;,&#34;Fraction of linespace for good overlap&#34;,&#34;Force parallel baselines&#34;,&#34;Use correct answer for fixed/prop&#34;,&#34;Max range test on pitch&#34;,&#34;Fraction of xheight for sameness&#34;,&#34;Scale scores on big words&#34;,&#34;Ding rate for mid-cuts&#34;,&#34;Use original wiseowl xheight&#34;,&#34;Chop underlines &amp;amp; put back&#34;,&#34;Display unsorted blobs&#34;,&#34;Display unsorted blobs&#34;,&#34;Display rows after expanding&#34;,&#34;Display blob bounds after pre-ass&#34;,&#34;Display rows after final fitting&#34;,&#34;Draw fixed pitch cell boundaries&#34;,&#34;Display forced fixed pitch words&#34;,&#34;Display row accumulation&#34;,&#34;Display separate words&#34;,&#34;Display separate words&#34;,&#34;Draw page-level cuts&#34;,&#34;Display page correlated rows&#34;,&#34;Draw row-level cuts&#34;,&#34;Show table regions&#34;,&#34;Script has no xheight, so use a single mode&#34;,&#34;Ile of gradients for page skew&#34;,&#34;Lag for skew on row accumulation&#34;,&#34;For smooth factor&#34;,&#34;For smooth factor&#34;,&#34;If true, word delimiter spaces are assumed to have variable width, even though characters have fixed pitch.&#34;,&#34;Min ratio space/nonspace&#34;,&#34;Min ratio space/nonspace&#34;,&#34;Size of window for spline segmentation&#34;,&#34;Min blobs in each spline segment&#34;,&#34;Fraction of line spacing for outlier&#34;,&#34;Fraction of line spacing for quad&#34;,&#34;Force straight baselines&#34;,&#34;Fraction of height used as a minimum gap for aligned blobs.&#34;,&#34;run table detection&#34;,&#34;Force using vertical text page mode&#34;,&#34;Only run stroke widths&#34;,&#34;Show final block bounds&#34;,&#34;Show stroke widths&#34;,&#34;Show column bounds&#34;,&#34;Show tab vectors&#34;,&#34;Show image blobs&#34;,&#34;Show partition bounds&#34;,&#34;Show tab candidates&#34;,&#34;Show partition bounds, waiting if &amp;gt;1&#34;,&#34;Show blobs rejected as noise&#34;,&#34;Show stroke widths&#34;,&#34;Debug line finding&#34;,&#34;Enable vertical detection&#34;,&#34;Fraction of textlines deemed vertical to use vertical page mode&#34;,&#34;Enables the table recognizer for table layout and filtering.&#34;,&#34;Debug table marking steps in detail&#34;,&#34;Show page stats used in table finding&#34;,&#34;Fraction of box matches required to declare a line vertical&#34;,&#34;max fraction of mean blob width allowed for vertical gaps in vertical text&#34;,&#34;Tests refer to land/port&#34;,&#34;Do current test&#34;,&#34;coord of test pt&#34;,&#34;coord of test pt&#34;,&#34;Bottom edge of debug rectangle&#34;,&#34;Left edge of debug reporting rectangle&#34;,&#34;Right edge of debug rectangle&#34;,&#34;Top edge of debug reporting rectangle&#34;,&#34;Fraction of x to ignore&#34;,&#34;Fraction of width occupied&#34;,&#34;Multiple of line_size for underline&#34;,&#34;Use CJK fixed pitch model&#34;,&#34;Max width of blobs to make rows&#34;,&#34;Smoothing width stats&#34;,&#34;Threshold for definite fixed&#34;,&#34;Threshold for definite prop&#34;,&#34;Max believable third space&#34;,&#34;Fraction of xheight&#34;,&#34;Fraction of xheight&#34;,&#34;Non-fuzzy spacing region&#34;,&#34;Max initial cluster size&#34;,&#34;Min initial cluster spacing&#34;,&#34;Multiple of xheight&#34;,&#34;Fraction of xheight&#34;,&#34;Fraction of valid gaps needed&#34;,&#34;Pitch sync threshold&#34;,&#34;Rows required to outvote a veto&#34;,&#34;Ile of blob widths for space est&#34;,&#34;Smoothing gap stats&#34;,&#34;Accepted variation&#34;,&#34;Min pile height to make xheight&#34;,&#34;Pass ANY flip to context?,tosp_fuzzy_limit_all,1,Dont restrict kn-&amp;gt;sp fuzzy limit to tables&#34;,&#34;Only stat OBVIOUS spaces&#34;,&#34;Debug data&#34;,&#34;Limit use of xht gap with odd small kns&#34;,&#34;Fract of kerns reqd for isolated row stats&#34;,&#34;or should we use mean&#34;,&#34;No.gaps reqd with 1 large gap to treat as a table&#34;,&#34;Dont autoflip kn to sp when large separation&#34;,&#34;Default flip&#34;,&#34;Default flip&#34;,&#34;New fuzzy kn alg&#34;,&#34;New fuzzy sp alg&#34;,&#34;Fract of xheight for fuzz sp&#34;,&#34;Fract of xheight for fuzz sp&#34;,&#34;Fract of xheight for fuzz sp&#34;,&#34;gap ratio to flip sp-&amp;gt;kern&#34;,&#34;xht multiplier&#34;,&#34;xht multiplier&#34;,&#34;Enable improvement heuristic&#34;,&#34;Thresh guess - mult kn by this&#34;,&#34;Thresh guess - mult xht by this&#34;,&#34;gap ratio to flip kern-&amp;gt;sp&#34;,&#34;gap ratio to flip kern-&amp;gt;sp&#34;,&#34;gap ratio to flip kern-&amp;gt;sp&#34;,&#34;Limit use of xht gap with large kns&#34;,&#34;Multiplier on kn to limit thresh&#34;,&#34;Dont trust spaces less than this time kn&#34;,&#34;narrow if w/h less than this&#34;,&#34;Only stat OBVIOUS spaces&#34;,&#34;Fract of xheight for narrow&#34;,&#34;Dont reduce box if the top left is non blank&#34;,&#34;Factor for defining space threshold in terms of space and kern sizes&#34;,&#34;Space stats use prechopping?,tosp_old_to_constrain_sp_kn,0,Constrain relative values of inter and intra-word gaps for old_to_method.&#34;,&#34;Better guess&#34;,&#34;Block stats to use fixed pitch rows?,tosp_force_wordbreak_on_punct,0,Force word breaks on punct to break long lines in non-space delimited langs&#34;,&#34;Only use within xht gap for wd breaks&#34;,&#34;How wide fuzzies need context&#34;,&#34;Use row alone when inadequate cert spaces&#34;,&#34;No.samples reqd to reestimate for row&#34;,&#34;rep gap multiplier for space&#34;,&#34;Only stat OBVIOUS spaces&#34;,&#34;Only stat OBVIOUS spaces&#34;,&#34;Dont chng kn to space next to punct&#34;,&#34;How to avoid being silly&#34;,&#34;No.gaps reqd with few cert spaces to use certs&#34;,&#34;Dont let sp minus kn get too small&#34;,&#34;Use within xht gap for wd breaks&#34;,&#34;Fuzzy if less than this&#34;,&#34;Min difference of kn &amp;amp; sp in table&#34;,&#34;Expect spaces bigger than this&#34;,&#34;how far between kern and space?&#34;,&#34;how far between kern and space?&#34;,&#34;Space stats use prechopping?,tosp_old_to_bug_fix,0,Fix suspected bug in old code&#34;,&#34;Use within xht gap for wd breaks&#34;,&#34;wide if w/h less than this&#34;,&#34;Fract of xheight for wide&#34;,&#34;Mark v.bad words for tilde crunch&#34;,&#34;Output char for unidentified blobs&#34;,&#34;Use ambigs for deciding whether to adapt to a character&#34;,&#34;Use definite ambiguities when running character classifier&#34;,&#34;Use only the first UTF8 step of the given string when computing log probabilities.&#34;,&#34;Specify DPI for input image&#34;,&#34;A filename of user-provided patterns.&#34;,&#34;A suffix of user-provided patterns located in tessdata.&#34;,&#34;A filename of user-provided words.&#34;,&#34;A suffix of user-provided words located in tessdata.&#34;,&#34;Word for which stopper debug information should be printed to stdout&#34;,&#34;Lengths of unichars in word_to_debug&#34;,&#34;Blob pause&#34;,&#34;Print blamer debug messages&#34;,&#34;Debug level for wordrec&#34;,&#34;Display Blobs&#34;,&#34;Display Words&#34;,&#34;Display Segmentations&#34;,&#34;Display splits&#34;,&#34;Associator Enable&#34;,&#34;Max number of broken pieces to associate&#34;,&#34;Dont output block information&#34;,&#34;Try to set the blame for errors&#34;,&#34;Only run OCR for words that had truth recorded in BlamerBundle&#34;,&#34;Worst segmentation state&#34;,&#34;Allowed size variance&#34;,&#34;Fraction of xheight&#34;,&#34;Fraction of xheight&#34;,&#34;Max initial cluster size&#34;,&#34;Min initial cluster spacing&#34;,&#34;Max allowed deviation of blob top outside of font data&#34;,&#34;Min change in xht before actually trying it&#34;,&#34;Score penalty (0.1 = 10%) added if an xheight is inconsistent.&#34;,&#34;Score penalty (0.1 = 10%) added if there are subscripts or superscripts in a word, but it is otherwise OK.&#34;]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt;Parameter&lt;\/th&gt;\n      &lt;th&gt;Value&lt;\/th&gt;\n      &lt;th&gt;Description&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;dom&#34;:&#34;ft&#34;,&#34;ordering&#34;:false,&#34;pageLength&#34;:650,&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false,&#34;lengthMenu&#34;:[10,25,50,100,650]}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting Tree Models</title>
      <link>/2019/2019-09-13-fitting-tree-models/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/2019-09-13-fitting-tree-models/</guid>
      <description>


&lt;!-- ```{r libraries, message=FALSE} --&gt;
&lt;!-- library(knitr) --&gt;
&lt;!-- library(kableExtra) --&gt;
&lt;!-- library(xgboost) --&gt;
&lt;!-- library(tidyverse) --&gt;
&lt;!-- library(rlang) --&gt;
&lt;!-- library(skimr) --&gt;
&lt;!-- library(ggthemes) --&gt;
&lt;!-- library(kowr) --&gt;
&lt;!-- library(countrycode) --&gt;
&lt;!-- library(rpart) --&gt;
&lt;!-- library(rpart.plot) --&gt;
&lt;!-- library(caret) --&gt;
&lt;!-- library(ModelMetrics) --&gt;
&lt;!-- library(randomForest) --&gt;
&lt;!-- library(ranger) --&gt;
&lt;!-- library(tictoc) --&gt;
&lt;!-- library(DT) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r, include=FALSE} --&gt;
&lt;!-- # data_url &lt;- &#34;/Users/kow/Downloads/adult.data&#34; --&gt;
&lt;!-- # names_url &lt;- &#34;/Users/kow/Downloads/adult.names&#34; --&gt;
&lt;!-- # test_url &lt;- &#34;/Users/kow/Downloads/adult.test&#34; --&gt;
&lt;!-- update_geom &lt;- function(geom) { --&gt;
&lt;!--   x &lt;- map(geom, ~ { --&gt;
&lt;!--     update_geom_defaults( --&gt;
&lt;!--       geom = .x, --&gt;
&lt;!--       new = list( --&gt;
&lt;!--         color = &#34;#1A1A1A&#34;, --&gt;
&lt;!--         fill = &#34;#4287c7&#34;, --&gt;
&lt;!--         alpha = 0.8) --&gt;
&lt;!--     ) --&gt;
&lt;!--   }) --&gt;
&lt;!-- } --&gt;
&lt;!-- theme_kow &lt;- function(ggplot_ojbect) { --&gt;
&lt;!--   update_geom(c(&#34;col&#34;, &#34;bar&#34;, &#34;boxplot&#34;, &#34;point&#34;)) --&gt;
&lt;!--   theme_gdocs() + --&gt;
&lt;!--     theme( --&gt;
&lt;!--       axis.text.x  = element_text(color = &#34;#1A1A1A&#34;), --&gt;
&lt;!--       axis.title.x = element_text(color = &#34;#1A1A1A&#34;), --&gt;
&lt;!--       axis.text.y  = element_text(color = &#34;#1A1A1A&#34;), --&gt;
&lt;!--       axis.title.y = element_text(color = &#34;#1A1A1A&#34;) --&gt;
&lt;!--     )  --&gt;
&lt;!-- } --&gt;
&lt;!-- theme_set(theme_kow()) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- The adult data is obtained from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). On the site they have the data split into multiple pieces: --&gt;
&lt;!-- - data: the training data --&gt;
&lt;!-- - test: the test data --&gt;
&lt;!-- - names: column names --&gt;
&lt;!-- We can read the data directly from their website. For readability the URLs will be held within variables. --&gt;
&lt;!-- ```{r urls} --&gt;
&lt;!-- data_url &lt;- &#34;https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data&#34; --&gt;
&lt;!-- names_url &lt;- &#34;https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names&#34; --&gt;
&lt;!-- test_url &lt;- &#34;https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test&#34; --&gt;
&lt;!-- ``` --&gt;
&lt;!-- &lt;hc&gt;read_csv()&lt;/hc&gt; will allow us to read in the data from the site, however, we know the data has no column names. Thus, we shall read in the column names first. The data from &lt;hc&gt;names_url&lt;/hc&gt; is not in a format for easy R interpritation. After some exploration on the website it looks like a .txt file, so we can use readlines to look at the data and find what we are looking for. --&gt;
&lt;!-- ```{r, class.output = &#34;scroll&#34;} --&gt;
&lt;!-- names_file &lt;- read_lines(names_url) --&gt;
&lt;!-- names_file --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Lucky for us the column names are at the end right after an empty string. &lt;hc&gt;names_file&lt;/hc&gt; is a character vector so we can use &lt;hc&gt;which()&lt;/hc&gt; to seek the pattern we want.  Each name/description combination is seperated by a &#34;:&#34;, we can turn the end of this file in a small data dictionary with two columns; &lt;hc&gt;column_name&lt;/hc&gt; and &lt;hc&gt;description&lt;/hc&gt;. We can use &lt;hc&gt;read_delim()&lt;/hc&gt; to read in the file. We can use the parameters to name the wanted columns, how many lines to skip, and specify what seperates the columns. --&gt;
&lt;!-- ```{r, message=FALSE} --&gt;
&lt;!-- skip_to_here &lt;- which(names_file == &#34;&#34;) %&gt;% --&gt;
&lt;!--   max() --&gt;
&lt;!-- skip_to_here --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- data_dictionary &lt;- read_delim( --&gt;
&lt;!--   file = names_url, --&gt;
&lt;!--   col_names = c(&#34;column_name&#34;, &#34;description&#34;), --&gt;
&lt;!--   skip = skip_to_here, --&gt;
&lt;!--   delim = &#34;:&#34; --&gt;
&lt;!-- ) --&gt;
&lt;!-- data_dictionary --&gt;
&lt;!-- ``` --&gt;
&lt;!-- From information seen on the website about our data, we know there should be 15 rows. Exploring the data in the browser we know the category *income* is the last column. With that information we will create an additional row to the data dictionary so we have all the columns we need. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- data_dictionary[nrow(data_dictionary) + 1, ] &lt;- c(&#34;income&#34;, &#34;&gt;50K, &lt;=50K&#34;) --&gt;
&lt;!-- data_dictionary --&gt;
&lt;!-- ``` --&gt;
&lt;!-- I am a fan of snake case, so we will replace the &#34;-&#34; with &#34;_&#34;. Then we will use the data dictionary&#39;s *column_name* feature to label the test/train data we will be reading in. For data exploration and cleaning I will be combining the test and training sets. To ensure an easy split of data when it comes to modeling I will add a *group* feature so we can later split the full data set with ease. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- data_dictionary &lt;- data_dictionary %&gt;%  --&gt;
&lt;!--   mutate(column_name = str_replace_all(column_name, &#34;-&#34;, &#34;_&#34;)) --&gt;
&lt;!-- train &lt;- read_csv( --&gt;
&lt;!--   file = data_url, --&gt;
&lt;!--   col_names = data_dictionary$column_name --&gt;
&lt;!--   ) %&gt;%  --&gt;
&lt;!--   mutate(group = &#34;train&#34;) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- When looking at the data online we see the first line of the test data has a line stating &#34;1x3 Cross validator&#34;. This will cause an error so we will skip the first line and then combine the two datasets. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- test &lt;- read_csv( --&gt;
&lt;!--   file = &#34;/Users/kow/Downloads/adult.test&#34;, --&gt;
&lt;!--   skip = 1, --&gt;
&lt;!--   col_names = data_dictionary$column_name --&gt;
&lt;!-- ) %&gt;%  --&gt;
&lt;!--   mutate(group = &#34;test&#34;) --&gt;
&lt;!-- all_data &lt;- bind_rows(train, test) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- # EDA --&gt;
&lt;!-- &lt;hc&gt;skim()&lt;/hc&gt; from the **skimr** package will give us the view &lt;hc&gt;summary()&lt;/hc&gt; or **dplyr**&#39;s &lt;hc&gt;glimpse()&lt;/hc&gt; does, but with additional information. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- skim(all_data) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ## Data Cleaning --&gt;
&lt;!-- I will now go through each variable and collapse character features where available. Hopefully this will increase our accuracy due to possible under-representation of groups in this data. The flow will be counting the data and grouping the factors to the best of my knowledge. Grouping will be with assumptions I make in my noggin&#39;. --&gt;
&lt;!-- ### Remove Variables --&gt;
&lt;!-- I am unsure what *fnlwgt* is exactly and *education_num* is a numeric version of *education*. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data &lt;- all_data %&gt;%  --&gt;
&lt;!--   select(-fnlwgt, -education_num) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ### Education --&gt;
&lt;!-- &#34;An ordered factor with levels Preschool &lt; 1st-4th &lt; 5th-6th &lt; 7th-8th &lt; 9th &lt; 10th &lt; 11th &lt; 12th &lt; HS-grad &lt; Prof-school &lt; Assoc-acdm &lt; Assoc-voc &lt; Some-college &lt; Bachelors &lt; Masters &lt; Doctorate.&#34; --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data %&gt;%  --&gt;
&lt;!--   count(education, sort = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data &lt;- all_data %&gt;%  --&gt;
&lt;!--   mutate( --&gt;
&lt;!--     education = fct_collapse( --&gt;
&lt;!--       education, --&gt;
&lt;!--       &#34;No Diploma&#34; = c(&#34;Preschool&#34;, &#34;1st-4th&#34;, &#34;5th-6th&#34;, &#34;7th-8th&#34;, paste0(9:12, &#34;th&#34;)), --&gt;
&lt;!--       &#34;High School&#34; = &#34;HS-grad&#34;, --&gt;
&lt;!--       &#34;Professional School&#34; = &#34;Prof-school&#34;, --&gt;
&lt;!--       &#34;Associates&#34; = c(&#34;Assoc-acdm&#34;, &#34;Assoc-voc&#34;),  --&gt;
&lt;!--       &#34;Some College&#34; = &#34;Some-college&#34;, --&gt;
&lt;!--       &#34;Bachelors&#34; = &#34;Bachelors&#34;, --&gt;
&lt;!--       &#34;Graduate&#34; = c(&#34;Doctorate&#34;, &#34;Masters&#34;) --&gt;
&lt;!--     ) --&gt;
&lt;!--   ) --&gt;
&lt;!-- all_data %&gt;%  --&gt;
&lt;!--   count(education, sort = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- --- --&gt;
&lt;!-- ### Marital Status --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data %&gt;%  --&gt;
&lt;!--   count(marital_status, sort = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data &lt;- all_data %&gt;%  --&gt;
&lt;!--   mutate( --&gt;
&lt;!--     marital_status = fct_collapse( --&gt;
&lt;!--       marital_status, --&gt;
&lt;!--       &#34;Not Married&#34; = c(&#34;Divorced&#34;, &#34;Separated&#34;, &#34;Widowed&#34;), --&gt;
&lt;!--       &#34;Never Married&#34; = &#34;Never-married&#34;, --&gt;
&lt;!--       &#34;Married&#34; = c(&#34;Married-civ-spouse&#34;, &#34;Married-spouse-absent&#34;, &#34;Married-AF-spouse&#34;) --&gt;
&lt;!--     ) --&gt;
&lt;!--   ) --&gt;
&lt;!-- all_data %&gt;%  --&gt;
&lt;!--   count(marital_status, sort = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- --- --&gt;
&lt;!-- ### Income --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data %&gt;%  --&gt;
&lt;!--   count(income, sort = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data &lt;- all_data %&gt;%  --&gt;
&lt;!--   mutate(income = str_remove_all(income, fixed(&#34;.&#34;))) --&gt;
&lt;!-- all_data %&gt;%  --&gt;
&lt;!--   count(income, sort = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- --- --&gt;
&lt;!-- ### Native Country --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data %&gt;%  --&gt;
&lt;!--   count(native_country, sort = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- country_table &lt;- all_data %&gt;%  --&gt;
&lt;!--   count(native_country, sort = TRUE) %&gt;%  --&gt;
&lt;!--   mutate( --&gt;
&lt;!--     code = countrycode::countrycode(native_country, --&gt;
&lt;!--                                     origin = &#34;country.name&#34;, --&gt;
&lt;!--                                     destination = &#34;continent&#34;) --&gt;
&lt;!--   ) --&gt;
&lt;!-- country_table --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- country_table %&gt;%  --&gt;
&lt;!--   mutate( --&gt;
&lt;!--     code2 = case_when( --&gt;
&lt;!--       native_country == &#34;?&#34; ~ &#34;Unknown&#34;, --&gt;
&lt;!--       native_country == &#34;South&#34; ~ &#34;Unknown&#34;, --&gt;
&lt;!--       native_country == &#34;England&#34; ~ &#34;Europe&#34;, --&gt;
&lt;!--       native_country == &#34;Columbia&#34; ~ &#34;Americas&#34;, --&gt;
&lt;!--       native_country == &#34;Hong&#34; ~ &#34;Asia&#34;, --&gt;
&lt;!--       native_country == &#34;Yugoslavia&#34; ~ &#34;Europe&#34;, --&gt;
&lt;!--       native_country == &#34;Scotland&#34; ~ &#34;Europe&#34;, --&gt;
&lt;!--       TRUE ~ code --&gt;
&lt;!--     ) --&gt;
&lt;!--   ) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data &lt;- all_data %&gt;%  --&gt;
&lt;!--   mutate( --&gt;
&lt;!--     code = countrycode( --&gt;
&lt;!--       native_country, --&gt;
&lt;!--       origin = &#34;country.name&#34;, --&gt;
&lt;!--       destination = &#34;continent&#34; --&gt;
&lt;!--       ), --&gt;
&lt;!--     native_continent = case_when( --&gt;
&lt;!--       native_country == &#34;?&#34; ~ &#34;Unknown&#34;, --&gt;
&lt;!--       native_country == &#34;South&#34; ~ &#34;Unknown&#34;, --&gt;
&lt;!--       native_country == &#34;England&#34; ~ &#34;Europe&#34;, --&gt;
&lt;!--       native_country == &#34;Columbia&#34; ~ &#34;Americas&#34;, --&gt;
&lt;!--       native_country == &#34;Hong&#34; ~ &#34;Asia&#34;, --&gt;
&lt;!--       native_country == &#34;Yugoslavia&#34; ~ &#34;Europe&#34;, --&gt;
&lt;!--       native_country == &#34;Scotland&#34; ~ &#34;Europe&#34;, --&gt;
&lt;!--       TRUE ~ code --&gt;
&lt;!--     ) --&gt;
&lt;!--   ) %&gt;%  --&gt;
&lt;!--   select(-native_country, -code) --&gt;
&lt;!-- all_data %&gt;%  --&gt;
&lt;!--   count(native_continent, sort = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- --- --&gt;
&lt;!-- ### Occupation --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data %&gt;%  --&gt;
&lt;!--   count(occupation, sort = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data &lt;- all_data %&gt;%  --&gt;
&lt;!--   mutate(occupation = ifelse(occupation == &#34;?&#34;, &#34;Unknown&#34;, occupation)) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- --- --&gt;
&lt;!-- ### Race --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data %&gt;%  --&gt;
&lt;!--   count(race, sort = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- --- --&gt;
&lt;!-- ### Relationship --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data %&gt;%  --&gt;
&lt;!--   count(relationship, sort = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- --- --&gt;
&lt;!-- ### Sex --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data %&gt;%  --&gt;
&lt;!--   count(sex, sort = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- --- --&gt;
&lt;!-- ### Workclass --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data %&gt;%  --&gt;
&lt;!--   count(workclass, sort = TRUE) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data %&gt;%  --&gt;
&lt;!--   count(workclass, sort = TRUE) %&gt;%  --&gt;
&lt;!--   mutate( --&gt;
&lt;!--     workclass2 = fct_collapse( --&gt;
&lt;!--       workclass, --&gt;
&lt;!--       &#34;Private&#34; = &#34;Private&#34;, --&gt;
&lt;!--       &#34;Government&#34; = c(&#34;Local-gov&#34;, &#34;State-gov&#34;, &#34;Federal-gov&#34;), --&gt;
&lt;!--       &#34;Self-Employed&#34; = c(&#34;Self-emp-not-inc&#34;, &#34;Self-emp-inc&#34;), --&gt;
&lt;!--       &#34;Unknown&#34; = &#34;?&#34;, --&gt;
&lt;!--       &#34;No Income&#34; = c(&#34;Without-pay&#34;, &#34;Never-worked&#34;) --&gt;
&lt;!--     ) --&gt;
&lt;!--   ) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data &lt;- all_data %&gt;%  --&gt;
&lt;!--   mutate( --&gt;
&lt;!--     workclass = fct_collapse( --&gt;
&lt;!--       workclass, --&gt;
&lt;!--       &#34;Private&#34; = &#34;Private&#34;, --&gt;
&lt;!--       &#34;Government&#34; = c(&#34;Local-gov&#34;, &#34;State-gov&#34;, &#34;Federal-gov&#34;), --&gt;
&lt;!--       &#34;Self-Employed&#34; = c(&#34;Self-emp-not-inc&#34;, &#34;Self-emp-inc&#34;), --&gt;
&lt;!--       &#34;Unknown&#34; = &#34;?&#34;, --&gt;
&lt;!--       &#34;No Income&#34; = c(&#34;Without-pay&#34;, &#34;Never-worked&#34;) --&gt;
&lt;!--     ) --&gt;
&lt;!--   ) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- --- --&gt;
&lt;!-- ## Plot Em&#39;! --&gt;
&lt;!-- I made a *mapping* function to use with *purrr*&#39;s &lt;hc&gt;map()&lt;/hc&gt; that will automate the graphing of all variables against our $y$ variable, in this case $y = income$. I will not go over this in detail, if there is any questions or you would like to discuss this or any future code contact me on social media (found on the homepage). --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- plot_em &lt;- as_mapper(~ { --&gt;
&lt;!--   main_var &lt;- parse_expr(.y) --&gt;
&lt;!--   if (is.numeric(.x)) { --&gt;
&lt;!--     p &lt;- ggplot(all_data, aes(x = income, y = !!main_var)) + --&gt;
&lt;!--       geom_boxplot() + --&gt;
&lt;!--       labs(x = .y) --&gt;
&lt;!--     p %&gt;% snake_to() --&gt;
&lt;!--   } else { --&gt;
&lt;!--     bar_data &lt;- all_data %&gt;%  --&gt;
&lt;!--       count(income, !!main_var, name = &#34;count&#34;) %&gt;%  --&gt;
&lt;!--       group_by(income) %&gt;%  --&gt;
&lt;!--       mutate(proportion = count / sum(count)) %&gt;%  --&gt;
&lt;!--       ungroup() %&gt;%  --&gt;
&lt;!--       arrange(desc(proportion)) %&gt;%  --&gt;
&lt;!--       mutate( --&gt;
&lt;!--         !!main_var := factor(!!main_var, levels = unique(!!main_var)), --&gt;
&lt;!--         income = factor(income, levels = unique(income)) --&gt;
&lt;!--         ) --&gt;
&lt;!--     p &lt;- ggplot( --&gt;
&lt;!--       data = bar_data, --&gt;
&lt;!--       mapping = aes(x = income, y = proportion, fill = !!main_var) --&gt;
&lt;!--       ) + --&gt;
&lt;!--       geom_col() + --&gt;
&lt;!--       labs(x = .y) --&gt;
&lt;!--     p %&gt;% snake_to() --&gt;
&lt;!--   } --&gt;
&lt;!-- }) --&gt;
&lt;!-- all_plots &lt;- map2(all_data, names(all_data), plot_em) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- &lt;hc&gt;all_plots&lt;/hc&gt; holds all plots in a *list*. We will go through one by one so we can have a deeper understanding of our data. We could use a graphing function such as &lt;hc&gt;pairs()&lt;/hc&gt; or &lt;hc&gt;ggpairs()&lt;/hc&gt;, however, I really wanted to look at all variables one by one closely and I got to practice some dplyr programming ;) --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_plots[[1]] --&gt;
&lt;!-- ``` --&gt;
&lt;!-- On average there are more people that have an income of *&gt;50k* who are older. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_plots[[2]] --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Higher Proportion per Outcome ($y = income$): --&gt;
&lt;!-- - Private: *&lt;=50k* --&gt;
&lt;!-- - Government: *&gt;50k* --&gt;
&lt;!-- - Self-Employed: *&gt;50k* --&gt;
&lt;!-- - Unknown: *&lt;=50k* --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_plots[[3]] --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Higher Proportion per Outcome ($y = income$): --&gt;
&lt;!-- - High School: *&lt;=50k* --&gt;
&lt;!-- - Bachelors: *&gt;50k* --&gt;
&lt;!-- - Some College: *&lt;=50k* --&gt;
&lt;!-- - No Diploma: *&lt;=50k* --&gt;
&lt;!-- - Graduate: *&gt;50k* --&gt;
&lt;!-- - Associates: *About Even* --&gt;
&lt;!-- - Professional School: *&gt;50k* --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_plots[[4]] --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Higher Proportion per Outcome ($y = income$): --&gt;
&lt;!-- - Married: *&gt;50k* --&gt;
&lt;!-- - Never Married: *&lt;=50k* --&gt;
&lt;!-- - Not Married: *&lt;=50k* --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_plots[[5]] --&gt;
&lt;!-- ``` --&gt;
&lt;!-- There are a lot of different occupations so I will point out the obvious points. --&gt;
&lt;!-- Higher Proportion per Outcome ($y = income$): --&gt;
&lt;!-- - Exec-managerial: *&gt;50k* --&gt;
&lt;!-- - Prof-specialty: *&gt;50k* --&gt;
&lt;!-- - Adm-clerical: *&lt;=50k* --&gt;
&lt;!-- - Sales: *&gt;50k* --&gt;
&lt;!-- - Machine-op-inspct: *&lt;=50k* --&gt;
&lt;!-- - Unknown: *&lt;=50k* --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_plots[[6]] --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Higher Proportion per Outcome ($y = income$): --&gt;
&lt;!-- - Husband: *&gt;50k* --&gt;
&lt;!-- - Not-in-family: *&lt;=50k* --&gt;
&lt;!-- - Own-child: *&lt;=50k* --&gt;
&lt;!-- - Unmarried: *&lt;=50k* --&gt;
&lt;!-- - Wife: *&gt;50k* --&gt;
&lt;!-- - Other-relative: *&lt;=50k* --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_plots[[7]] --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Higher Proportion per Outcome ($y = income$): --&gt;
&lt;!-- - White: *&gt;50k* --&gt;
&lt;!-- - Black: *&lt;=50k* --&gt;
&lt;!-- - Asian-Pac-Islander: *&gt;50k* --&gt;
&lt;!-- - Amer-Indian-Eskimo: *&lt;=50k* --&gt;
&lt;!-- - Other: *&lt;=50k* --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_plots[[8]] --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Higher Proportion per Outcome ($y = income$): --&gt;
&lt;!-- - Male: *&gt;50k* --&gt;
&lt;!-- - Female: *&lt;=50k* --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_plots[[9]] --&gt;
&lt;!-- ``` --&gt;
&lt;!-- No comment. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_plots[[10]] --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Also no comment. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_plots[[11]] --&gt;
&lt;!-- ``` --&gt;
&lt;!-- We can see the average between line is around 40 for both *&lt;=50k* and *&gt;50k*. It appears on average more people in the *&gt;50k* group work more hours. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_plots[[14]] --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Mostly equal, Americas has a high N so it is hard to see a signal. --&gt;
&lt;!-- # Modeling --&gt;
&lt;!-- ## Split the Data --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- train &lt;- all_data %&gt;%  --&gt;
&lt;!--   filter(group == &#34;train&#34;) %&gt;%  --&gt;
&lt;!--   select(-group) --&gt;
&lt;!-- test &lt;- all_data %&gt;%  --&gt;
&lt;!--   filter(group == &#34;test&#34;) %&gt;%  --&gt;
&lt;!--   select(-group) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ## Classification Tree --&gt;
&lt;!-- A classification tree uses a decision tree to take features (columns) of our data and comes up with a final value, in our case this the final value is a label. Decision trees can also handle numerical outputs. Generally this method is reffered to as CART, **C**lassifican **A**nd **R**egression **T**rees. --&gt;
&lt;!-- A decision tree has a hierarchrical structure: --&gt;
&lt;!-- - Top: Root Node. This is where all of the data from our features begin their journey --&gt;
&lt;!-- - Bottom: Leaf Nodes. This is the finish line of the tree. Once the data reaches this point we obtain the final value. Either a numeric $y$ value or a label for $y$. --&gt;
&lt;!-- - Middle: Internal Nodes. These are the nodes between the root and leaf nodes. --&gt;
&lt;!-- Advantages of Decision Trees: --&gt;
&lt;!-- - Easy to interprit --&gt;
&lt;!--   - If you can read a flow chart you can read a decision tree --&gt;
&lt;!-- - Training and prediction flows are easy to explain --&gt;
&lt;!-- - Easier to explain than a linear model --&gt;
&lt;!-- - Following the path of the tree allows for full explanation of the data --&gt;
&lt;!-- - Easy to interprit and visualize --&gt;
&lt;!-- - Handles categorical and numerical data with ease --&gt;
&lt;!--   - No dummy data for categorical data --&gt;
&lt;!--   - No need to normalize or transform numeric data --&gt;
&lt;!-- - Missing data? No problem! --&gt;
&lt;!--   - One method to handle missing data is when going down the branch and the value is NA for that feature it will randomly choose left or right and continue onward --&gt;
&lt;!--   - Another method involves going down both branches at the split with missing data and when the leaf nodes are reached you average the leafs values for the final prediction --&gt;
&lt;!-- - Robust to outliers --&gt;
&lt;!-- - Requires little data prep --&gt;
&lt;!-- - Can model non-linearity in the data --&gt;
&lt;!-- - Trains quickly on large data sets --&gt;
&lt;!-- Disadvantages of Decision Trees: --&gt;
&lt;!-- - Large trees can be hard to interpret --&gt;
&lt;!-- - Trees can have high variance --&gt;
&lt;!--   - Causes model performance to be poor --&gt;
&lt;!-- - Trees overfit easily --&gt;
&lt;!-- ### Model Selection --&gt;
&lt;!-- We will use grid search for hyperperameter searching. Hyper perameters are the different knobs and settings we can tune to get the best results from our data. Grid search is an exhaustive and iterative search through a manually defined set of model hyperperameters. --&gt;
&lt;!-- The two parameters we will be iterating over are &lt;hc&gt;minsplit&lt;/hc&gt; and &lt;hc&gt;maxdepth&lt;/hc&gt;. --&gt;
&lt;!-- - &lt;hc&gt;minsplit&lt;/hc&gt;: Minumum number of data points requeired to attempt a split --&gt;
&lt;!-- - &lt;hc&gt;maxdepth&lt;/hc&gt;: Maximum depth of our tree --&gt;
&lt;!-- The goal of the grid search is to train model per row of the grid and then evaluate which model is best. The best model, in this instance, is accuracy of correct predictions. Below we will use some *tidy* methods for training the models. This brilliant flow is thanks to the post located [here](https://drsimonj.svbtle.com/grid-search-in-the-tidyverse)! --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- grid_search &lt;- list( --&gt;
&lt;!--   minsplit = seq(10, 45, 15), --&gt;
&lt;!--   maxdepth = c(1, 5, 10, 25) --&gt;
&lt;!-- ) %&gt;%  --&gt;
&lt;!--   cross_df() --&gt;
&lt;!-- grid_search --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Now that we have our grid we can train a model per row. We expect there to be 12 models at the end. We can define our own function to simply pass the grid to the &lt;hc&gt;control&lt;/hc&gt; parameter of &lt;hc&gt;rpart()&lt;/hc&gt; using &lt;hc&gt;...&lt;/hc&gt;. We will use mutate to add a model per row to the &lt;hc&gt;grid_search&lt;/hc&gt; data frame, --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- dt_mod &lt;- function(...) { --&gt;
&lt;!--   rpart( --&gt;
&lt;!--     formula = income ~ ., --&gt;
&lt;!--     data = train, --&gt;
&lt;!--     control = rpart.control(...) --&gt;
&lt;!--     ) --&gt;
&lt;!-- } --&gt;
&lt;!-- # for reproducability --&gt;
&lt;!-- set.seed(19) --&gt;
&lt;!-- grid_search &lt;- grid_search %&gt;%  --&gt;
&lt;!--   mutate(fit = pmap(grid_search, dt_mod)) --&gt;
&lt;!-- grid_search --&gt;
&lt;!-- ``` --&gt;
&lt;!-- The &lt;hc&gt;fit&lt;/hc&gt; column now holds all 12 of our models. If we print the  model column we can see the normal output of an *rpart* model for all 12 models. --&gt;
&lt;!-- ```{r, class.output = &#34;scroll&#34;} --&gt;
&lt;!-- grid_search$fit --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ### Model Evaluation and Tuning --&gt;
&lt;!-- Following the flow from the post linked above we can easily make make predictions on the test data and check the accuracy. We will seperate the independant variables and the dependent variable from the test data to compute the accuracy per model. We will add the accuracy per model to the &lt;hc&gt;grid_search&lt;/hc&gt; data using &lt;hc&gt;mutate&lt;/hc&gt; and arrange to show the best and most simple model. Simplicity is always preferred! --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- compute_accuracy &lt;- function(fit, test_features, test_labels) { --&gt;
&lt;!--   predicted &lt;- predict( --&gt;
&lt;!--     fit, --&gt;
&lt;!--     test_features, --&gt;
&lt;!--     type = &#34;class&#34; --&gt;
&lt;!--     ) --&gt;
&lt;!--   mean(predicted == test_labels) --&gt;
&lt;!-- } --&gt;
&lt;!-- test_features &lt;- test %&gt;% --&gt;
&lt;!--   select(-income) --&gt;
&lt;!-- test_labels   &lt;- test %&gt;%  --&gt;
&lt;!--   pull(income) --&gt;
&lt;!-- grid_search &lt;- grid_search %&gt;% --&gt;
&lt;!--   mutate( --&gt;
&lt;!--     test_accuracy = map_dbl( --&gt;
&lt;!--       fit, --&gt;
&lt;!--       compute_accuracy, --&gt;
&lt;!--       test_features, --&gt;
&lt;!--       test_labels) --&gt;
&lt;!--     ) %&gt;% --&gt;
&lt;!--   arrange(desc(test_accuracy), minsplit, maxdepth) --&gt;
&lt;!-- grid_search --&gt;
&lt;!-- ``` --&gt;
&lt;!-- We can see the best and most simplistic model has a &lt;hc&gt;minsplit&lt;/hc&gt; of 10 and a &lt;hc&gt;maxdepth&lt;/hc&gt; of 5. A lot of these models appear to split and look at the data in the same way due to all of them having the same accuracy. 84% is a pretty great result for such a simple model! But is it as good as it seems? We will revisit this number at the end of this section. Now we can use the **rpart.plot** library to produce a nice visualize of the model. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- best_rpart &lt;- grid_search %&gt;%  --&gt;
&lt;!--   pull(fit) %&gt;%  --&gt;
&lt;!--   .[[1]] --&gt;
&lt;!-- rpart.plot( --&gt;
&lt;!--   x = best_rpart, --&gt;
&lt;!--   yesno = 2, --&gt;
&lt;!--   type = 0, --&gt;
&lt;!--   extra = 0 --&gt;
&lt;!--   ) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Using this flow we can take a person and their information and make a prediction on how much income they make. This is the power of a tree, we can interpreit the model and how it works every step of the way with ease! --&gt;
&lt;!-- There are a few more things we can do to our model. **Pruning** can possibly reduce the size of the tree (simple is ideal!) without losing and predictive power on classifications. We can do this by looking at the &lt;hc&gt;cp&lt;/hc&gt;, aka *complexity parameter*. --&gt;
&lt;!-- CP is a penalty term that helps control tree size. The smaller the CP the more complex a tree will be. The &lt;hc&gt;rpart()&lt;/hc&gt; function computes the 10-fold cross validation error of the model over various values for CP and stores the results in a table inside the model. We can plot the cross validation error across different values of CP using &lt;hc&gt;plotcp()&lt;/hc&gt;. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- plotcp(best_rpart) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Here we can quickly get an estimate for the omptimal value of CP. To retreieve the optimal CP value we can look at the CP table stored in the model and look where the *xerror* is minimized. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- best_rpart$cptable --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- opt_index &lt;- which.min(best_rpart$cptable[, &#34;xerror&#34;]) --&gt;
&lt;!-- cp_opt &lt;- best_rpart$cptable[opt_index, &#34;CP&#34;] --&gt;
&lt;!-- cp_opt --&gt;
&lt;!-- ``` --&gt;
&lt;!-- With this value we can use &lt;hc&gt;prune()&lt;/hc&gt; to possibly trim our model. &lt;hc&gt;prune()&lt;/hc&gt; will return the optimized model. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- best_rpart_opt &lt;- prune( --&gt;
&lt;!--   tree = best_rpart, --&gt;
&lt;!--   cp = cp_opt --&gt;
&lt;!-- ) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Did this change anything? --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- rpart.plot( --&gt;
&lt;!--   x = best_rpart_opt, --&gt;
&lt;!--   yesno = 2, --&gt;
&lt;!--   type = 0, --&gt;
&lt;!--   extra = 0 --&gt;
&lt;!--   ) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- It appears in this instance, pruning did not change the visual of our model. Now onto the last part of of the evaluation is to look at a confusion matrix. A confusion matrix will show us a more detailed breakdown of correct and incorrect classifications for each class. We will use the **caret** package to produce the matrix. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- class_prediction &lt;- predict( --&gt;
&lt;!--   object = best_rpart_opt, --&gt;
&lt;!--   newdata = test, --&gt;
&lt;!--   type = &#34;class&#34; --&gt;
&lt;!-- ) --&gt;
&lt;!-- caret::confusionMatrix( --&gt;
&lt;!--   data = class_prediction, --&gt;
&lt;!--   reference = as.factor(test$income) --&gt;
&lt;!-- ) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- There is a lot of output to this function. We will focus on the 2x2 table, accuracy, sensitivity, and specificity. --&gt;
&lt;!-- - **Accuracy**: Correct Prediction % --&gt;
&lt;!-- - **Sensitivity**: True Positive Rate --&gt;
&lt;!--   - As noted on the last line of the output, the &#34;positive&#34; factor is *&lt;=50k*. So the decision tree get 95% of the predictions correct when they are positive. Sounds great, right? --&gt;
&lt;!-- - **Specificity**: True Negative Rate --&gt;
&lt;!--   - The decision tree only has a 50% accuracy when it comes to incomes *&gt;50k*. What gives? Lets look at the count in each group. It appears the tree model labels a strong majority of predictions as *&lt;=50k*. Since most of the data belongs to this group and three likes to predict this label, it will have a high accuracy overall. This is why it is important to look at sensitivity and specificity. Our tree is not as great once we look at accuracy per label! --&gt;
&lt;!-- ### Other Accuracy Metrics [WIP] --&gt;
&lt;!-- If accuracy is deceiving at times, what can we use? Great question person reading this. We can use AUC...  --&gt;
&lt;!-- ## Bagging &amp; Random Forests --&gt;
&lt;!-- The drawbacks of decision trees is the high variability. A small change in the data, introduction to new data, or changing the test / training groups can change the model drastically. How can we approach these issues drawbacks? Enter bagging. :) --&gt;
&lt;!-- Bagged trees averages many trees to reduce the variance. Combing multiple models like this, in this instance it is multiple decision trees, is called an **ensemble model**. Another issue bagging helps with is overfitting.  --&gt;
&lt;!-- Bagging is an ensemble method and the term *bagging* is shorthand for **B**ootstrap **AGG**regat**ING**. Bagging uses [bootstrap sampling](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) and aggregates the individual models by averaging. Bootstrap sampling means sampling rows at random from the training dataset with replacement. Bagging also starts with all available features of the data. --&gt;
&lt;!-- &lt;center&gt; --&gt;
&lt;!--   &lt;figure&gt; --&gt;
&lt;!--     &lt;img src=&#34;https://i.imgur.com/JYBMwak.png&#34; /&gt; --&gt;
&lt;!--     &lt;figcaption&gt;&lt;a href=&#34;https://www.datacamp.com/courses/machine-learning-with-tree-based-models-in-r&#34;&gt;Image Source&lt;/a&gt;&lt;/figcaption&gt; --&gt;
&lt;!--   &lt;/figure&gt; --&gt;
&lt;!-- &lt;/center&gt; --&gt;
&lt;!-- With replacement means there is a chance the bootstrap sample will have the same observation more than once. This can also lead some rows be absent. This allows us to have &#34;new&#34; data. By doing this we can fit many different, yet similar, models. --&gt;
&lt;!-- ### Bagging Steps: --&gt;
&lt;!-- **Step 1**: Draw $B$ samples with replacement from the original training set, where B is a number less than or equal to the $N$ ($N = total training rows $). A common choice for $B$ is $\frac{N}{2}$. --&gt;
&lt;!-- **Step 2**: Train a decision tree on the newly created bootstrapped sample.  --&gt;
&lt;!-- **Step 3**: Repeat steps 1 and 2 multiple times. 10, 50, 100, 1000, etc. Typically, the more trees the better. --&gt;
&lt;!-- ### Predicting with Bagging --&gt;
&lt;!-- If we have 1,000 bootstrapped trees that makes up our ensemble model, each bootstrap tree may have different terminal nodes compared to the other. To generate a prediction with this model, the model will make a prediction with all 1,000 trees and then average the predictions together to end up with the final prediciton. Due to bagging averaging the predictions, this will lower the variability and lead to a better performing model. I found a picture that repsents a decision trees prediction versus a bagging ensemble method. Hint: the decision tree is on the left ;) --&gt;
&lt;!-- https://cdn.imgbin.com/17/0/7/imgbin-the-wisdom-of-crowds-wisdom-of-the-crowd-crowdsourcing-others-0Ehm17am7cWP7TMhUGkA7wXjv.jpg --&gt;
&lt;!-- ![](https://cdn.imgbin.com/17/0/7/imgbin-the-wisdom-of-crowds-wisdom-of-the-crowd-crowdsourcing-others-0Ehm17am7cWP7TMhUGkA7wXjv.jpg) --&gt;
&lt;!-- ### What about Random Forests? --&gt;
&lt;!-- The only difference between bagging and random forest is that random forests will use a random subset of the datas features to build the models while bagging uses all features. This can lead to random forest removing a random feature and possibly finding an pattern that was not noticable while using all features. This parameter in functions to fit the models is called &lt;hc&gt;mtry&lt;/hc&gt;. When &lt;hc&gt;mtry&lt;/hc&gt; is equal to the count of independant variables then it is bagging. Anything less than that is random forest. We will fit an example model using the randomForest package for demonstration and when we need to use grid search for parameter selection we will use the **ranger** package (ranger is written in C# and is much faster when scaling up). The model will be fit using the default parameters, aside from mtry for bagging, and to go over the output of the model. --&gt;
&lt;!-- To demonstrate the time difference between fitting models between randomForest and ranger I will use the package **tictoc** to measure the amount of time it takes to run the code and compare them when we transfer to ranger. Note: `randomForest()` requires character columns to be of the factor data type. We will transfrom this into a different data set so we dont change our original data. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- bagging &lt;- ncol(train) - 1 # Do not include the Y variable --&gt;
&lt;!-- train_rf &lt;- train %&gt;%  --&gt;
&lt;!--   mutate_if(is.character, as.factor) --&gt;
&lt;!-- tic() --&gt;
&lt;!-- rf_example &lt;- randomForest( --&gt;
&lt;!--   formula = income ~ ., --&gt;
&lt;!--   data = train_rf, --&gt;
&lt;!--   mtry = bagging, --&gt;
&lt;!--   ntree = 1000 --&gt;
&lt;!-- ) --&gt;
&lt;!-- toc() --&gt;
&lt;!-- tic() --&gt;
&lt;!-- ranger_example &lt;- ranger( --&gt;
&lt;!--   formula = income ~ ., --&gt;
&lt;!--   data = train_rf, --&gt;
&lt;!--   mtry = bagging, --&gt;
&lt;!--   num.trees = 1000, --&gt;
&lt;!--   verbose = FALSE --&gt;
&lt;!-- ) --&gt;
&lt;!-- toc() --&gt;
&lt;!-- ``` --&gt;
&lt;!-- For 1,000 trees it saves us almost 1 minute! To explore random forests and bagging, the `randomForest()` function has nice built in functions for visualizing the model. When we want to find an optimized model we will grid search with **ranger**. --&gt;
&lt;!-- #### randomForest Output --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- rf_example --&gt;
&lt;!-- ``` --&gt;
&lt;!-- When we *print* the model we can see the original call to create the data, the type of random forest, tree count, number of variables per split, OOB estimate, and a confusion matrix. The OOB estimate takes the prediction for each tree. For example, we trained 1,000 trees. The OOB --&gt;
&lt;!-- We can see the error rate as the tree count increases using the &lt;hc&gt;plot()&lt;/hc&gt; function. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- plot(rf_example) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- We can see that around 175 trees the error stops decreasing. This meaning we used too many trees, which translate to too much time spent waiting in terms of computation time. This means we can train a model with a much smaller amount of trees. This will be covered when we do a grid search, but it is nice to visual a model and see how many trees optimizes the error. --&gt;
&lt;!-- ### Random Forest Grid Search --&gt;
&lt;!-- Just like the decision tree we will grid search for the best random forest model. The two parameters we will be tuning are *mtry*, total variables to use, and *num.tree*, the tree count. The choice for *mtry* is a standard way of checking when I was in college. Due to what we saw in the first random forest model, we shouldnt need anywhere near 1000 trees, but it doesn&#39;t hurt to check. Remember, more trees = more computationally expensive for us. By us I mean my computer :) So I will only check a few tree values. Idealy these models will be trained when there is idle time and/or on a computer that can handle training a lot of large models. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- # bagging = total number of independent variables --&gt;
&lt;!-- rf_grid_search &lt;- list( --&gt;
&lt;!--   mtry = unique(ceiling(c( --&gt;
&lt;!--     bagging, --&gt;
&lt;!--     sqrt(bagging), --&gt;
&lt;!--     bagging / 2 --&gt;
&lt;!--     ))), --&gt;
&lt;!--   num.trees = c(100, 300, 500), --&gt;
&lt;!--   splitrule = c(&#34;variance&#34;, &#34;extratrees&#34;, &#34;maxstat&#34;) --&gt;
&lt;!-- ) %&gt;%  --&gt;
&lt;!--   cross_df() %&gt;%  --&gt;
&lt;!--   arrange(mtry, num.trees) --&gt;
&lt;!-- rf_grid_search --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ### RF Model Fitting --&gt;
&lt;!-- Again we will create a model function. This time we will use **ranger** to fit our RF models. The pmap handles passing in the parameter using ...! The other method we could go with is a for loop, however, this is shorter and elegant. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- rf_mod &lt;- function(...) { --&gt;
&lt;!--   ranger(income ~ ., data = train, ...) --&gt;
&lt;!-- } --&gt;
&lt;!-- rf_grid_search &lt;- rf_grid_search %&gt;%  --&gt;
&lt;!--   mutate(fit = pmap(rf_grid_search, rf_mod)) --&gt;
&lt;!-- rf_grid_search --&gt;
&lt;!-- ``` --&gt;
&lt;!-- We now have 9 ranger models. We will look at accuracy. To reiterate, in a real life setting it may be more applicable to use AUC. Below, notice how &lt;hc&gt;predicted&lt;/hc&gt; needs to be set up. Before when we used predict it returned a vector. **ranger**&#39;s predict function returns a list object with **predictions** being one of the list elements. We can use the &lt;hc&gt;pluck()&lt;/hc&gt; function from **purrr** to grab the named element from the list. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- compute_accuracy &lt;- function(fit, test_features, test_labels) { --&gt;
&lt;!--   predicted &lt;- predict(fit, test_features) %&gt;%  --&gt;
&lt;!--     pluck(predictions) --&gt;
&lt;!--   mean(predicted == test_labels) --&gt;
&lt;!-- } --&gt;
&lt;!-- test_features &lt;- test %&gt;% select(-income) --&gt;
&lt;!-- test_labels   &lt;- test$income --&gt;
&lt;!-- rf_grid_search &lt;- rf_grid_search %&gt;% --&gt;
&lt;!--   mutate( --&gt;
&lt;!--     test_accuracy = map_dbl( --&gt;
&lt;!--       fit, --&gt;
&lt;!--       compute_accuracy, --&gt;
&lt;!--       test_features, --&gt;
&lt;!--       test_labels --&gt;
&lt;!--       ) --&gt;
&lt;!--     ) %&gt;% --&gt;
&lt;!--   arrange( --&gt;
&lt;!--     desc(test_accuracy), --&gt;
&lt;!--     mtry,  --&gt;
&lt;!--     num.trees --&gt;
&lt;!--     ) --&gt;
&lt;!-- rf_grid_search --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ### Best Model Evaluation --&gt;
&lt;!-- In terms of overall accuracy the best model has a *mtry* value of 4 and uses a tree count of 300. Much like what we did with the rpart model we can make a confusion matrix to check our best models performance. --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- best_rf_mod &lt;- rf_grid_search %&gt;%  --&gt;
&lt;!--   pull(fit) %&gt;%  --&gt;
&lt;!--   .[[1]] --&gt;
&lt;!-- best_rf_mod --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- features &lt;- test %&gt;% --&gt;
&lt;!--   select(-income) --&gt;
&lt;!-- labels   &lt;- test %&gt;%  --&gt;
&lt;!--   pull(income) --&gt;
&lt;!-- class_prediction &lt;- predict( --&gt;
&lt;!--   best_rf_mod, --&gt;
&lt;!--   test_features --&gt;
&lt;!-- ) %&gt;%  --&gt;
&lt;!--   pluck(predictions) --&gt;
&lt;!-- caret::confusionMatrix( --&gt;
&lt;!--   data = class_prediction, --&gt;
&lt;!--   reference = as.factor(labels) --&gt;
&lt;!-- ) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- Our previous simple model had: --&gt;
&lt;!-- - **Accuracy**: 84.45% --&gt;
&lt;!-- - **Sensitivity**: 94.93% --&gt;
&lt;!-- - **Specificity**: 50.57% --&gt;
&lt;!-- Our best random forest model does better in every category besides sensitivity, where it dropp by ~ 1%. The RF model was able to catch a lot more (13% more!) people who made more than $50,000! Fantastic! This is the power of the ensemble method of random forest, it will build multiple trees with different data and use different features to find different patterns that are used in the final outcome! I find that so awesome. --&gt;
&lt;!-- Now there is still one more tree method to go! This next method isn&#39;t too different from the RF methods. Onto boosted trees! --&gt;
&lt;!-- ## Boosted Trees --&gt;
&lt;!-- another tree based ensemble method --&gt;
&lt;!-- boosting algorithmL gradient boosting machine (GBM) --&gt;
&lt;!-- uses the adaboost algorithm: --&gt;
&lt;!-- - train decision tree with equal weights on all observations --&gt;
&lt;!-- - after evaluation the first tree, the algoritm increases the weights for the observations that it had a hard time classifying and lower the weights for the observations where it was easy to classify. --&gt;
&lt;!-- - the second tree is grown on the weighted data, improve upon the predictions of the first --&gt;
&lt;!-- - new model: tree 1 + tree 2 --&gt;
&lt;!-- - classification error from this new 2-tree ensemble model --&gt;
&lt;!-- - grow 3rd tree to predict the revised residuals --&gt;
&lt;!-- - repeat this process for a specified number of trees --&gt;
&lt;!-- GBM NAME --&gt;
&lt;!-- gradient boosting = gradient descent + boosting --&gt;
&lt;!-- - fit an additive model in a forward stage wise manner --&gt;
&lt;!-- - in each stage the algorithm introduces a *weak learner* (in this situation the first decision tree) to compensate the shortcomings of existing weak learners --&gt;
&lt;!-- - in adaboost, shortcomings are identified by high weight data points --&gt;
&lt;!-- - in gradient boosting, the shortcomings are identified by gradients --&gt;
&lt;!-- Pros n cons --&gt;
&lt;!-- often performs better than any other algorithm if tuned properly --&gt;
&lt;!--  - including deep learning! --&gt;
&lt;!-- directly optimizes cost function --&gt;
&lt;!-- cons --&gt;
&lt;!-- overfits (needs a proper stopping point) --&gt;
&lt;!-- sensitive to extreme values and noise --&gt;
&lt;!-- from other datacamp course: --&gt;
&lt;!-- ensemble method that builds up a model by incrementally improving the existing one. start the model by fitting a single, usually shallow tree to the data, this is $M_1$. next fit a new tree with the residuals of the first model and find the weighted some of that tree, that is $M_2$. For regularized boosting decrease learning by factor $\eta$, between 0 and 1. $\eta$ closer to 1 gives faster learning but increases the risk of overfitting. smaller $\eta$ slows learning but lowers risk of overfitting. repeat until the residuals are small enough or the max number of iter are met. --&gt;
&lt;!-- since gradient boosting optimizes the error on training data, its easy to overfit the model. best practice is to estiamate out of sample error via cross validation for each incremental model. then retroactivly decide how many trees to use.  --&gt;
&lt;!-- &lt;hc&gt;xgb.cv()&lt;/hc&gt; fits a model and calculates the cross validated errors. we will run this function with a high number of trees to get our tree count approximation.  --&gt;
&lt;!-- &lt;hc&gt;xgb.cv()&lt;/hc&gt; records the estimated errors in the &lt;hc&gt;evaluation_log&lt;/hc&gt; element of its output. we can use the &lt;hc&gt;evaluation_log&lt;/hc&gt; to find the number of trees with the lowest estimated error, called $n_{best}$ --&gt;
&lt;!-- run &lt;hc&gt;xgboost()&lt;/hc&gt; setting &lt;hc&gt;nrounds = $n_{best}$&lt;/hc&gt;  --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- all_data &lt;- all_data %&gt;%  --&gt;
&lt;!--   mutate(income = as.numeric(as.factor(income)) - 1) --&gt;
&lt;!-- dummy_data &lt;- dummyVars(income ~ ., all_data, fullRank = T, sep = &#34;_&#34;) %&gt;%  --&gt;
&lt;!--   predict(., newdata = all_data) %&gt;%  --&gt;
&lt;!--   as.data.frame() --&gt;
&lt;!-- train_x &lt;- dummy_data %&gt;%  --&gt;
&lt;!--   filter(grouptrain == 1) %&gt;%  --&gt;
&lt;!--   select(-grouptrain) %&gt;%  --&gt;
&lt;!--   as.matrix() --&gt;
&lt;!-- train_y &lt;- all_data %&gt;%  --&gt;
&lt;!--   filter(group == &#34;train&#34;) %&gt;%  --&gt;
&lt;!--   pull(income) --&gt;
&lt;!-- test_x &lt;- dummy_data %&gt;%  --&gt;
&lt;!--   filter(grouptrain == 0) %&gt;%  --&gt;
&lt;!--   select(-grouptrain) %&gt;%  --&gt;
&lt;!--   as.matrix() --&gt;
&lt;!-- test_y &lt;- all_data %&gt;%  --&gt;
&lt;!--   filter(group == &#34;test&#34;) %&gt;%  --&gt;
&lt;!--   pull(income) --&gt;
&lt;!-- head(train_x, 20) %&gt;%  --&gt;
&lt;!--   datatable( --&gt;
&lt;!--     options = list( --&gt;
&lt;!--       paging = TRUE, searching = FALSE, info = FALSE, --&gt;
&lt;!--       sort = TRUE, scrollX = TRUE --&gt;
&lt;!--     ) --&gt;
&lt;!--   ) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- library(xgboost) --&gt;
&lt;!-- train_data &lt;- xgb.DMatrix(train_x, label = train_y) --&gt;
&lt;!-- tree_count &lt;- 500 --&gt;
&lt;!-- xgb_grid_search &lt;- list( --&gt;
&lt;!--   eta = c(.1, .3), --&gt;
&lt;!--   max_depth = c(6, 8), --&gt;
&lt;!--   colsample_bytree = c(0.8, 1.0), --&gt;
&lt;!--   subsample = c(0.75, 1.0) --&gt;
&lt;!-- ) %&gt;%  --&gt;
&lt;!--   cross_df() --&gt;
&lt;!-- xgb_mod &lt;- function(...) { --&gt;
&lt;!--   xgb.cv( --&gt;
&lt;!--     data = train_data, --&gt;
&lt;!--     nrounds = tree_count, --&gt;
&lt;!--     nfold = 10, --&gt;
&lt;!--     early_stopping_rounds = 10, --&gt;
&lt;!--     params = list( --&gt;
&lt;!--       objective = &#34;binary:logistic&#34;, --&gt;
&lt;!--       ... --&gt;
&lt;!--     ), --&gt;
&lt;!--     verbose = FALSE --&gt;
&lt;!--   ) --&gt;
&lt;!-- } --&gt;
&lt;!-- xgb_grid_search &lt;- xgb_grid_search %&gt;%  --&gt;
&lt;!--   mutate(fit = pmap(xgb_grid_search, xgb_mod)) --&gt;
&lt;!-- beep(&#34;complete&#34;) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- compute_accuracy &lt;- function(fit, test_features, test_labels) { --&gt;
&lt;!--   predicted &lt;- predict(fit, test_features) --&gt;
&lt;!--   mean(predicted == test_labels) --&gt;
&lt;!-- } --&gt;
&lt;!-- test_features &lt;- test %&gt;% select(-income) --&gt;
&lt;!-- test_labels   &lt;- test$income --&gt;
&lt;!-- rf_grid_search &lt;- rf_grid_search %&gt;% --&gt;
&lt;!--   mutate( --&gt;
&lt;!--     test_accuracy = map_dbl( --&gt;
&lt;!--       fit, --&gt;
&lt;!--       compute_accuracy, --&gt;
&lt;!--       test_features, --&gt;
&lt;!--       test_labels --&gt;
&lt;!--       ) --&gt;
&lt;!--     ) %&gt;% --&gt;
&lt;!--   arrange( --&gt;
&lt;!--     desc(test_accuracy), --&gt;
&lt;!--     mtry,  --&gt;
&lt;!--     num.trees --&gt;
&lt;!--     ) --&gt;
&lt;!-- elog &lt;- fit$evaluation_log --&gt;
&lt;!-- elog %&gt;%  --&gt;
&lt;!--   summarize( --&gt;
&lt;!--     ntrees_train = which.min(train_error_mean), --&gt;
&lt;!--     ntrees_test = which.min(test_error_mean) --&gt;
&lt;!--   ) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- x_mod &lt;- xgboost( --&gt;
&lt;!--   data = xgb.DMatrix(train_x, label = train_y), --&gt;
&lt;!--   nrounds = 119, --&gt;
&lt;!--   params = list( --&gt;
&lt;!--     objective = &#34;binary:logistic&#34;, --&gt;
&lt;!--     eta = 0.1, --&gt;
&lt;!--     max_depth = 6 --&gt;
&lt;!--   ) --&gt;
&lt;!-- ) --&gt;
&lt;!-- all_data2 &lt;- all_data %&gt;%  --&gt;
&lt;!--   filter(group == &#34;test&#34;) %&gt;%  --&gt;
&lt;!--   mutate(pred = as.numeric(predict(x_mod, xgb.DMatrix(test_x, label = test_y)) &gt; 0.5)) --&gt;
&lt;!-- mean(all_data2$income == all_data2$pred) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- caret::confusionMatrix( --&gt;
&lt;!--   all_data2$pred %&gt;% as.factor(), --&gt;
&lt;!--   all_data2$income %&gt;% as.factor() --&gt;
&lt;!--   )  --&gt;
&lt;!-- ``` --&gt;
&lt;!-- ```{r} --&gt;
&lt;!-- xgb_grid_search &lt;- list( --&gt;
&lt;!--   eta = (2:5)/tree_count, --&gt;
&lt;!--   max_depth = c(4, 6), --&gt;
&lt;!--   colsample_bytree = c(0.8, 1.0), --&gt;
&lt;!--   subsample = c(0.75, 1.0) --&gt;
&lt;!-- ) %&gt;%  --&gt;
&lt;!--   cross_df() --&gt;
&lt;!-- nrounds &lt;- 1000 --&gt;
&lt;!-- tune_grid &lt;- expand.grid( --&gt;
&lt;!--   nrounds = 1000, --&gt;
&lt;!--   eta = c(0.025, 0.05, 0.1, 0.3), --&gt;
&lt;!--   max_depth = c(2, 3, 4, 5, 6), --&gt;
&lt;!--   gamma = 0, --&gt;
&lt;!--   colsample_bytree = 1, --&gt;
&lt;!--   min_child_weight = 1, --&gt;
&lt;!--   subsample = 1 --&gt;
&lt;!-- ) --&gt;
&lt;!-- tune_control &lt;- caret::trainControl( --&gt;
&lt;!--   method = &#34;cv&#34;, # cross-validation --&gt;
&lt;!--   number = 10, # with n folds  --&gt;
&lt;!--   #index = createFolds(tr_treated$Id_clean), # fix the folds --&gt;
&lt;!--   verboseIter = TRUE, # no training log --&gt;
&lt;!--   allowParallel = TRUE, # FALSE for reproducible results, --&gt;
&lt;!--   classProbs = TRUE --&gt;
&lt;!-- ) --&gt;
&lt;!-- train_y &lt;- all_data %&gt;%  --&gt;
&lt;!--   filter(group == &#34;train&#34;) %&gt;% --&gt;
&lt;!--   pull(income) %&gt;%  --&gt;
&lt;!--   recode(&#34;&lt;=50K&#34; = &#34;LTOE_50k&#34;, &#34;&gt;50K&#34; = &#34;GT_50k&#34;) %&gt;% --&gt;
&lt;!--   as.factor() --&gt;
&lt;!-- xgb_tune &lt;- caret::train( --&gt;
&lt;!--   x = train_x, --&gt;
&lt;!--   y = train_y, --&gt;
&lt;!--   trControl = tune_control, --&gt;
&lt;!--   tuneGrid = tune_grid, --&gt;
&lt;!--   method = &#34;xgbTree&#34;, --&gt;
&lt;!--   verbose = TRUE --&gt;
&lt;!-- ) --&gt;
&lt;!-- ``` --&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Neural Networks</title>
      <link>/2019/2019-06-15-introduction-to-neural-networks/</link>
      <pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/2019-06-15-introduction-to-neural-networks/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://use.fontawesome.com/releases/v5.8.1/css/all.css&#34; integrity=&#34;sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;follow-me-on&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Follow Me On&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/KoderKow&#34;&gt; &lt;i class=&#39;fab fa-twitter fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://www.instagram.com/koderkow/&#34;&gt; &lt;i class=&#39;fab fa-instagram fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KoderKow/&#34;&gt;&lt;i class=&#39;fab fa-github fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://koderkow.rbind.io&#34;&gt;&lt;i class=&#39;fas fa-globe fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-are-neural-networks&#34;&gt;What Are Neural Networks?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#structure-of-a-neuron&#34;&gt;Structure of a Neuron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#visual-representation-of-neural-networks&#34;&gt;Visual Representation of Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#layers-of-a-neural-network&#34;&gt;Layers of a Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feedforward-and-feedback-neural-networks&#34;&gt;Feedforward and Feedback Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#neural-network-application---the-faraway-way&#34;&gt;Neural Network Application - The Faraway Way&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#examine-the-estimated-weights&#34;&gt;Examine the Estimated Weights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#drawbacks-of-a-neural-network-model&#34;&gt;Drawbacks of a Neural Network Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weight-interpretation&#34;&gt;Weight Interpretation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#improving-the-fit&#34;&gt;Improving the Fit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#demonstration-wrap-up&#34;&gt;Demonstration Wrap-Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#final-model-fit&#34;&gt;Final Model Fit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;goal&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Goal&lt;/h1&gt;
&lt;p&gt;This post was first built as a Xaringan presentation for the final in the Contemporary Regression course at IUPUI (Indiana University Purdue University Indianapolis) which is part of the amazing Health Data Science program. I enjoyed making the presentation (link below) so much that I wanted to transfer the information I gathered into a blog post. I hope this serves as a launching point for those who have not had an opportunity to work with basic neural networks.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;strong&gt;&lt;a href=&#34;http://bit.ly/intro-to-neural-networks&#34; class=&#34;uri&#34;&gt;http://bit.ly/intro-to-neural-networks&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-are-neural-networks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What Are Neural Networks?&lt;/h1&gt;
&lt;p&gt;A Neural network, or Artificial Neural Network, is a set of algorithms, modeled loosely after the human brain to help recognize patterns. The brain has about &lt;span class=&#34;math inline&#34;&gt;\(1.5 \times 10^{10}\)&lt;/span&gt; neurons each with 10 to 104 connections called synapses. The speed of messages between neurons is about 100 m/sec which is much slower than CPU speed. The human brain’s fastest reaction time is around 100 ms. A neuron computation time is 1–10 ms. Computation (to no surprise) is 10 times faster! That is just for one simple task!&lt;/p&gt;
&lt;p&gt;The original idea behind neural networks was to use a computer-based model of the human brain. We can recognize people in fractions of a second, but computers find this task difficult. So why not make the software more like the human brain? The brain model of connected neurons, first suggested by &lt;a href=&#34;http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html&#34;&gt;McCulloch and Pitts (1943)&lt;/a&gt;, is too simplistic given more recent research.&lt;/p&gt;
&lt;p&gt;As with artificial intelligence and the sentient takeover, the promise of neural networks is not matched by the reality of their performance. At least for now…&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img class=&#34;lazyload&#34; src=&#34;https://img.buzzfeed.com/buzzfeed-static/static/2015-04/1/17/enhanced/webdr07/anigif_enhanced-29933-1427925503-3.gif&#34; width=40% /&gt;
&lt;figcaption&gt;
&lt;a href=&#34;https://www.buzzfeed.com/norbertobriceno/01101010101001001&#34;&gt;Image Source&lt;/a&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;Neural networks have various purposes such as biological models, hardware implementation for adaptive control and many more! We are interested in the data analysis application of neural network; classification, clustering methods, regression methods.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-a-neuron&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Structure of a Neuron&lt;/h1&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img class=&#34;lazyload&#34; src=&#34;https://3c1703fe8d.site.internapcdn.net/newman/csz/news/800/2018/2-whyareneuron.jpg&#34; width=80% /&gt;
&lt;figcaption&gt;
&lt;a href=&#34;https://medicalxpress.com/news/2018-07-neuron-axons-spindly-theyre-optimizing.html&#34;&gt;Image Source&lt;/a&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Dendrites&lt;/em&gt; receive signals&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Cell body&lt;/em&gt; sums up the inputs of the signals to generate output&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Axon terminals&lt;/em&gt; is the final output&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;visual-representation-of-neural-networks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visual Representation of Neural Networks&lt;/h1&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img class=&#34;lazyload&#34; src=&#34;https://cdn-images-1.medium.com/max/1600/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg&#34; width=40% /&gt;
&lt;figcaption&gt;
&lt;a href=&#34;https://cdn-images-1.medium.com/max/1600/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg&#34;&gt;Image Source&lt;/a&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;Here we can see how a neural network resembles a neuron. Neural networks are collections of thousands of these simple processing units that together perform useful computations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Inputs &lt;span class=&#34;math inline&#34;&gt;\(x_1, x_2, \dots, x_n\)&lt;/span&gt;&lt;/strong&gt;: independent variables&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Weights &lt;span class=&#34;math inline&#34;&gt;\(w_1, w_2, \dots, w_n\)&lt;/span&gt;:&lt;/strong&gt; learns the weights from the data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bias &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;:&lt;/strong&gt; the intercept&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Activation Function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;:&lt;/strong&gt; defines the output of the neuron
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Identity Function:&lt;/em&gt; linear fit&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Sigmoid Function:&lt;/em&gt; logistic fit, where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is binary&lt;/li&gt;
&lt;li&gt;&lt;em&gt;ReLu (rectified linear fit):&lt;/em&gt; linear fit, outputs 0 for negative values&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;layers-of-a-neural-network&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Layers of a Neural Network&lt;/h1&gt;
&lt;center&gt;
&lt;img class=&#34;lazyload&#34; src=&#34;https://i.stack.imgur.com/Kc50L.jpg&#34; width = 40%/&gt;
&lt;/center&gt;
&lt;p&gt;&lt;strong&gt;Input Layer:&lt;/strong&gt; the raw data, think of each “node” as a variable in our data
&lt;strong&gt;Hidden Layer:&lt;/strong&gt; this is where the “black magic” happens&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each layer is focused on learning about the data&lt;/li&gt;
&lt;li&gt;We can think about each layer is learning about an aspect of the data&lt;/li&gt;
&lt;li&gt;Larger and more complex data may require multiple hidden layers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Output Layer:&lt;/strong&gt; the final output. This is generally a single output of the input(s)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;feedforward-and-feedback-neural-networks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Feedforward and Feedback Neural Networks&lt;/h1&gt;
&lt;div class=&#34;column-left&#34;&gt;
&lt;h3&gt;
Feedforward
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Signal goes from input to output&lt;/li&gt;
&lt;li&gt;No loops
&lt;br&gt;&lt;br&gt;
&lt;center&gt;
&lt;img class=&#34;lazyload&#34; src=&#34;https://thumbs.gfycat.com/EnviousNiftyCorydorascatfish-size_restricted.gif&#34; width=&#34;450px&#34; height=&#34;250px&#34;/&gt;
&lt;/center&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;column-right&#34;&gt;
&lt;h3&gt;
Feedback
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The neural network is recursive&lt;/li&gt;
&lt;li&gt;Data loops; goes in both directions
&lt;br&gt;&lt;br&gt;
&lt;center&gt;
&lt;img class=&#34;lazyload&#34; src=&#34;https://thumbs.gfycat.com/MiniatureDependentCob-size_restricted.gif&#34; width=&#34;450px&#34; height=&#34;250px&#34;/&gt;
&lt;/center&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=aircAruvnKk&#34;&gt;Image Source&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;neural-network-application---the-faraway-way&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Neural Network Application - The Faraway Way&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# libaries
library(nnet)
library(tidyverse)
library(ggthemes)
library(glue)
library(plotly)
library(kowr)
data(ozone, package = &amp;quot;faraway&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will start with three variables from the &lt;a href=&#34;https://cran.r-project.org/web/packages/faraway/faraway.pdf&#34;&gt;ozone data set from the faraway package&lt;/a&gt; for demonstrative purposes. We fit a feed-forward neural network with one hidden layer containing two units with a linear output unit:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2019)
nnet_model &amp;lt;- nnet(
  formula = O3 ~ temp + ibh + ibt,
  data = ozone,
  size = 2,
  linout = TRUE,
  trace = FALSE
  )&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nnet()&lt;/code&gt; fits a single-hidden-layer neural network&lt;/li&gt;
&lt;li&gt;&lt;code&gt;formula = O3 ~ temp + ibh + ibt&lt;/code&gt;: formula interface&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data = ozone&lt;/code&gt;: data where the formula variables reside&lt;/li&gt;
&lt;li&gt;&lt;code&gt;size = 2&lt;/code&gt;: number of neurons in the hidden layer (this can be optimized)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;linout = TRUE&lt;/code&gt;: tells the model that it will have lienar output units&lt;/li&gt;
&lt;li&gt;&lt;code&gt;trace = FALSE&lt;/code&gt;: hides the printed out optimization information&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;neural-network-application&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Neural Network Application&lt;/h1&gt;
&lt;p&gt;If you repeat this, your result may differ slightly because of the random starting point of the algorithm, but you will likely get a similar result.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## RSS Value: 21099.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The RSS of 21099.4 is almost equal to &lt;span class=&#34;math inline&#34;&gt;\(\sum_i(y_i - \hat{y})^2\)&lt;/span&gt;, so the fit is not any better than the null model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum((ozone$O3 - mean(ozone$O3))^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 21115.41&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem lies with the initial selection of weights. It is hard to do this well when the variables have very different scales.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scale_ozone &amp;lt;- scale(ozone)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Due to the random starting point, the algorithm uses it may not actually converge. We will fit the model 100 times and pick the one that has the lowest RSS. In theory, this will choose a random starting point that leads to the true minimum.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2019)

## fit 100 nn models
results &amp;lt;- 1:100 %&amp;gt;%
  map(~nnet(
    formula = O3 ~ temp + ibh + ibt,
    data = scale_ozone,
    size = 2,
    linout = TRUE,
    trace = FALSE))

## get the index of the model with the lowest RSS
best_model_index &amp;lt;- results %&amp;gt;%
  map_dbl(~.x$value) %&amp;gt;%
  which.min()

## select best model
best_nn &amp;lt;- results[[best_model_index]]&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;/center&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_nn&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## a 3-2-1 network with 11 weights
## inputs: temp ibh ibt 
## output(s): O3 
## options were - linear output units&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Best RSS Value: 89.0786&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our &lt;code&gt;best_nn&lt;/code&gt; model has 11 parameters or weights (The parameters are shown below). For each of the parameters, there is an optimization that occurs. The surface optimization problem has multiple peaks and valleys. The model can converge on one of these minimums. This is why we run our model 100 times to test out multiple random starting points for our model, to hopefully find the global minimum!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;examine-the-estimated-weights&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Examine the Estimated Weights&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(best_nn)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## a 3-2-1 network with 11 weights
## options were - linear output units 
##  b-&amp;gt;h1 i1-&amp;gt;h1 i2-&amp;gt;h1 i3-&amp;gt;h1 
##  -1.14   0.95  -0.83  -0.28 
##  b-&amp;gt;h2 i1-&amp;gt;h2 i2-&amp;gt;h2 i3-&amp;gt;h2 
##  35.90 -18.32  63.10  34.91 
##   b-&amp;gt;o  h1-&amp;gt;o  h2-&amp;gt;o 
##  -1.83   4.51   0.69&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;: bias&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(i_x\)&lt;/span&gt;: input where &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the index of the variable&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(h_y\)&lt;/span&gt;: hidden neuron where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the index of the hidden neuron&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(o\)&lt;/span&gt;: output&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(i_1 \rightarrow h_1\)&lt;/span&gt;: refers to the link between input 1 and the first hidden neuron&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b \rightarrow o\)&lt;/span&gt;: is a one skip-layer connection, from the bias to the output&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;drawbacks-of-a-neural-network-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Drawbacks of a Neural Network Model&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Parameters are uninterpretable&lt;/li&gt;
&lt;li&gt;Not based on a probability model that expresses the structure and variation
&lt;ul&gt;
&lt;li&gt;No standard errors&lt;/li&gt;
&lt;li&gt;Some inference is possible with bootstrapping&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can get an &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; estimate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 - best_nn$value / sum((scale_ozone[, 1] - mean(scale_ozone[, 1]))^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7292443&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is similar to the additive model fit for these predictors that Faraway fits in previous chapters of his &lt;a href=&#34;https://www.amazon.com/Extending-Linear-Model-Generalized-Nonparametric/dp/149872096X&#34;&gt;book&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weight-interpretation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Weight Interpretation&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-06-15-introduction-to-neural-networks_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although the neural network weights may be difficult to interpret, we can get some sense of the effect of the predictors by observing the marginal effect of changes in one or more predictor as other predictors are held fixed. Here, we vary each predictor individually while keeping the other predictors fixed at their mean values. Because the data has been centered and scaled for the neural network fitting, we need to restore the original scales. As seen in the plots there are large discontinuities in the lines plots. This does not follow the linear trend we are expecting. Looking back at the weights of &lt;code&gt;summary(best_nn)&lt;/code&gt; we can see that some weights have extremely large values despite the scaling of the data, &lt;span class=&#34;math inline&#34;&gt;\(i_2 \rightarrow h_2 = 63.10\)&lt;/span&gt;. This means there is a lot of variability in this neuron. This is analogous to the collinearity problem in linear regression. The neural network is choosing these large values to optimize the fit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;improving-the-fit&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Improving the Fit&lt;/h1&gt;
&lt;p&gt;We can use a penalty function, as with smoothing splines, to obtain a more stable fit. Instead of minimizing MSE, we minimize: &lt;span class=&#34;math inline&#34;&gt;\(MSE + \lambda \sum\limits_{i} w_i^2\)&lt;/span&gt;. We can introduce a &lt;em&gt;weight decay&lt;/em&gt; to our neural network, this is a similar approach we take with ridge regression. Lets set &lt;span class=&#34;math inline&#34;&gt;\(\lambda = 0.001\)&lt;/span&gt; and create 100 models again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2019)
## fit 100 nn models
results_decay &amp;lt;- 1:100 %&amp;gt;%
  map(~nnet(
    formula = O3 ~ temp + ibh + ibt,
    data = scale_ozone,
    size = 2,
    linout = TRUE,
    trace = FALSE,
    `decay = 0.001`))

## get the index of the model with the lowest RSS
best_decay_index &amp;lt;- results_decay %&amp;gt;%
  map_dbl(~.x$value) %&amp;gt;%
  which.min()

## select best model
best_decay &amp;lt;- results[[best_decay_index]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_decay$value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 91.8121&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our previous value was 89.0786, our new RSS is a little bit higher. This is expected because we are sacrificing some of the fit for a more stable result.&lt;/p&gt;
&lt;div class=&#34;column-left&#34;&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-06-15-introduction-to-neural-networks_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;column-right&#34;&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img class=&#34;lazyload&#34; src=&#34;https://media1.tenor.com/images/154e8427624e163c030970a795b6f169/tenor.gif?itemid=5143620&#34; /&gt;
&lt;figcaption&gt;
&lt;a href=&#34;https://tenor.com/&#34;&gt;Image Source&lt;/a&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;br&gt;&lt;br&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;demonstration-wrap-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Demonstration Wrap-Up&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(best_decay)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## a 3-2-1 network with 11 weights
## options were - linear output units 
##  b-&amp;gt;h1 i1-&amp;gt;h1 i2-&amp;gt;h1 i3-&amp;gt;h1 
##   1.16  -0.63   0.42  -0.44 
##  b-&amp;gt;h2 i1-&amp;gt;h2 i2-&amp;gt;h2 i3-&amp;gt;h2 
##  13.09   2.46   8.71  -3.30 
##  b-&amp;gt;o h1-&amp;gt;o h2-&amp;gt;o 
##  1.55 -3.93  1.28&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The weights of the second row are not as extreme now. There is not a way to assess the significance of any of the variables. Neural networks do have interactions built in and these can be observed by the method we used before by varying two variables in our model at a time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-model-fit&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Final Model Fit&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2019)

## fit 100 nn models
results &amp;lt;- 1:100 %&amp;gt;%
  map(~nnet(
    formula = O3 ~ .,
    data = scale_ozone,
    size = 4,
    linout = TRUE,
    trace = FALSE))

## get the index of the model with the lowest RSS
best_model_index &amp;lt;- results_decay %&amp;gt;%
  map_dbl(~.x$value) %&amp;gt;%
  which.min()

## select best model
best_model &amp;lt;- results[[best_model_index]]
best_model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## a 9-4-1 network with 45 weights
## inputs: vh wind humidity temp ibh dpg ibt vis doy 
## output(s): O3 
## options were - linear output units&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(best_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## a 9-4-1 network with 45 weights
## options were - linear output units 
##  b-&amp;gt;h1 i1-&amp;gt;h1 i2-&amp;gt;h1 i3-&amp;gt;h1 i4-&amp;gt;h1 i5-&amp;gt;h1 i6-&amp;gt;h1 i7-&amp;gt;h1 i8-&amp;gt;h1 i9-&amp;gt;h1 
##  -1.47   0.29  -0.36   0.40   0.35  -0.29   0.49   0.29   0.15  -0.02 
##  b-&amp;gt;h2 i1-&amp;gt;h2 i2-&amp;gt;h2 i3-&amp;gt;h2 i4-&amp;gt;h2 i5-&amp;gt;h2 i6-&amp;gt;h2 i7-&amp;gt;h2 i8-&amp;gt;h2 i9-&amp;gt;h2 
##  40.19   6.03  15.36 -17.61 -13.86  17.07 -22.75   7.20  -6.12 -22.51 
##  b-&amp;gt;h3 i1-&amp;gt;h3 i2-&amp;gt;h3 i3-&amp;gt;h3 i4-&amp;gt;h3 i5-&amp;gt;h3 i6-&amp;gt;h3 i7-&amp;gt;h3 i8-&amp;gt;h3 i9-&amp;gt;h3 
## -11.16  -8.76  11.96   3.34   8.17  -1.90 -11.21  17.59 -25.30  -5.20 
##  b-&amp;gt;h4 i1-&amp;gt;h4 i2-&amp;gt;h4 i3-&amp;gt;h4 i4-&amp;gt;h4 i5-&amp;gt;h4 i6-&amp;gt;h4 i7-&amp;gt;h4 i8-&amp;gt;h4 i9-&amp;gt;h4 
##  38.72   6.96  13.35 -22.89 -34.75   9.49   5.97   7.99  -4.86   1.02 
##   b-&amp;gt;o  h1-&amp;gt;o  h2-&amp;gt;o  h3-&amp;gt;o  h4-&amp;gt;o 
##  -1.47   3.60   1.03   0.53  -0.62&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; estimate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 - best_model$value / sum((scale_ozone[,1] - mean(scale_ozone))^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8314248&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Neural networks cannot be used for inference&lt;/li&gt;
&lt;li&gt;Flexible, Easy to fit large complex data&lt;/li&gt;
&lt;li&gt;Can be easily overfit&lt;/li&gt;
&lt;li&gt;Truly a “black box”, plots only give a rough idea of what is happening with our data&lt;/li&gt;
&lt;li&gt;Lacks the diagnostics, model selection, and theory&lt;/li&gt;
&lt;li&gt;Initially developed to address real-life issues, not statistical issues&lt;/li&gt;
&lt;li&gt;“Neural networks can outperform their statistical competitors for some problems provided they are carefully used. However, one should not be fooled by the evocative name, as neural networks are just another tool in the box.” (Faraway, 2016)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;thanks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Thanks!&lt;/h1&gt;
&lt;div id=&#34;slides&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Slides&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Slides&lt;/strong&gt;: &lt;em&gt;&lt;a href=&#34;http://bit.ly/intro-to-neural-networks&#34; class=&#34;uri&#34;&gt;http://bit.ly/intro-to-neural-networks&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Source Code&lt;/strong&gt;: &lt;em&gt;&lt;a href=&#34;https://github.com/KoderKow/intro-to-neural-networks&#34; class=&#34;uri&#34;&gt;https://github.com/KoderKow/intro-to-neural-networks&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;All Posts Source Code&lt;/strong&gt;: &lt;em&gt;&lt;a href=&#34;https://github.com/KoderKow/personal-site/tree/master/content/posts&#34; class=&#34;uri&#34;&gt;https://github.com/KoderKow/personal-site/tree/master/content/posts&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;References&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html&#34;&gt;McCulloch and Pitts (1943)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/community/tutorials/neural-network-models-r&#34;&gt;DataCamp: Neural Network Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/35345191/what-is-a-layer-in-a-neural-network&#34;&gt;Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Extending-Linear-Model-Generalized-Nonparametric/dp/149872096X&#34;&gt;Faraway: Extending the Linear Model with R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/yihui/xaringan&#34;&gt;R Package Used for Slides: Xaringan&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Fri, 10 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>

&lt;h1 id=&#34;hello&#34;&gt;Hello!&lt;/h1&gt;

&lt;p&gt;My name is Kyle Harris. I am a data analytics associate at CLA (CliftonLarsonAllen). I have a double major in Health Data Science and Informatics. My educational interests include solving data related issues, R programming, and machine learning.&lt;/p&gt;

&lt;p&gt;My personal hobbies include spending time with my girlfriend Lexi, petting my cats, playing video games, studying/reading about data science and machine learning, and listening to audio books.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Indy Civic Food Security Hackathon</title>
      <link>/2018/2018-07-03-indy-civic-food-security-hackathon/</link>
      <pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/2018-07-03-indy-civic-food-security-hackathon/</guid>
      <description>


&lt;div id=&#34;follow-me-on&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Follow Me On&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/KoderKow&#34;&gt; &lt;i class=&#39;fab fa-twitter fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://www.instagram.com/koderkow/&#34;&gt; &lt;i class=&#39;fab fa-instagram fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KoderKow/&#34;&gt;&lt;i class=&#39;fab fa-github fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://koderkow.rbind.io&#34;&gt;&lt;i class=&#39;fas fa-globe fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;indy-civic-food-security-hackathon&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Indy Civic Food Security Hackathon&lt;/h1&gt;
&lt;p&gt;This will be a short post (Hopefully a header image soon. Ha!). This is more of a reference to show what my friends and I from Health Data Science were able to build in 24 hours at the Indy Civic Food Security Hackathon! Link to their site is below.&lt;/p&gt;
&lt;p&gt;Our App was made with Rshiny to help show nearby food options for those in need. It will show U-Picks (open gardens), SNAP certified grocery stores, and farmers markets. We also put bus stops in to help with transportation needs.&lt;/p&gt;
&lt;p&gt;Go to the website &lt;a href=&#34;https://sugarshoedev.shinyapps.io/foodhackathon/&#34;&gt;here&lt;/a&gt; and you can enable the markers on the map by clicking the top right box to bring up a menu. There is also an option to get your current location as well as a voice assistance for those who need help using the application. There is one command; ‘ZIP’. If you want you can enable microphone access and say a zipcode in indianapolis you are interested in. Ex: “ZIP 46202”. This will result in the mapview to change to that zip code.&lt;/p&gt;
&lt;p&gt;There is also a skeleton logic of snap questions on the other tab we made to show the power and multiple options that can be implemented with RShiny.&lt;/p&gt;
&lt;p&gt;Again, this is a quick write up to share this great app we made! Shout out to Dan, Becca, and Satirios on this amazing application, and thank you to Indy Chamber for hosting this event!&lt;/p&gt;
&lt;p&gt;Links:
&lt;a href=&#34;https://indychamber.com/hack/&#34; class=&#34;uri&#34;&gt;https://indychamber.com/hack/&lt;/a&gt;
&lt;a href=&#34;https://sugarshoedev.shinyapps.io/foodhackathon/&#34; class=&#34;uri&#34;&gt;https://sugarshoedev.shinyapps.io/foodhackathon/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Making the Transcribing Process a Bit Simpler</title>
      <link>/2018/2018-06-10-making-the-transcribing-process-a-bit-simpler/</link>
      <pubDate>Sun, 10 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/2018-06-10-making-the-transcribing-process-a-bit-simpler/</guid>
      <description>


&lt;div id=&#34;follow-me-on&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Follow Me On&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/KoderKow&#34;&gt; &lt;i class=&#39;fab fa-twitter fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://www.instagram.com/koderkow/&#34;&gt; &lt;i class=&#39;fab fa-instagram fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KoderKow/&#34;&gt;&lt;i class=&#39;fab fa-github fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://koderkow.rbind.io&#34;&gt;&lt;i class=&#39;fas fa-globe fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro&lt;/h1&gt;
&lt;p&gt;For the past year I have had the pleasure of working with the data visualization team at IUPUI. I approached Professor Reda looking to explore the realm of research. He gave me the opportunity to sit in their team meetings for the spring semester. As fall came around I joined the team.&lt;/p&gt;
&lt;p&gt;For those who have worked in research, you may have crossed paths with user testing and the art of transcription. This was my first encounter with transcription and as the few jokes I heard about it, it wasn’t/isn’t that great of a time. With each user test being close to a hour, the time to transribe can be cumbersome. After some failed attemptes of hacky audio to text methods I ran into something that worked fairly well, YouTube!&lt;/p&gt;
&lt;p&gt;YouTube makes auto closed captions for videos. Using their automation I was able to &lt;b&gt;privately&lt;/b&gt; upload the videos and have them be transcribed fairly accurately. YouTube has a great UI for editing closed captions, allowing me to watch the full video with the auto generated captions to ensure accuracy. After that all that is left to do is download the file and convert it to a csv file.&lt;/p&gt;
&lt;p&gt;During this process I added certain code words at the start of sentences, for example if any of the research members spoke during the test I started the line with “staff:”. Later on this will help seperate the text into different columns (user text and staff text). If anyone is reading this that wants to do this for their transcription, feel free to make your own code words, you will have to change the R code a bit for the sbv to csv converter below. Sbv is the file format you save the YouTube close captionings.&lt;/p&gt;
&lt;p&gt;There can be difficulties with this method. A big one is audio quality. Making sure the audio is clear with no background noise is important. I have had multiple videos be translated to spanish and german due to the service not being able to depict the language at the start. Another issue is two people talking at once. The service will get confused and try to choose a word that sounds like the words that both people are saying. Making sure one person speaks at a time ensures a easy transcription process. At worst YouTube does not auto transcribe for you. You can still make your own file on the site. There are convenient hotkeys to use while transcribing. Again, the UI is great for this type of work.&lt;/p&gt;
&lt;p&gt;All around this method has saved our team a lot of time during the transcribing process. I wanted to share this method in hopes to save others who do transcriptions by ear. Below is a write I did for my team and the R code to convert the sbv files.&lt;/p&gt;
&lt;p&gt;I want to emphasize about privacy. Videos are uploaded privately and deleted right after the process is complete. On top of that there is no way to identify any user in the videos due to not being able to see them.&lt;/p&gt;
&lt;p&gt;Here is a great article that summarizes the work our research team does at IUPUI:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://soic.iupui.edu/news/reda-nsf-crii-grant/&#34;&gt;HCC faculty member awarded NSF grant for data visualization research&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Download the video(s) from box of the necessary session&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Upload them to YouTube (you will need a YouTube/google account)
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Make sure you set publish setting to PRIVATE!&lt;/li&gt;
&lt;li&gt;Also make sure you delete the video after you are completely done&lt;br /&gt;
 &lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;After the video has finished upload, it may take several hours for YouTube to automatically generate close captioning’s.
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Close captioning’s will be our transcription in this case&lt;br /&gt;
 &lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;To see if the automatic translation is complete, start at the homepage of YouTube
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Click on the top right icon (your profile icon) and click ‘Creator Studio.’&lt;/li&gt;
&lt;li&gt;On the next page click ‘Video Manager’ on the left side.&lt;/li&gt;
&lt;li&gt;Find the video you need on the list and click ‘Edit’ on that video&lt;/li&gt;
&lt;li&gt;On the top menu, all the way to the right, click on ‘Subtitles/CC’&lt;/li&gt;
&lt;li&gt;To the right of the video underneath the blue bar that says ‘Add new subtitles or CC’ it will have a title that says ‘Published’&lt;/li&gt;
&lt;li&gt;If it has gone according to plan, it will say ‘English (Automatic)’. Click this.
&lt;ul&gt;
&lt;li&gt;If it lists a different language, it means the audio quality was not clear and you will have to either start doing the transcription manually, or if you are efficient with video/audio editing you can try to clean up the audio&lt;/li&gt;
&lt;li&gt;If it does not show up at all after 6 hours, it will be due to the audio quality not being clear (from what I have read online)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt; 
5. After clicking ‘English (automatic)’ click edit on the top right
 
6. Get familiar with this layout!
a. You can click the times on the left to go directly to that spot on the timeline
b. You can edit the text directly while listening
c. Hotkeys I use:
- Shift + Space = play/pause
- Shift + Left/Right arrow = go forward/back a few seconds&lt;/p&gt;
&lt;p&gt; 
7. Listen to the whole video and read along with what is said in the video&lt;/p&gt;
&lt;p&gt; 
8. Important: When a member of the research team speaks start that line with ‘staff:’
a. This is very important for the format of the final csv, I use this exact string to separate user text and staff text into separate columns in the R script.&lt;/p&gt;
&lt;p&gt; 
9. I manually insert a few important parts
a. When a video starts; ‘MVI 0001 Starts’
b. When a video ends; ‘MVI 0001 Ends’
c. When a question is SUBMITTED; ‘Q. I want to see…’
d. When a graph is generated ‘**Graph is Generated**’&lt;/p&gt;
&lt;p&gt; 
10. Once you are happy with the entirety of the proof-read script, click on ‘Actions’ which is located right above the text editing area.&lt;/p&gt;
&lt;p&gt; 
11. Save it as a ‘.sbv’ file&lt;/p&gt;
&lt;p&gt; 
12. Once you have saved it you can message me and I can quickly do the rest, upload it to the box [Symbol] (I will add the R .sbv to .csv script soon)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# If you do not have tidyverse:
# install.packages(&amp;#39;tidyverse&amp;#39;)
#
# CTRL + ENTER to run lines. run library and function.
#
# The last line is where you need to edit the
# string to files you want to edit

library(tidyverse)

sbv_to_csv &amp;lt;- function(filepath, savename){
  rawData &amp;lt;- read.table(filepath,
                    header = F,
                    sep = &amp;quot;\n&amp;quot;,
                    quote=&amp;quot;\&amp;quot;&amp;quot;)
  
  rawData &amp;lt;- droplevels(rawData)
  rawData &amp;lt;- rawData %&amp;gt;% mutate(dont_keep = str_detect(rawData$V1, &amp;quot;0:&amp;quot;))
  
  # We decided we did not team time stamps (dont_keep)
  df &amp;lt;- data.frame(words = rawData %&amp;gt;%
                     filter(dont_keep == F) %&amp;gt;% select(V1))
  colnames(df) &amp;lt;- &amp;#39;words&amp;#39;
  
  df$words &amp;lt;- as.character(df$words)
  
  df &amp;lt;- df %&amp;gt;% mutate(test = str_detect(words, &amp;#39;staff:&amp;#39;) |
                        str_detect(words, &amp;#39;Staff:&amp;#39;),
                      interviewer = ifelse(test, words, &amp;quot;&amp;quot;),
                      text = ifelse(test, &amp;quot;&amp;quot;, words))
  
# this line is for start/end time colums. fix select on line 26 to include this.
#transcribedData &amp;lt;- df %&amp;gt;% separate(time, c(&amp;quot;startTime&amp;quot;, &amp;quot;endTime&amp;quot;), &amp;quot;,&amp;quot;)
  
  transcribedData &amp;lt;- df %&amp;gt;% select(text, interviewer)
  
  write.csv(transcribedData, file = savename)
}

# First param is path to file, if you are on windows
# you will need to escape the \, so &amp;quot;\\&amp;quot; as shown below
#
# Second param is the output, make sure it ends with
# .csv. this can also be a file path to save where you want it
#
# By default it is your documents I believe, or you
# can set your working directory with setwd(*FOLDERPATH*)

sbv_to_csv(&amp;#39;C:...\\MVI_0115.sbv&amp;#39;, &amp;#39;0115.csv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;!-- ```{r echo=FALSE} --&gt;
&lt;!-- blogdown::shortcode(&#39;googleAdsense&#39;) --&gt;
&lt;!-- ``` --&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Accidental Drug Related Deaths</title>
      <link>/2018/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut/</link>
      <pubDate>Sun, 27 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;follow-me-on&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Follow Me On&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/KoderKow&#34;&gt; &lt;i class=&#39;fab fa-twitter fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://www.instagram.com/koderkow/&#34;&gt; &lt;i class=&#39;fab fa-instagram fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KoderKow/&#34;&gt;&lt;i class=&#39;fab fa-github fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://koderkow.rbind.io&#34;&gt;&lt;i class=&#39;fas fa-globe fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro&lt;/h1&gt;
&lt;p&gt;The opioid epidemic is a nationwide issue for the United States. Among the states Connecticut takes second place in terms of deaths from opioids in the years 2014-2015. As the years go on death rates are doubling from the abuse of opioids. Heroin overdoses continue to rise and now fentanyl is joining its ranks. Fentanyl has been getting mixed with heroin to increase its potency and in return it increases the chance of overdosing (Rondinone, 2017). This report will be looking over the top 5 opioids that are related to overdosing. The data used is from &lt;a href=&#34;https://data.ct.gov/Health-and-Human-Services/Accidental-Drug-Related-Deaths-2012-2017/rybz-nyjw&#34;&gt;data.ct.gov&lt;/a&gt;. This project was done during a class at IUPUI in November 2017.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;data-exploration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Exploration&lt;/h2&gt;
&lt;p&gt;We loaded the data into a dataframe called ‘odDat’. Our first step is to make more columns according to the date.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;odDat$yearMon &amp;lt;- as.yearmon(odDat$Date,&amp;#39;%m/%d/%Y&amp;#39;)
odDat$yearMonNum &amp;lt;- as.numeric(odDat$yearMon)
odDat$year &amp;lt;- year(mdy(odDat$Date))
odDat$month &amp;lt;- month(mdy(odDat$Date))
odDat$day &amp;lt;- day(mdy(odDat$Date))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will be looking into the location type for the drug over doses. Lets see a count of them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;odDat %&amp;gt;%
  group_by(Location) %&amp;gt;%
  summarise(n = n()) %&amp;gt;%
  arrange(desc(n))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Location
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Residence
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1854
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Hospital
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1175
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Other
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
528
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Convalescent Home
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Hospice
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Most of our data is within 3 locations. So we will be aggragating and grouping our data based off these 3 locations. We will be working with the dataset renamed as ‘od’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;od &amp;lt;- odDat %&amp;gt;%
  filter(Location == &amp;quot;Residence&amp;quot; |
           Location == &amp;quot;Hospital&amp;quot; |
           Location == &amp;quot;Other&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets look at an example of the data we are wanting to find out about. For this example we will be looking at heroin overdoses in locations marked as “Other.”&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;od %&amp;gt;%
  filter(Location == &amp;quot;Other&amp;quot; &amp;amp; Heroin == &amp;quot;Y&amp;quot;) %&amp;gt;%
  summarise(n = n())&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
330
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There are 330 counts of heroin overdose in the location known as other. I want to clean the overdose data to perform better insights. Any case of ‘Y’ or ‘y’ will be marked as a 1, if it is not a Y then we will label it as a 0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;od[,16:27] &amp;lt;- ifelse(od[,16:27] == &amp;quot;Y&amp;quot; | od[,16:27] == &amp;#39;y&amp;#39;,1, 0)
od[,29] &amp;lt;- ifelse(od[,29] == &amp;quot;Y&amp;quot; | od[,29] == &amp;#39;y&amp;#39;, 1, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this project we are interested in the top 5 drugs by count of overdoses. We will use dplyr to select the columns, and then summarize them to find the top 5 most common overdoses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;drugs &amp;lt;- od %&amp;gt;%
  select_(.dots = c(&amp;quot;Location&amp;quot;, &amp;quot;Heroin&amp;quot;, &amp;quot;Cocaine&amp;quot;, &amp;quot;Fentanyl&amp;quot;,
                    &amp;quot;Oxycodone&amp;quot;, &amp;quot;Oxymorphone&amp;quot;, &amp;quot;EtOH&amp;quot;,
                    &amp;quot;Hydrocodone&amp;quot;, &amp;quot;Benzodiazepine&amp;quot;, &amp;quot;Methadone&amp;quot;,
                    &amp;quot;Amphet&amp;quot;, &amp;quot;Tramad&amp;quot;, &amp;quot;Morphine..not.heroin.&amp;quot;,
                    &amp;quot;Any.Opioid&amp;quot;, &amp;quot;yearMon&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: select_() is deprecated. 
## Please use select() instead
## 
## The &amp;#39;programming&amp;#39; vignette or the tidyeval book can help you
## to program with select() : https://tidyeval.tidyverse.org
## This warning is displayed once per session.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;drugCount &amp;lt;- drugs %&amp;gt;%
  summarise(Heroin = sum(Heroin),
            Cocaine = sum(Cocaine),
            Fenentanyl = sum(Fentanyl),
            Oxycodone = sum(Oxycodone),
            Oxymorphone = sum(Oxymorphone),
            Etoh = sum(EtOH),
            Hydrocodone = sum(Hydrocodone),
            Benzodiazepine = sum(Benzodiazepine),
            Methadone = sum(Methadone),
            Amphet = sum(Amphet),
            Tramad = sum(Tramad))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Display the top 10 opioids.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;drugCount %&amp;gt;%
  gather() %&amp;gt;%
  top_n(10) %&amp;gt;%
  arrange(desc(value))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
key
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
value
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Heroin
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1926
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fenentanyl
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1102
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Cocaine
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
990
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Benzodiazepine
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
877
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Etoh
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
768
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Oxycodone
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
489
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Methadone
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
333
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Hydrocodone
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
97
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Oxymorphone
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
95
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amphet
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
80
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The top 5 in order of highest to lowest is heroin, fentanyl, cocaine, benzodiazepine, and etOh. It is no surprise to see heroin and fentanyl are on top. Looking at the information for these opioids we are able to look into where the most common area for overdosing is to happen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;drugs5 &amp;lt;- od %&amp;gt;%
  select_(.dots = c(&amp;quot;Location&amp;quot;, &amp;quot;Heroin&amp;quot;, &amp;quot;Cocaine&amp;quot;,
                    &amp;quot;Fentanyl&amp;quot;, &amp;quot;EtOH&amp;quot;, &amp;quot;Benzodiazepine&amp;quot;,
                    &amp;quot;yearMon&amp;quot;))

drugs5 %&amp;gt;% summarize(Residence = sum(Location == &amp;#39;Residence&amp;#39;),
                     Hospital = sum(Location == &amp;#39;Hospital&amp;#39;),
                     Other = sum(Location == &amp;#39;Other&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Residence
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Hospital
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Other
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1854
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1175
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
528
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Residence has the most occurances, followed by hospital then other. Now lets look at how location is distributed among the top 5 drugs. We will do this with two different bar plots. The first will be a normal bar plot, with each bar representing counts of location per drug.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;drugs5Loc &amp;lt;- drugs5 %&amp;gt;%
  group_by(Location) %&amp;gt;%
  summarize(nHeroin = sum(Heroin),
            nFentanyl = sum(Fentanyl),
            nCocaine = sum(Cocaine),
            nBenzodiazepine = sum(Benzodiazepine),
            nEtOh = sum(EtOH)) %&amp;gt;%
  gather(opioid, count, 2:6)

drugs5Loc$opioid &amp;lt;- str_replace(drugs5Loc$opioid, &amp;#39;n&amp;#39;, &amp;#39;&amp;#39;)

ggplot(data = drugs5Loc,
       aes(x = reorder(opioid, count), y = count, fill = Location)) +
  geom_bar(stat = &amp;#39;identity&amp;#39;, position=&amp;#39;dodge&amp;#39;,
           color = &amp;#39;black&amp;#39;) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  labs(x = &amp;#39;opioid&amp;#39;, y = &amp;#39;Count&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The other bar plot I will use is a proportion based bar plot so it is easier to see how location makes up each opiod.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = drugs5Loc, aes(x = opioid, fill = Location, y = count)) +
  geom_bar(position = &amp;#39;fill&amp;#39;, stat = &amp;#39;identity&amp;#39;) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  ylab(&amp;#39;Proportion&amp;#39;) +
  xlab(&amp;#39;opioid&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at these graphs we can confirm that residence makes up most cases for each drug. Generally hospital and other are lower counts, with hospital being the next common and then other. Now that we have an understanding of the top opioids and locations that they occur, it is time to look into overdose counts over the months and years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yearMonDrug &amp;lt;- od %&amp;gt;%
  group_by(yearMon, Location) %&amp;gt;%
  summarize(Heroin = sum(Heroin),
            Cocaine = sum(Cocaine),
            Fentanyl = sum(Fentanyl),
            Etoh = sum(EtOH),
            Benzo = sum(Benzodiazepine)) %&amp;gt;%
  gather(opioid, count, 3:7)

yearMonDrug %&amp;gt;%
  ggplot(aes(x = yearMon, y = count, fill = opioid)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  facet_grid(opioid ~ Location) +
  labs(y = &amp;#39;Count&amp;#39;, x = &amp;#39;Year&amp;#39;) +
  scale_colour_gdocs() + theme_gdocs()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that fentanyl has had a huge spike in overdoses in 2016. Before late 2015 fentanyl is one of the lowest overdosed drugs. This truly shines the light on how fentanyl is a major epidemic. There is a constant increase for all drugs in all locations with a very few observations of strong decreases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yearMonDrug %&amp;gt;%
  ggplot(aes(x = yearMon, y = count, fill = opioid)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;) + facet_wrap( ~ Location) +
  labs(x = &amp;quot;Year&amp;quot;, y = &amp;quot;Count&amp;quot;) +
  theme(axis.text.x  = element_text(angle=45)) + 
          scale_x_continuous(breaks=c(2012, 2013, 2014, 2015, 2016
                                      ,2017))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can clearly see the constant increase of overdoses. Next we will facet the plot to look at each opioid individually.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot to show count of top 5 drugs over 2012-2017
yearMonDrug %&amp;gt;%
  ggplot(aes(x = yearMon, y = count, fill = opioid)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;) + facet_wrap( ~ opioid) +
  labs(x = &amp;quot;Year&amp;quot;, y = &amp;quot;Count&amp;quot;) +
  theme(axis.text.x  = element_text(angle=45)) + 
          scale_x_continuous(breaks=c(2012, 2013, 2014, 2015, 2016
                                      ,2017))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at all of the data we can see that the primary location for overdosing is residence. In all locations we can see that the count is increasing. Heroin and fentanyl are the most common as well. It is interesting to see the huge spike in fentanyl count over the past few years. The large increase is so much it now has a higher count than heroin.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation&lt;/h2&gt;
&lt;p&gt;Moving on past visual representation and aggragation of our data we will now look into the last 6 months of our data set, more specifically the first 6 months of 2017. I will be using these 6 months to simulate the next few months that are not present in our data. Janurary overdose count will be used for July, Feburary for August, ect. I Will be simulated the first 3 months for each drug. Then I will break down the simulation by location for the first three months, then I am interested in doing heroin, the most popular opioid, by location for the last 6 months.&lt;/p&gt;
&lt;p&gt;This process may not make sense logically. This was a requirement by the project to implement simulation. It is moreso theory in how to apply simulation in this kind of setting and how to interpret it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)

od2 &amp;lt;- od %&amp;gt;%
  select(Heroin, Cocaine, Fentanyl, EtOH,
         Benzodiazepine, yearMon, year, month, Location)

# values for overdoses in Jan 2017
od20171 &amp;lt;- od2 %&amp;gt;%
  group_by(Location) %&amp;gt;%
  filter(year == 2017 &amp;amp; month == 1) %&amp;gt;%
  summarize(nHeroin = sum(Heroin),
            nCocaine = sum(Cocaine),
            nFentanyl = sum(Fentanyl),
            nEtoh = sum(EtOH),
            nBenzo = sum(Benzodiazepine))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Location
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
nHeroin
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
nCocaine
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
nFentanyl
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
nEtoh
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
nBenzo
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Hospital
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Other
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Residence
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This shows us the counts of opioids in Janurary 2017.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sim for july
set.seed(char2seed(&amp;#39;Kyle&amp;#39;))
# sims for each drug by total
july2017 &amp;lt;- data.frame(Heroin=rpois(1000,sum(od20171$nHeroin)),
                       Cocaine=rpois(1000,sum(od20171$nCocaine)),
                       Fent=rpois(1000,sum(od20171$nFentanyl)),
                       Etoh=rpois(1000,sum(od20171$nEtoh)),
                       Benzo=rpois(1000,
                                sum(od20171$nBenzo)))
july2017 %&amp;gt;%
  gather() %&amp;gt;% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = &amp;quot;free&amp;quot;) +
  geom_histogram(color=&amp;#39;black&amp;#39;,binwidth = 2,aes(fill=key)) +
  theme(legend.position=&amp;quot;none&amp;quot;) +
  labs(x=&amp;quot;opioid&amp;quot;, y=&amp;quot;Overdose Count&amp;quot;, title = &amp;quot;July 2017 Simulation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is five separate simulations for each drug to show, in a simulated manner, the count of overdoses for July 2017 based off the counts of Janurary 2017. Taking information from these simulations we can see that Benzo will have around 10 overdoses on the low end and we can expect a high of around 30 (10-30). Following this template, cocaine will have 20-42, Etoh 5-17, fentanyl will have 35 - 70, and heroin 31 - 65. Looking at summaries of our data before it is to be expected heroin would be higher. I will do a few more simulations for August and September to see if fentanyl is on top and to see if our simulated results show an increase/decrease to other.&lt;/p&gt;
&lt;p&gt;To avoid summarizing each set, I will add the results of observations into a data frame and print it out.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;od20172 &amp;lt;- od2 %&amp;gt;%
  group_by(Location) %&amp;gt;%
  filter(year == 2017 &amp;amp; month == 2) %&amp;gt;%
  summarize(nHeroin = sum(Heroin),
            nCocaine = sum(Cocaine),
            nFentanyl = sum(Fentanyl),
            nEtoh = sum(EtOH),
            nBenzodiazepine = sum(Benzodiazepine))

set.seed(char2seed(&amp;#39;Kyle&amp;#39;))
# sims for each drug by total
august2017 &amp;lt;- data.frame(Heroin=rpois(1000,sum(od20172$nHeroin)),
                       Cocaine=rpois(1000,sum(od20172$nCocaine)),
                       Fent=rpois(1000,sum(od20172$nFentanyl)),
                       Etoh=rpois(1000,sum(od20172$nEtoh)),
                       Benzo=rpois(1000,
                                sum(od20172$nBenzodiazepine)))
august2017 %&amp;gt;%
  gather() %&amp;gt;% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = &amp;quot;free&amp;quot;) +
  geom_histogram(color=&amp;#39;black&amp;#39;,binwidth = 2,aes(fill=key)) +
  theme(legend.position=&amp;quot;none&amp;quot;) +
  labs(x=&amp;quot;opioid&amp;quot;, y=&amp;quot;Overdose Count&amp;quot;, title = &amp;quot;August 2017 Simulation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;od20173 &amp;lt;- od2 %&amp;gt;%
  group_by(Location) %&amp;gt;%
  filter(year == 2017 &amp;amp; month == 3) %&amp;gt;%
  summarize(nHeroin = sum(Heroin),
            nCocaine = sum(Cocaine),
            nFentanyl = sum(Fentanyl),
            nEtoh = sum(EtOH),
            nBenzodiazepine = sum(Benzodiazepine))

set.seed(char2seed(&amp;#39;Kyle&amp;#39;))
# sims for each drug by total
sept2017 &amp;lt;- data.frame(Heroin=rpois(1000,sum(od20173$nHeroin)),
                       Cocaine=rpois(1000,sum(od20173$nCocaine)),
                       Fent=rpois(1000,sum(od20173$nFentanyl)),
                       Etoh=rpois(1000,sum(od20173$nEtoh)),
                       Benzo=rpois(1000,
                                sum(od20173$nBenzodiazepine)))
sept2017 %&amp;gt;%
  gather() %&amp;gt;% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = &amp;quot;free&amp;quot;) +
  geom_histogram(color=&amp;#39;black&amp;#39;,binwidth = 2,aes(fill=key)) +
  theme(legend.position=&amp;quot;none&amp;quot;) +
  labs(x=&amp;quot;opioid&amp;quot;, y=&amp;quot;Overdose Count&amp;quot;, title = &amp;quot;September 2017 Simulation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simRange %&amp;gt;%
  filter(opioid %in% c(&amp;quot;Fentanyl&amp;quot;, &amp;quot;Heroin&amp;quot;, &amp;quot;Benzo&amp;quot;,
                                   &amp;quot;Cocaine&amp;quot;, &amp;quot;Etoh&amp;quot;)) %&amp;gt;%
  arrange(opioid)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
opioid
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Month
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Range
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Benzo
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jul
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
10-30
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Benzo
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Aug
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
12-36
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Benzo
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sept
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
15-40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Cocaine
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jul
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
20-42
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Cocaine
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Aug
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
15-42
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Cocaine
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sept
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
18-44
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Etoh
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jul
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5-17
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Etoh
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Aug
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0-17
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Etoh
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sept
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0-20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fentanyl
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jul
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
35-70
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fentanyl
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Aug
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
30-66
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fentanyl
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sept
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
37-76
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Heroin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jul
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
41-65
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Heroin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Aug
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
22-50
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Heroin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sept
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
27-56
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To conclude our simulations for the next three months we can see that benzo overdoses are on the rise. Cocaine is roughly constant. Etoh is also constant and is the lower overdose count. Fentanyl seems to be the most common overdose in 2017, it has a large range and the highest values. Heroin is now the second most common in these 2017 simulations, and it appears there is a big drop off from 41 to the 20s of overdose counts.&lt;/p&gt;
&lt;p&gt;To finalize the simulations I will simulate July 2017 for heroin based off of location.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;locationSim &amp;lt;- od2 %&amp;gt;%
  group_by(Location) %&amp;gt;%
  filter(year == 2017 &amp;amp; month == 1) %&amp;gt;%
  summarize(nHeroin = sum(Heroin))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Location
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
nHeroin
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Hospital
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Other
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Residence
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;julyLocSim &amp;lt;- data.frame(Hospital = rpois(1000,
                                        locationSim$nHeroin[1]),
                         Other = rpois(1000,
                                         locationSim$nHeroin[2]),
                         Residence = rpois(1000,
                                        locationSim$nHeroin[3]))

julyLocSim %&amp;gt;%
  gather() %&amp;gt;% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = &amp;quot;free&amp;quot;) +
  geom_histogram(color=&amp;#39;black&amp;#39;,binwidth = 2,aes(fill=key)) +
  theme(legend.position=&amp;quot;none&amp;quot;) +
  labs(x=&amp;quot;Location&amp;quot;, y=&amp;quot;Overdose Count&amp;quot;, title = &amp;quot;July 2017 Location Simulation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The distribution of these poisson simulations look normal. This is because they have a smaller lamba due to the total count for the month being split into three separate locations. For heroin count in the hospital in July 20717 is no less than 8 and expect a high of 26. For ther it is 4 to 23, and 10 to 37 for residence. From this breakdown we are able to confirm what was observed earlier with our data exploration. According to our simulations residence is the most common location, followed by hospital, then other. I wanted to show a simulation breakdown according to location for a particular drug. This can be applied to any drug of interest.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;predictive-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Predictive Modeling&lt;/h2&gt;
&lt;p&gt;Making a model for this dataset will require aggregating the data and getting the total counts by month and year. This is because we will be using a poisson model and it will require total counts. If we try to use the base data set we will not get the results we are expecting because opioids are composed of 0’s and 1’s. The training set, all data before 2017, will be used to train our model. For this example we will be looking specifically at the opioid fentanyl. Then I will test the model against a test set, all data after 2017. At the end the error will be calculated to show us the accuracy of the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trainod &amp;lt;- od2 %&amp;gt;%
  filter(year &amp;lt; 2017) %&amp;gt;%
  group_by(Location, year, month) %&amp;gt;%
  summarize(nHeroin = sum(Heroin),
            nCocaine = sum(Cocaine),
            nFentanyl = sum(Fentanyl),
            nEtoh = sum(EtOH),
            nBenzodiazepine = sum(Benzodiazepine))

model1 &amp;lt;- glm(nFentanyl ~ Location + year + month, data = trainod,
            family = &amp;#39;poisson&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
term
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
estimate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
std.error
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
statistic
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
p.value
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
(Intercept)
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1826.96
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
77.81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-23.48
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
LocationOther
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.86
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-7.48
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
LocationResidence
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.49
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
year
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.91
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23.50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
month
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.42
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The model, model1, will be used now predict the first month of 2017. Showing the actual data and the predicted data will show if the model is predicting close to the real data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new.predict1 &amp;lt;- predict(model1,newdata = data.frame(year=2016, Location = c(&amp;#39;Hospital&amp;#39;, &amp;#39;Other&amp;#39;, &amp;#39;Residence&amp;#39;), month=12), type = &amp;quot;response&amp;quot;)
names(new.predict1) &amp;lt;- c(&amp;#39;Hospital&amp;#39;,&amp;#39;Other&amp;#39;,&amp;#39;Residence&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# predicted values for Jan 2017 in all locations for fentanyl
new.predict1&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Hospital
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Other
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Residence
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.91
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
26.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Actual values for Jan 2017 in all locations for fentanyl.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;od2 %&amp;gt;%
  filter(year == 2017 &amp;amp; month == 1) %&amp;gt;%
  summarize(nFentanyl = sum(Fentanyl))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
nFentanyl
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
52
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As shown after adding up the three individual predictions our predicted values are very close to the actual data. Exciting! To test the accuracy of the model I will now run the model against the test data, which is data in 2017. This way we will be able to gauge the accuracy of our model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;testod &amp;lt;- od2 %&amp;gt;%
  filter(year &amp;lt; 2017) %&amp;gt;%
  group_by(Location, year, month) %&amp;gt;%
  summarize(nHeroin = sum(Heroin),
            nCocaine = sum(Cocaine),
            nFentanyl = sum(Fentanyl),
            nEtoh = sum(EtOH),
            nBenzodiazepine = sum(Benzodiazepine))

predTest &amp;lt;- predict(model1, newdata = testod, type = &amp;quot;response&amp;quot;) 

errTest &amp;lt;- mean((testod$nFentanyl - predTest) ^ 2)

sqrt(errTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model has a 2.5 error, meaning our predications are potentially off by 2.5. I believe this is accurate for our model. This could be very useful to predict upcoming months for the overdoses. The model is for heroin, but it can easily be switched with any of the other drugs in the data set.&lt;/p&gt;
&lt;p&gt;To summarize our findings there is definite proof that overdose counts are on the rise. Residences are common locations for these accidents. Heroin is the most common in the data set, but it is slowly getting over taken by fentanyl. We are able to simulate ranges for expected overdoses in the data. This could be used for professionals to get an idea what they are looking for in the future. Finally we built a fairly accurate model. This model is able to take in year, month, and location and return a value that represents the count of overdoses. I believe this model could be useful to see if counts increase in the future. Furthermore after more data is acquired it would be great to see predicted counts go down because this would show overdose situations are under control.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;sources-related-links&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sources + Related Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.courant.com/breaking-news/hc-cdc-fentanyl-overdoses-20170105-story.html&#34;&gt;Nicholas Rondinone, CDC: Connecticut Second In Percent Increase Of Synthetic Opioid Deaths Rate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.middletownpress.com/news/article/Sen-Murphy-in-Haddam-on-opioid-crisis-This-12548669.php&#34;&gt;Jeff Mill, Sen. Murphy in Haddam on opioid crisis: ‘This is getting worse, not better’&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.law.com/ctlawtribune/sites/ctlawtribune/2018/01/24/attorneys-for-connecticut-cities-pledge-to-fight-any-move-to-opioid-mdl/&#34;&gt;Robert Storace, Attorneys for CT Cities Pledge Fight Against Move to Opioid MDL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.courant.com/politics/hc-pol-new-connecticut-laws-january-20171227-story.html&#34;&gt;Russel Blair, Eight New Laws That Take Effect in Connecticut on Jan. 1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- ```{r echo=FALSE} --&gt;
&lt;!-- blogdown::shortcode(&#39;googleAdsense&#39;) --&gt;
&lt;!-- ``` --&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Web Scraping World of Warcraft Weapons</title>
      <link>/2018/2018-05-24-web-scraping-world-of-warcraft-weapons/</link>
      <pubDate>Thu, 24 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/2018-05-24-web-scraping-world-of-warcraft-weapons/</guid>
      <description>


&lt;div id=&#34;follow-me-on&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Follow Me On&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/KoderKow&#34;&gt; &lt;i class=&#39;fab fa-twitter fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://www.instagram.com/koderkow/&#34;&gt; &lt;i class=&#39;fab fa-instagram fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KoderKow/&#34;&gt;&lt;i class=&#39;fab fa-github fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://koderkow.rbind.io&#34;&gt;&lt;i class=&#39;fas fa-globe fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro&lt;/h1&gt;
&lt;p&gt;The main goal of this project was to learn how to use the R libraries rvest and RSelenium. The web scraping would be taking place on the website wowhead.com. There is about 10,000 rows of data to collect.&lt;/p&gt;
&lt;div id=&#34;learning-points&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Learning Points&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Navigate the DOM to capture desired element values&lt;/li&gt;
&lt;li&gt;Used R’s &lt;em&gt;RSelenium&lt;/em&gt; and &lt;em&gt;rvest&lt;/em&gt; library to capture data&lt;/li&gt;
&lt;li&gt;Construct a R script to automate the complete web scraping process&lt;/li&gt;
&lt;li&gt;Made a data frame with captured data to save to a csv file for data analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;web-scraping-process&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Web Scraping Process&lt;/h2&gt;
&lt;p&gt;The web scraper goes to the wowhead.com weapon page. There is about 10,000 weapons worth of data. All of the data we want to collect from these weapons are not presented in the rows of the table. For each page, there are 50 weapons.&lt;/p&gt;
&lt;div id=&#34;example-of-table-view-of-weapons&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Example of table view of weapons&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;/img/wow-scraper/table_view.png&#34; width=&#34;80%&#34; height=&#34;80%&#34;&gt;&lt;/p&gt;
&lt;p&gt;In order to capture all of the data we want we will need to click on each row, collect the data, go back to the previous page, and then click on the next item.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-of-weapon-view-for-all-attributes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Example of weapon view for all attributes&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;/img/wow-scraper/item_view.png&#34; width=&#34;80%&#34; height=&#34;80%&#34;&gt;&lt;/p&gt;
&lt;p&gt;At the start I was collecting the data directly from the elements shown on the top left box of the page. Issues started coming up when a weapon had a different amount of attributes. For example, if a weapon had two attributes such as strength and agility, that takes up 2 spaces on the DOM. Then if the next weapon had only one attribute, the script would return an error. After exploring the DOM I was able to find all of the data in a &amp;lt;noscript&amp;gt; tag. All of the attributes had different code tags (ie;‘!–stat3–&amp;gt;\+’ = agility). After sorting out all of the tags I was able to use string manipulation to return the information desired.&lt;/p&gt;
&lt;p&gt;First time running the script all the way through with no errors showed the next hurdle to jump. The original default weapon data only showed about 1,000 of the 10,000 total weapons. The next step was to automate a filtering process to collect all the weapons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;by-default-the-page-displays-around-1000-max&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;By default, the page displays around 1000 max&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;/img/wow-scraper/item_displayed_vs_total.png&#34; width=&#34;80%&#34; height=&#34;80%&#34;&gt;&lt;/p&gt;
&lt;p&gt;I decided to search for weapons based on their item level. For example, levels 1-25, 26-35, etc. This would return all the weapons in chunks. Once the first chunk of weapons was collected the script clears the filter and inputs the next number range.&lt;/p&gt;
&lt;p&gt;After thinking this project was complete I showed the data to my professor. We found out the weapon attributes were not correct. My string manipulation was not collecting stats correctly that had a comma in it (ex: 1,247). This was a simple fix, but then I noticed something completely different on the website. When you click a weapon and then go back, the order the weapons on the table were not static. The order changed! Eventually I found a solution, constantly sort the table by the weapon name everytime the script goes back to the main page.&lt;/p&gt;
&lt;p&gt;Unfortunately during this time, World of Warcraft introduced weapon scalability to their weapons. To put it simply and in terms I understand, a large amount of weapons in their game now ‘upgrade’ as you level up. For example, I am level one and have a level one sword that does 1-2 damage and has +1 strength. When I level up to level two the sword now does 2-3 damage and has +2 strength. World of Warcraft has 120 levels. This now means a lot of the weapons have different stats on 120 levels. This really put a thud on this web scraper.&lt;/p&gt;
&lt;p&gt;After talking to my professor, if I chose to continue with this project and wanted to do data analysis on the data, I could make an assumption that all weapons would be looked at in a maxed level view (player level = 120). Sadly, however, this will require going back and changing how a lot of the data is collected.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;reflection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reflection&lt;/h2&gt;
&lt;p&gt;It is unfortunate to get hit by that update, but the process of collecting the data and fixing all the issues that came up was an amazing learning experience. There is still possibility of data analysis with the old data I collected that had the comma issue.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;The data (.csv file) is on my &lt;a href=&#34;https://github.com/KoderKow/wow_scraper&#34;&gt;github&lt;/a&gt;. I wanted to share what the data looked like and that I had success in gathering a lot of different attributes for every weapon.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;I want to thank Professor Lourens for sharing his idea for this project. Professor Lourens assisted me on learning web scraping through rvest/RSelenium and addressed errors I came across.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# scraping libraries
library(RSelenium)
library(rvest)
# XML, allows access to xpath selector
library(XML)
# string splice library
library(stringr)

## create a remoteDriver class object
rD &amp;lt;- rsDriver()

## extract client from rD
remDr &amp;lt;- rD[[&amp;quot;client&amp;quot;]]

## Navigate to a page 
remDr$maxWindowSize()
remDr$navigate(&amp;quot;http://www.wowhead.com/weapons&amp;quot;)
## If the popup shows up, click it to get rid of it

popUp &amp;lt;- remDr$findElement(using =&amp;#39;css&amp;#39;, value = &amp;#39;#item-gallery-listview &amp;gt; div.walkthrough-details-wrapper &amp;gt; div &amp;gt; div &amp;gt; div &amp;gt; div.walkthrough-details-text.right &amp;gt; div &amp;gt; a&amp;#39;)
popUp$clickElement()

## ---- Loop ----
i &amp;lt;- 1 # for rows 1 - 50
j &amp;lt;- 1 # for data frame rows
f &amp;lt;- 1 # for filter seach count
rlvl90counter &amp;lt;- 0
filterHighValue &amp;lt;- &amp;#39;&amp;#39;
timeVec &amp;lt;- numeric()

# Total Weapons Amount
totalWeapons &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = &amp;#39;//*[@id=&amp;quot;tab-items&amp;quot;]/div[1]/div[2]&amp;#39;)
totalWeapons &amp;lt;- totalWeapons$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
totalWeapons &amp;lt;- as.numeric(str_split(totalWeapons, pattern = &amp;#39; &amp;#39;)[[1]][1])

# test for filter automation
reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;1&amp;#39;)) 
reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;25&amp;#39;)) 

# apply filter button
applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
applyFilter$clickElement()

# filtered weapon count
filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)

# sort table by name to avoid random order on back()s
sortByItem &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = &amp;#39;//*[@id=&amp;quot;tab-items&amp;quot;]/div[2]/table/thead/tr/th[2]/div/a/span&amp;#39;)
sortByItem$clickElement()

initialStart &amp;lt;- Sys.time()

while(j &amp;lt;= 10000){
  
  startTime &amp;lt;- Sys.time()
  # sort back to original name order
  sortByItem &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = &amp;#39;//*[@id=&amp;quot;tab-items&amp;quot;]/div[2]/table/thead/tr/th[2]/div/a/span&amp;#39;)
  sortLogic &amp;lt;- sortByItem$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
  if(sortLogic != &amp;quot;&amp;lt;span&amp;gt;Name&amp;lt;/span&amp;gt;&amp;quot;){
    sortByItem$clickElement()
    if(sortLogic != &amp;quot;&amp;lt;span&amp;gt;Name&amp;lt;/span&amp;gt;&amp;quot;){
      sortByItem$clickElement()
    }
  }
  
  URLData &amp;lt;- remDr$getCurrentUrl()
  
  # find all weapons ----
  print(paste(&amp;#39;select weapons table round&amp;#39;, j, &amp;#39;..&amp;#39;))
  weapons &amp;lt;- remDr$findElements(using = &amp;quot;css&amp;quot;, value = &amp;quot;#tab-items &amp;gt; div.listview-scroller &amp;gt; table &amp;gt; tbody &amp;gt; tr&amp;quot;)
  
  # select weapon i from list
  print(paste(&amp;#39;selecting weapon&amp;#39;, i, &amp;#39;..&amp;#39;))
  selectWeapon &amp;lt;- weapons[[i]]$findChildElement(using = &amp;quot;css&amp;quot;, value = &amp;quot;td:nth-child(3) &amp;gt; div &amp;gt; a&amp;quot;)
  
  # required lvl
  print(paste(&amp;#39;selecting weapon&amp;#39;, i, &amp;#39;required level...&amp;#39;))
  reqLvl &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = paste(&amp;quot;//*[@id=\&amp;quot;tab-items\&amp;quot;]/div[2]/table/tbody/tr[&amp;quot;, i, &amp;quot;]/td[5]&amp;quot;))
  reqLvl &amp;lt;- reqLvl$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
  reqLvl &amp;lt;- as.numeric(reqLvl)
  
  
  # click selected weapon
  print(paste(&amp;#39;clicking weapon&amp;#39;, i, &amp;#39;..&amp;#39;))
  selectWeapon$clickElement()
  
  # grab page&amp;#39;s url for item # (not sure if this is used anymore..)
  print(paste(&amp;#39;selecting URL&amp;#39;, i, &amp;#39;..&amp;#39;))
  URL &amp;lt;- remDr$getCurrentUrl()
  
  # split string for item number
  print(paste(&amp;#39;selecting item #&amp;#39;, i, &amp;#39;..&amp;#39;))
  itemNum &amp;lt;- str_split(str_split(URL, pattern = &amp;#39;=&amp;#39;)[[1]][2], pattern = &amp;#39;/&amp;#39;)[[1]][1]
  
  # Weapon Data from div[x]/noscript. x changes between 2 or 3 based on ads----
  weaponData &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = paste(&amp;quot;//*[@id=\&amp;quot;main-contents\&amp;quot;]/div[2]/noscript&amp;quot;, sep = &amp;quot;&amp;quot;))
  WeaponDataText &amp;lt;- weaponData$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
  
  # Weapon Name ----
  print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;name...&amp;#39;))
  weaponName &amp;lt;- remDr$findElement(using = &amp;quot;xpath&amp;quot;, value = paste(&amp;quot;//*[@id=&amp;#39;main-contents&amp;#39;]/div[2]/h1&amp;quot;, sep = &amp;quot;&amp;quot;))
  name &amp;lt;- weaponName$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
  name &amp;lt;- str_split(name, &amp;#39;&amp;lt;span&amp;#39;)[[1]][1]
  
  # Weapon Item Level ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;item level...&amp;#39;))
  weaponItemLevel &amp;lt;- remDr$findElement(using = &amp;quot;xpath&amp;quot;, value = paste(&amp;quot;/html/head/meta[5]&amp;quot;, sep = &amp;quot;&amp;quot;))
  itemLevel &amp;lt;- weaponItemLevel$getElementAttribute(&amp;#39;content&amp;#39;)
  itemLevel &amp;lt;- str_split(itemLevel,&amp;#39;item level of &amp;#39;)[[1]][2]
  itemLevel &amp;lt;- str_split(itemLevel,&amp;#39;. &amp;#39;)[[1]][1]
  
  # Upgrade ----
  weaponUpgrade &amp;lt;- str_split(WeaponDataText, &amp;#39;Level &amp;amp;lt;!--uindex--&amp;amp;gt;&amp;#39;)[[1]][2]
  weaponUpgrade &amp;lt;- str_trim(str_split(weaponUpgrade, &amp;#39;/&amp;#39;)[[1]][2])
  upgradeLvl &amp;lt;- str_split(weaponUpgrade, &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]
  
  # Weapon Bind on Pick Up ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i, &amp;#39;bind setting...&amp;#39;))
  bind &amp;lt;- str_split(WeaponDataText, &amp;#39;!--bo--&amp;amp;gt;&amp;#39;)
  bind &amp;lt;- str_split(bind[[1]][2], &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]
  
  # Weapon Hand Characteristic ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;hand value...&amp;#39;))
  hand &amp;lt;- str_split(WeaponDataText, &amp;#39;width=\&amp;quot;100%\&amp;quot;&amp;amp;gt;&amp;amp;lt;tr&amp;amp;gt;&amp;amp;lt;td&amp;amp;gt;&amp;#39;)[[1]][2]
  hand &amp;lt;- str_split(hand, &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]

  # Weapon Type ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;type...&amp;#39;))
  type &amp;lt;- str_split(WeaponDataText, &amp;#39;class=\&amp;quot;q1\&amp;quot;&amp;amp;gt;&amp;#39;)[[1]][2]
  type &amp;lt;- str_split(type, &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]
  
  # Weapon Damage Range (High + Low) ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i, &amp;#39;damage range...&amp;#39;))
  damageRange &amp;lt;- str_split(WeaponDataText, &amp;#39;!--dmg--&amp;amp;gt;&amp;#39;)[[1]][2]
  damageRange &amp;lt;- str_split(damageRange, &amp;#39; &amp;#39;)
  # Weapon Low Damage
  damageLow &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, damageRange[[1]][1]))
  # Weapon High Damage
  damageHigh &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, damageRange[[1]][3]))
  
  # Weapon Speed ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i, &amp;#39;speed...&amp;#39;))
  speed &amp;lt;- str_split(WeaponDataText, &amp;#39;!--spd--&amp;amp;gt;&amp;#39;)[[1]][2]
  speed &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_split(speed, &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]))
  
  # Weapon DPS ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i, &amp;#39;dps...&amp;#39;))
  dps &amp;lt;- str_split(WeaponDataText,&amp;#39;!--dps--&amp;amp;gt;\\(&amp;#39;)
  dps &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(dps[[1]][2], &amp;#39;damage&amp;#39;)[[1]][1])))
  
  # Weapon Or Option ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i, &amp;#39;Or Option...&amp;#39;))
  or &amp;lt;- str_split(WeaponDataText,&amp;#39;damage per second\\)&amp;amp;lt;br /&amp;amp;gt;&amp;amp;lt;span&amp;amp;gt;&amp;amp;lt;&amp;#39;)
  or &amp;lt;- str_split(dps[[1]][2], &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]
  if(!is.na(str_detect(or, &amp;#39;or&amp;#39;))){
    orOption &amp;lt;- 1
  }else{
    orOption &amp;lt;- NA
  }
  
  # Agility Test ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Agility level...&amp;#39;))
  weaponAgility &amp;lt;- str_split(WeaponDataText, &amp;#39;!--stat3--&amp;amp;gt;\\+&amp;#39;)[[1]][2]
  agility &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(weaponAgility, &amp;#39;Agility&amp;#39;)[[1]][1])))
  
  # Intellect Test ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Intellect level...&amp;#39;))
  weaponIntellect &amp;lt;- str_split(WeaponDataText, &amp;#39;!--stat5--&amp;amp;gt;\\+&amp;#39;)[[1]][2]
  intellect &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(weaponIntellect, &amp;#39;Intellect&amp;#39;)[[1]][1])))
  
  # Mastery Test ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Mastery level...&amp;#39;))
  weaponMastery &amp;lt;- str_split(WeaponDataText, &amp;#39;!--rtg49--&amp;amp;gt;&amp;#39;)[[1]][2]
  mastery &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(weaponMastery, &amp;#39;Mastery&amp;#39;)[[1]][1])))
  
  # Stamina Test ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Stanima level...&amp;#39;))
  weaponStamina &amp;lt;- str_split(WeaponDataText, &amp;#39;!--stat7--&amp;amp;gt;\\+&amp;#39;)[[1]][2]
  stamina &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(weaponStamina, &amp;#39;Stamina&amp;#39;)[[1]][1])))
  
  
  # Strength Test ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Stanima level...&amp;#39;))
  weaponStrength &amp;lt;- str_split(WeaponDataText, &amp;#39;!--stat4--&amp;amp;gt;\\+&amp;#39;)[[1]][2]
  strength &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(weaponStrength, &amp;#39;Strength&amp;#39;)[[1]][1])))
  
  # Critical Strike Test ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Critical Strike level...&amp;#39;))
  weaponCritStrike &amp;lt;- str_split(WeaponDataText, &amp;#39;!--rtg32--&amp;amp;gt;&amp;#39;)[[1]][2]
  critStrike &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(weaponCritStrike, &amp;#39;Critical&amp;#39;)[[1]][1])))
  
  # Durability Test ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Durability level...&amp;#39;))
  weaponDurability &amp;lt;- str_split(WeaponDataText, &amp;#39;Durability &amp;#39;)[[1]][2]
  durability &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_split(weaponDurability, &amp;#39; &amp;#39;)[[1]][1]))
  
  # Haste ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Haste level...&amp;#39;))
  weaponHaste &amp;lt;- str_split(WeaponDataText, &amp;#39;!--rtg36--&amp;amp;gt;&amp;#39;)[[1]][2]
  haste &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(weaponHaste, &amp;#39;Haste&amp;#39;)[[1]][1])))
  
  # Versatility ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Versatility level...&amp;#39;))
  weaponVersatility &amp;lt;- str_split(WeaponDataText, &amp;#39;!--rtg40--&amp;amp;gt;&amp;#39;)[[1]][2]
  versatility &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(weaponVersatility, &amp;#39;Versatility&amp;#39;)[[1]][1])))
  
  # Classes ---- took out, unsure of best method
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Class level...&amp;#39;))
  # weaponClasses &amp;lt;- str_split(WeaponDataText, &amp;#39;class=\&amp;quot;c7\&amp;quot;&amp;amp;gt;&amp;#39;)[[1]][2]
  # classes &amp;lt;- str_split(weaponClasses, &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]
  
  # Weapon Sell Price ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i, &amp;#39;sell price...&amp;#39;))
  sellPriceGold &amp;lt;- str_split(WeaponDataText, &amp;#39;class=\&amp;quot;moneygold\&amp;quot;&amp;amp;gt;&amp;#39;)
  sellPriceGold &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_split(sellPriceGold[[1]][2], &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]))
  # Silver
  sellPriceSilver &amp;lt;- str_split(WeaponDataText, &amp;#39;class=\&amp;quot;moneysilver\&amp;quot;&amp;amp;gt;&amp;#39;)
  sellPriceSilver &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_split(sellPriceSilver[[1]][2], &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]))
  # Copper
  sellPriceCopper &amp;lt;- str_split(WeaponDataText, &amp;#39;class=\&amp;quot;moneycopper\&amp;quot;&amp;amp;gt;&amp;#39;)
  sellPriceCopper &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_split(sellPriceCopper[[1]][2], &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]))
  
  # Weapon Patch / Expansion ----
  weaponPatch &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = paste(&amp;quot;/html/head/meta[6]&amp;quot;, sep = &amp;quot;&amp;quot;))
  patch &amp;lt;- weaponPatch$getElementAttribute(&amp;#39;content&amp;#39;)
  patch &amp;lt;- str_split(patch, &amp;#39;Item, &amp;#39;)[[1]][2]
  patch &amp;lt;- str_split(patch, &amp;#39;, &amp;#39;)
  patchNum &amp;lt;- patch[[1]][1]
  if(patch[[1]][2] == &amp;quot;Alliance&amp;quot; | patch[[1]][2] == &amp;quot;Horde&amp;quot;){
    side &amp;lt;- patch[[1]][2]
    expansion &amp;lt;- patch[[1]][3]
  }else{
    expansion &amp;lt;- patch[[1]][2]
    side &amp;lt;- NA
  }
  
  # Create Date Frame ----
  print(paste(&amp;#39;Creating data frame row&amp;#39;, j))
  if(!exists(&amp;#39;wowWeapons&amp;#39;)){
    wowWeapons &amp;lt;- data.frame(name, itemLevel, upgradeLvl, bind, hand, type, damageLow, damageHigh, speed, dps, orOption, agility, critStrike, haste, intellect, mastery, stamina, strength, versatility, durability, reqLvl, sellPriceGold, sellPriceSilver, sellPriceCopper, side, expansion, patchNum)
    colnames(wowWeapons)[1] &amp;lt;- &amp;#39;name&amp;#39;
    colnames(wowWeapons)[5] &amp;lt;- &amp;#39;hand&amp;#39;
    colnames(wowWeapons)[6] &amp;lt;- &amp;#39;type&amp;#39;
    
  }else{
    newRow &amp;lt;- data.frame(name, itemLevel, upgradeLvl, bind, hand, type, damageLow, damageHigh, speed, dps, orOption, agility, critStrike, haste, intellect, mastery, stamina, strength, versatility, durability, reqLvl, sellPriceGold, sellPriceSilver, sellPriceCopper, side, expansion, patchNum)
    names(newRow) &amp;lt;- names(wowWeapons)
    wowWeapons &amp;lt;- rbind(wowWeapons, newRow)
  }
  
  print(paste(&amp;#39;Weapon&amp;#39;, j, &amp;#39;complete! Going Back..&amp;#39;))
  timeVec[j] &amp;lt;- endTime - startTime
  remDr$goBack()

  # logic for going to next page
  if(i == 50){
    print(&amp;#39;Going to the next page&amp;#39;)
    nextPage &amp;lt;- remDr$findElement(using =&amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; a:nth-child(4)&amp;#39;)
    nextPage$clickElement()
    i &amp;lt;- 0
  }
  
  # filter automation start (up to rlvl 89) ----
  if(f == filterWeaponCount){
    print(&amp;#39;f == filterWeaponCount&amp;#39;)
    filterHighValue &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = &amp;#39;//*[@id=&amp;quot;filter-facet-max-req-level&amp;quot;]&amp;#39;)
    filterHighValue &amp;lt;- filterHighValue$getElementAttribute(&amp;#39;value&amp;#39;)
    
    # 26-55
    if(filterHighValue == &amp;#39;25&amp;#39;){
      print(&amp;#39;f == filterWeaponCount&amp;#39;)
      
      # test for filter automation
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;26&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;55&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      #56-65
    }else if(filterHighValue == &amp;#39;55&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;56&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;65&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      # 66 - 70
    }else if(filterHighValue == &amp;#39;65&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;66&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;70&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      # 71-79
    }else if(filterHighValue == &amp;#39;70&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;71&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;79&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      # 80-80
    }else if(filterHighValue == &amp;#39;79&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;80&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;80&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      #81-85
    }else if(filterHighValue == &amp;#39;80&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;81&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;85&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      #86-90
    }else if(filterHighValue == &amp;#39;85&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;86&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;89&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      # 91-99
    }else if(filterHighValue == &amp;#39;89&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;90&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;90&amp;#39;))
      
      # rlvl 90 filter 1
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(1)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(2)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(3)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(4)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(5)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(6)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(7)&amp;#39;)
      f90$clickElement()
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      rlvl90filter1 &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = &amp;#39;//*[@id=&amp;quot;tab-items&amp;quot;]/div[1]/div[1]/span/b[3]&amp;#39;)
      rlvl90filter1 &amp;lt;- rlvl90filter1$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      rlvl90filter1 &amp;lt;- as.numeric(rlvl90filter1)
      
      filterHighValue &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = &amp;#39;//*[@id=&amp;quot;filter-facet-max-req-level&amp;quot;]&amp;#39;)
      filterHighValue &amp;lt;- filterHighValue$getElementAttribute(&amp;#39;value&amp;#39;)
      
      
    }
    
    else if(rlvl90counter == rlvl90filter1){
      # clear filter 1 button
      clearfilterlist &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type-clear-link&amp;#39;)
      clearfilterlist$clickElement()
      
      # rlvl 90 filter 2
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(8)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(9)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(10)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(11)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(12)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(13)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(14)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(15)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(16)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(17)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(18)&amp;#39;)
      f90$clickElement()
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      # 100
    }else if(filterHighValue == &amp;#39;90&amp;#39;){
      clearfilterlist &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type-clear-link&amp;#39;)
      clearfilterlist$clickElement()
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;91&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;99&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      # 100
    }
    
    else if(filterHighValue == &amp;#39;99&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;100&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;100&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      # 101-110
    }else if(filterHighValue == &amp;#39;100&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;101&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;110&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
    }else if(filterHighValue == &amp;#39;110&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;0&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;0&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
    }else if(filterHighValue == &amp;#39;0&amp;#39;){
      print(&amp;#39;testing breaks&amp;#39;)
      break
    }
  }
  # end filter logic 
  
  i &amp;lt;- i + 1
  j &amp;lt;- j + 1
  f &amp;lt;- f + 1
  if(filterHighValue == &amp;#39;90&amp;#39;){
    rlvl90counter &amp;lt;- rlvl90counter + 1
  }
  
}
finalTime &amp;lt;- Sys.time()
# Loop Finish ----&lt;/code&gt;&lt;/pre&gt;
&lt;!-- ```{r echo=FALSE} --&gt;
&lt;!-- blogdown::shortcode(&#39;googleAdsense&#39;) --&gt;
&lt;!-- ``` --&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
