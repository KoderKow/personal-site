<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kyle Harris on Kyle Harris</title>
    <link>/</link>
    <description>Recent content in Kyle Harris on Kyle Harris</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 13 Sep 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Fitting Tree Models</title>
      <link>/2019/2019-09-13-fitting-tree-models/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/2019-09-13-fitting-tree-models/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#eda&#34;&gt;EDA&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-cleaning&#34;&gt;Data Cleaning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot-em&#34;&gt;Plot Emâ€™!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modeling&#34;&gt;Modeling&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#split-the-data&#34;&gt;Split the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#classification-tree&#34;&gt;Classification Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bagging-random-forests&#34;&gt;Bagging &amp;amp; Random Forests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#boosted-trees&#34;&gt;Boosted Trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(knitr)
library(kableExtra)
library(xgboost)
library(tidyverse)
library(rlang)
library(skimr)
library(ggthemes)
library(kowr)
library(countrycode)
library(rpart)
library(rpart.plot)
library(caret)
library(ModelMetrics)
library(randomForest)
library(ranger)
library(tictoc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The adult data is obtained from the &lt;a href=&#34;https://archive.ics.uci.edu/ml/index.php&#34;&gt;UCI Machine Learning Repository&lt;/a&gt;. On the site they have the data split into multiple pieces:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data: the training data&lt;/li&gt;
&lt;li&gt;test: the test data&lt;/li&gt;
&lt;li&gt;names: column names&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can read the data directly from their website. For readability the URLs will be held within variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_url &amp;lt;- &amp;quot;https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data&amp;quot;
names_url &amp;lt;- &amp;quot;https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names&amp;quot;
test_url &amp;lt;- &amp;quot;https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;hc&gt;read_csv()&lt;/hc&gt; will allow us to read in the data from the site, however, we know the data has no column names. Thus, we shall read in the column names first. The data from &lt;hc&gt;names_url&lt;/hc&gt; is not in a format for easy R interpritation. After some exploration on the website it looks like a .txt file, so we can use readlines to look at the data and find what we are looking for.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names_file &amp;lt;- read_lines(names_url)
names_file&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;scroll&#34;&gt;&lt;code&gt;##   [1] &amp;quot;| This data was extracted from the census bureau database found at&amp;quot;                                                                                                                                                                                                                                                                                                                                                                             
##   [2] &amp;quot;| http://www.census.gov/ftp/pub/DES/www/welcome.html&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                           
##   [3] &amp;quot;| Donor: Ronny Kohavi and Barry Becker,&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                        
##   [4] &amp;quot;|        Data Mining and Visualization&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                         
##   [5] &amp;quot;|        Silicon Graphics.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                     
##   [6] &amp;quot;|        e-mail: ronnyk@sgi.com for questions.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                 
##   [7] &amp;quot;| Split into train-test using MLC++ GenCVFiles (2/3, 1/3 random).&amp;quot;                                                                                                                                                                                                                                                                                                                                                                              
##   [8] &amp;quot;| 48842 instances, mix of continuous and discrete    (train=32561, test=16281)&amp;quot;                                                                                                                                                                                                                                                                                                                                                                 
##   [9] &amp;quot;| 45222 if instances with unknown values are removed (train=30162, test=15060)&amp;quot;                                                                                                                                                                                                                                                                                                                                                                 
##  [10] &amp;quot;| Duplicate or conflicting instances : 6&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                       
##  [11] &amp;quot;| Class probabilities for adult.all file&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                       
##  [12] &amp;quot;| Probability for the label &amp;#39;&amp;gt;50K&amp;#39;  : 23.93% / 24.78% (without unknowns)&amp;quot;                                                                                                                                                                                                                                                                                                                                                                       
##  [13] &amp;quot;| Probability for the label &amp;#39;&amp;lt;=50K&amp;#39; : 76.07% / 75.22% (without unknowns)&amp;quot;                                                                                                                                                                                                                                                                                                                                                                       
##  [14] &amp;quot;|&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [15] &amp;quot;| Extraction was done by Barry Becker from the 1994 Census database.  A set of&amp;quot;                                                                                                                                                                                                                                                                                                                                                                 
##  [16] &amp;quot;|   reasonably clean records was extracted using the following conditions:&amp;quot;                                                                                                                                                                                                                                                                                                                                                                     
##  [17] &amp;quot;|   ((AAGE&amp;gt;16) &amp;amp;&amp;amp; (AGI&amp;gt;100) &amp;amp;&amp;amp; (AFNLWGT&amp;gt;1)&amp;amp;&amp;amp; (HRSWK&amp;gt;0))&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                        
##  [18] &amp;quot;|&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [19] &amp;quot;| Prediction task is to determine whether a person makes over 50K&amp;quot;                                                                                                                                                                                                                                                                                                                                                                              
##  [20] &amp;quot;| a year.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                      
##  [21] &amp;quot;|&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [22] &amp;quot;| First cited in:&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [23] &amp;quot;| @inproceedings{kohavi-nbtree,&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                
##  [24] &amp;quot;|    author={Ron Kohavi},&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                      
##  [25] &amp;quot;|    title={Scaling Up the Accuracy of Naive-Bayes Classifiers: a&amp;quot;                                                                                                                                                                                                                                                                                                                                                                              
##  [26] &amp;quot;|           Decision-Tree Hybrid},&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [27] &amp;quot;|    booktitle={Proceedings of the Second International Conference on&amp;quot;                                                                                                                                                                                                                                                                                                                                                                          
##  [28] &amp;quot;|               Knowledge Discovery and Data Mining},&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                          
##  [29] &amp;quot;|    year = 1996,&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [30] &amp;quot;|    pages={to appear}}&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                        
##  [31] &amp;quot;|&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [32] &amp;quot;| Error Accuracy reported as follows, after removal of unknowns from&amp;quot;                                                                                                                                                                                                                                                                                                                                                                           
##  [33] &amp;quot;|    train/test sets):&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                         
##  [34] &amp;quot;|    C4.5       : 84.46+-0.30&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                  
##  [35] &amp;quot;|    Naive-Bayes: 83.88+-0.30&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                  
##  [36] &amp;quot;|    NBTree     : 85.90+-0.28&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                  
##  [37] &amp;quot;|&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [38] &amp;quot;|&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [39] &amp;quot;| Following algorithms were later run with the following error rates,&amp;quot;                                                                                                                                                                                                                                                                                                                                                                          
##  [40] &amp;quot;|    all after removal of unknowns and using the original train/test split.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                    
##  [41] &amp;quot;|    All these numbers are straight runs using MLC++ with default values.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                      
##  [42] &amp;quot;|&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [43] &amp;quot;|    Algorithm               Error&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [44] &amp;quot;| -- ----------------        -----&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [45] &amp;quot;| 1  C4.5                    15.54&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [46] &amp;quot;| 2  C4.5-auto               14.46&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [47] &amp;quot;| 3  C4.5 rules              14.94&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [48] &amp;quot;| 4  Voted ID3 (0.6)         15.64&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [49] &amp;quot;| 5  Voted ID3 (0.8)         16.47&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [50] &amp;quot;| 6  T2                      16.84&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [51] &amp;quot;| 7  1R                      19.54&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [52] &amp;quot;| 8  NBTree                  14.10&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [53] &amp;quot;| 9  CN2                     16.00&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [54] &amp;quot;| 10 HOODG                   14.82&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [55] &amp;quot;| 11 FSS Naive Bayes         14.05&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [56] &amp;quot;| 12 IDTM (Decision table)   14.46&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [57] &amp;quot;| 13 Naive-Bayes             16.12&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [58] &amp;quot;| 14 Nearest-neighbor (1)    21.42&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [59] &amp;quot;| 15 Nearest-neighbor (3)    20.35&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [60] &amp;quot;| 16 OC1                     15.04&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                             
##  [61] &amp;quot;| 17 Pebls                   Crashed.  Unknown why (bounds WERE increased)&amp;quot;                                                                                                                                                                                                                                                                                                                                                                     
##  [62] &amp;quot;|&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [63] &amp;quot;| Conversion of original data as follows:&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                      
##  [64] &amp;quot;| 1. Discretized agrossincome into two ranges with threshold 50,000.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                           
##  [65] &amp;quot;| 2. Convert U.S. to US to avoid periods.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                      
##  [66] &amp;quot;| 3. Convert Unknown to \&amp;quot;?\&amp;quot;&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                  
##  [67] &amp;quot;| 4. Run MLC++ GenCVFiles to generate data,test.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                               
##  [68] &amp;quot;|&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [69] &amp;quot;| Description of fnlwgt (final weight)&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                         
##  [70] &amp;quot;|&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [71] &amp;quot;| The weights on the CPS files are controlled to independent estimates of the&amp;quot;                                                                                                                                                                                                                                                                                                                                                                  
##  [72] &amp;quot;| civilian noninstitutional population of the US.  These are prepared monthly&amp;quot;                                                                                                                                                                                                                                                                                                                                                                  
##  [73] &amp;quot;| for us by Population Division here at the Census Bureau.  We use 3 sets of&amp;quot;                                                                                                                                                                                                                                                                                                                                                                   
##  [74] &amp;quot;| controls.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                    
##  [75] &amp;quot;|  These are:&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                  
##  [76] &amp;quot;|          1.  A single cell estimate of the population 16+ for each state.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                    
##  [77] &amp;quot;|          2.  Controls for Hispanic Origin by age and sex.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                    
##  [78] &amp;quot;|          3.  Controls by Race, age and sex.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                  
##  [79] &amp;quot;|&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [80] &amp;quot;| We use all three sets of controls in our weighting program and \&amp;quot;rake\&amp;quot; through&amp;quot;                                                                                                                                                                                                                                                                                                                                                              
##  [81] &amp;quot;| them 6 times so that by the end we come back to all the controls we used.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                    
##  [82] &amp;quot;|&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [83] &amp;quot;| The term estimate refers to population totals derived from CPS by creating&amp;quot;                                                                                                                                                                                                                                                                                                                                                                   
##  [84] &amp;quot;| \&amp;quot;weighted tallies\&amp;quot; of any specified socio-economic characteristics of the&amp;quot;                                                                                                                                                                                                                                                                                                                                                                  
##  [85] &amp;quot;| population.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                  
##  [86] &amp;quot;|&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                              
##  [87] &amp;quot;| People with similar demographic characteristics should have&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                  
##  [88] &amp;quot;| similar weights.  There is one important caveat to remember&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                  
##  [89] &amp;quot;| about this statement.  That is that since the CPS sample is&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                  
##  [90] &amp;quot;| actually a collection of 51 state samples, each with its own&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                 
##  [91] &amp;quot;| probability of selection, the statement only applies within&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                  
##  [92] &amp;quot;| state.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                       
##  [93] &amp;quot;&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                               
##  [94] &amp;quot;&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                               
##  [95] &amp;quot;&amp;gt;50K, &amp;lt;=50K.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                   
##  [96] &amp;quot;&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                               
##  [97] &amp;quot;age: continuous.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                               
##  [98] &amp;quot;workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.&amp;quot;                                                                                                                                                                                                                                                                                                                              
##  [99] &amp;quot;fnlwgt: continuous.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                            
## [100] &amp;quot;education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.&amp;quot;                                                                                                                                                                                                                                                                              
## [101] &amp;quot;education-num: continuous.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                     
## [102] &amp;quot;marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.&amp;quot;                                                                                                                                                                                                                                                                                                                     
## [103] &amp;quot;occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.&amp;quot;                                                                                                                                                                                                          
## [104] &amp;quot;relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.&amp;quot;                                                                                                                                                                                                                                                                                                                                                              
## [105] &amp;quot;race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                             
## [106] &amp;quot;sex: Female, Male.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                             
## [107] &amp;quot;capital-gain: continuous.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                      
## [108] &amp;quot;capital-loss: continuous.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                      
## [109] &amp;quot;hours-per-week: continuous.&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                    
## [110] &amp;quot;native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&amp;amp;Tobago, Peru, Hong, Holand-Netherlands.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lucky for us the column names are at the end right after an empty string. &lt;hc&gt;names_file&lt;/hc&gt; is a character vector so we can use &lt;hc&gt;which()&lt;/hc&gt; to seek the pattern we want. Each name/description combination is seperated by a â€œ:â€, we can turn the end of this file in a small data dictionary with two columns; &lt;hc&gt;column_name&lt;/hc&gt; and &lt;hc&gt;description&lt;/hc&gt;. We can use &lt;hc&gt;read_delim()&lt;/hc&gt; to read in the file. We can use the parameters to name the wanted columns, how many lines to skip, and specify what seperates the columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;skip_to_here &amp;lt;- which(names_file == &amp;quot;&amp;quot;) %&amp;gt;%
  max()

skip_to_here&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 96&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_dictionary &amp;lt;- read_delim(
  file = names_url,
  col_names = c(&amp;quot;column_name&amp;quot;, &amp;quot;description&amp;quot;),
  skip = skip_to_here,
  delim = &amp;quot;:&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   column_name = col_character(),
##   description = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_dictionary&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 14 x 2
##    column_name    description                                              
##    &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;                                                    
##  1 age            &amp;quot; continuous.&amp;quot;                                           
##  2 workclass      &amp;quot; Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, â€¦
##  3 fnlwgt         &amp;quot; continuous.&amp;quot;                                           
##  4 education      &amp;quot; Bachelors, Some-college, 11th, HS-grad, Prof-school, Aâ€¦
##  5 education-num  &amp;quot; continuous.&amp;quot;                                           
##  6 marital-status &amp;quot; Married-civ-spouse, Divorced, Never-married, Separatedâ€¦
##  7 occupation     &amp;quot; Tech-support, Craft-repair, Other-service, Sales, Execâ€¦
##  8 relationship   &amp;quot; Wife, Own-child, Husband, Not-in-family, Other-relativâ€¦
##  9 race           &amp;quot; White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, â€¦
## 10 sex            &amp;quot; Female, Male.&amp;quot;                                         
## 11 capital-gain   &amp;quot; continuous.&amp;quot;                                           
## 12 capital-loss   &amp;quot; continuous.&amp;quot;                                           
## 13 hours-per-week &amp;quot; continuous.&amp;quot;                                           
## 14 native-country &amp;quot; United-States, Cambodia, England, Puerto-Rico, Canada,â€¦&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From information seen on the website about our data, we know there should be 15 rows. Exploring the data in the browser we know the category &lt;em&gt;income&lt;/em&gt; is the last column. With that information we will create an additional row to the data dictionary so we have all the columns we need.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_dictionary[nrow(data_dictionary) + 1, ] &amp;lt;- c(&amp;quot;income&amp;quot;, &amp;quot;&amp;gt;50K, &amp;lt;=50K&amp;quot;)

data_dictionary&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 15 x 2
##    column_name    description                                              
##  * &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;                                                    
##  1 age            &amp;quot; continuous.&amp;quot;                                           
##  2 workclass      &amp;quot; Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, â€¦
##  3 fnlwgt         &amp;quot; continuous.&amp;quot;                                           
##  4 education      &amp;quot; Bachelors, Some-college, 11th, HS-grad, Prof-school, Aâ€¦
##  5 education-num  &amp;quot; continuous.&amp;quot;                                           
##  6 marital-status &amp;quot; Married-civ-spouse, Divorced, Never-married, Separatedâ€¦
##  7 occupation     &amp;quot; Tech-support, Craft-repair, Other-service, Sales, Execâ€¦
##  8 relationship   &amp;quot; Wife, Own-child, Husband, Not-in-family, Other-relativâ€¦
##  9 race           &amp;quot; White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, â€¦
## 10 sex            &amp;quot; Female, Male.&amp;quot;                                         
## 11 capital-gain   &amp;quot; continuous.&amp;quot;                                           
## 12 capital-loss   &amp;quot; continuous.&amp;quot;                                           
## 13 hours-per-week &amp;quot; continuous.&amp;quot;                                           
## 14 native-country &amp;quot; United-States, Cambodia, England, Puerto-Rico, Canada,â€¦
## 15 income         &amp;gt;50K, &amp;lt;=50K&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am a fan of snake case, so we will replace the â€œ-â€ with &amp;quot;_&amp;quot;. Then we will use the data dictionaryâ€™s &lt;em&gt;column_name&lt;/em&gt; feature to label the test/train data we will be reading in. For data exploration and cleaning I will be combining the test and training sets. To ensure an easy split of data when it comes to modeling I will add a &lt;em&gt;group&lt;/em&gt; feature so we can later split the full data set with ease.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_dictionary &amp;lt;- data_dictionary %&amp;gt;% 
  mutate(column_name = str_replace_all(column_name, &amp;quot;-&amp;quot;, &amp;quot;_&amp;quot;))

train &amp;lt;- read_csv(
  file = data_url,
  col_names = data_dictionary$column_name
  ) %&amp;gt;% 
  mutate(group = &amp;quot;train&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   age = col_double(),
##   workclass = col_character(),
##   fnlwgt = col_double(),
##   education = col_character(),
##   education_num = col_double(),
##   marital_status = col_character(),
##   occupation = col_character(),
##   relationship = col_character(),
##   race = col_character(),
##   sex = col_character(),
##   capital_gain = col_double(),
##   capital_loss = col_double(),
##   hours_per_week = col_double(),
##   native_country = col_character(),
##   income = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When looking at the data online we see the first line of the test data has a line stating â€œ1x3 Cross validatorâ€. This will cause an error so we will skip the first line and then combine the two datasets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test &amp;lt;- read_csv(
  file = &amp;quot;/Users/kow/Downloads/adult.test&amp;quot;,
  skip = 1,
  col_names = data_dictionary$column_name
) %&amp;gt;% 
  mutate(group = &amp;quot;test&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   age = col_double(),
##   workclass = col_character(),
##   fnlwgt = col_double(),
##   education = col_character(),
##   education_num = col_double(),
##   marital_status = col_character(),
##   occupation = col_character(),
##   relationship = col_character(),
##   race = col_character(),
##   sex = col_character(),
##   capital_gain = col_double(),
##   capital_loss = col_double(),
##   hours_per_week = col_double(),
##   native_country = col_character(),
##   income = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data &amp;lt;- bind_rows(train, test)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;eda&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;EDA&lt;/h1&gt;
&lt;p&gt;&lt;hc&gt;skim()&lt;/hc&gt; from the &lt;strong&gt;skimr&lt;/strong&gt; package will give us the view &lt;hc&gt;summary()&lt;/hc&gt; or &lt;strong&gt;dplyr&lt;/strong&gt;â€™s &lt;hc&gt;glimpse()&lt;/hc&gt; does, but with additional information.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;skim(all_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Skim summary statistics
##  n obs: 48842 
##  n variables: 16 
## 
## â”€â”€ Variable type:character â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
##        variable missing complete     n min max empty n_unique
##       education       0    48842 48842   3  12     0       16
##           group       0    48842 48842   4   5     0        2
##          income       0    48842 48842   4   6     0        4
##  marital_status       0    48842 48842   7  21     0        7
##  native_country       0    48842 48842   1  26     0       42
##      occupation       0    48842 48842   1  17     0       15
##            race       0    48842 48842   5  18     0        5
##    relationship       0    48842 48842   4  14     0        6
##             sex       0    48842 48842   4   6     0        2
##       workclass       0    48842 48842   1  16     0        9
## 
## â”€â”€ Variable type:numeric â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
##        variable missing complete     n      mean        sd    p0      p25
##             age       0    48842 48842     38.64     13.71    17     28  
##    capital_gain       0    48842 48842   1079.07   7452.02     0      0  
##    capital_loss       0    48842 48842     87.5     403        0      0  
##   education_num       0    48842 48842     10.08      2.57     1      9  
##          fnlwgt       0    48842 48842 189664.13 105604.03 12285 117550.5
##  hours_per_week       0    48842 48842     40.42     12.39     1     40  
##       p50    p75    p100     hist
##      37       48      90 â–‡â–‡â–‡â–†â–ƒâ–‚â–â–
##       0        0   99999 â–‡â–â–â–â–â–â–â–
##       0        0    4356 â–‡â–â–â–â–â–â–â–
##      10       12      16 â–â–â–â–â–‡â–â–ƒâ–
##  178144.5 237642 1490400 â–‡â–…â–â–â–â–â–â–
##      40       45      99 â–â–â–â–‡â–â–â–â–&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;data-cleaning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Cleaning&lt;/h2&gt;
&lt;p&gt;I will now go through each variable and collapse character features where available. Hopefully this will increase our accuracy due to possible under-representation of groups in this data. The flow will be counting the data and grouping the factors to the best of my knowledge. Grouping will be with assumptions I make in my nogginâ€™.&lt;/p&gt;
&lt;div id=&#34;remove-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Remove Variables&lt;/h3&gt;
&lt;p&gt;I am unsure what &lt;em&gt;fnlwgt&lt;/em&gt; is exactly and &lt;em&gt;education_num&lt;/em&gt; is a numeric version of &lt;em&gt;education&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data &amp;lt;- all_data %&amp;gt;% 
  select(-fnlwgt, -education_num)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;education&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Education&lt;/h3&gt;
&lt;p&gt;â€œAn ordered factor with levels Preschool &amp;lt; 1st-4th &amp;lt; 5th-6th &amp;lt; 7th-8th &amp;lt; 9th &amp;lt; 10th &amp;lt; 11th &amp;lt; 12th &amp;lt; HS-grad &amp;lt; Prof-school &amp;lt; Assoc-acdm &amp;lt; Assoc-voc &amp;lt; Some-college &amp;lt; Bachelors &amp;lt; Masters &amp;lt; Doctorate.â€&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data %&amp;gt;% 
  count(education, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 16 x 2
##    education        n
##    &amp;lt;chr&amp;gt;        &amp;lt;int&amp;gt;
##  1 HS-grad      15784
##  2 Some-college 10878
##  3 Bachelors     8025
##  4 Masters       2657
##  5 Assoc-voc     2061
##  6 11th          1812
##  7 Assoc-acdm    1601
##  8 10th          1389
##  9 7th-8th        955
## 10 Prof-school    834
## 11 9th            756
## 12 12th           657
## 13 Doctorate      594
## 14 5th-6th        509
## 15 1st-4th        247
## 16 Preschool       83&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data &amp;lt;- all_data %&amp;gt;% 
  mutate(
    education = fct_collapse(
      education,
      &amp;quot;No Diploma&amp;quot; = c(&amp;quot;Preschool&amp;quot;, &amp;quot;1st-4th&amp;quot;, &amp;quot;5th-6th&amp;quot;, &amp;quot;7th-8th&amp;quot;, paste0(9:12, &amp;quot;th&amp;quot;)),
      &amp;quot;High School&amp;quot; = &amp;quot;HS-grad&amp;quot;,
      &amp;quot;Professional School&amp;quot; = &amp;quot;Prof-school&amp;quot;,
      &amp;quot;Associates&amp;quot; = c(&amp;quot;Assoc-acdm&amp;quot;, &amp;quot;Assoc-voc&amp;quot;), 
      &amp;quot;Some College&amp;quot; = &amp;quot;Some-college&amp;quot;,
      &amp;quot;Bachelors&amp;quot; = &amp;quot;Bachelors&amp;quot;,
      &amp;quot;Graduate&amp;quot; = c(&amp;quot;Doctorate&amp;quot;, &amp;quot;Masters&amp;quot;)
    )
  )

all_data %&amp;gt;% 
  count(education, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 2
##   education               n
##   &amp;lt;fct&amp;gt;               &amp;lt;int&amp;gt;
## 1 High School         15784
## 2 Some College        10878
## 3 Bachelors            8025
## 4 No Diploma           6408
## 5 Associates           3662
## 6 Graduate             3251
## 7 Professional School   834&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;marital-status&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Marital Status&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data %&amp;gt;% 
  count(marital_status, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 2
##   marital_status            n
##   &amp;lt;chr&amp;gt;                 &amp;lt;int&amp;gt;
## 1 Married-civ-spouse    22379
## 2 Never-married         16117
## 3 Divorced               6633
## 4 Separated              1530
## 5 Widowed                1518
## 6 Married-spouse-absent   628
## 7 Married-AF-spouse        37&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data &amp;lt;- all_data %&amp;gt;% 
  mutate(
    marital_status = fct_collapse(
      marital_status,
      &amp;quot;Not Married&amp;quot; = c(&amp;quot;Divorced&amp;quot;, &amp;quot;Separated&amp;quot;, &amp;quot;Widowed&amp;quot;),
      &amp;quot;Never Married&amp;quot; = &amp;quot;Never-married&amp;quot;,
      &amp;quot;Married&amp;quot; = c(&amp;quot;Married-civ-spouse&amp;quot;, &amp;quot;Married-spouse-absent&amp;quot;, &amp;quot;Married-AF-spouse&amp;quot;)
    )
  )

all_data %&amp;gt;% 
  count(marital_status, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##   marital_status     n
##   &amp;lt;fct&amp;gt;          &amp;lt;int&amp;gt;
## 1 Married        23044
## 2 Never Married  16117
## 3 Not Married     9681&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;income&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Income&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data %&amp;gt;% 
  count(income, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 2
##   income     n
##   &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;
## 1 &amp;lt;=50K  24720
## 2 &amp;lt;=50K. 12435
## 3 &amp;gt;50K    7841
## 4 &amp;gt;50K.   3846&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data &amp;lt;- all_data %&amp;gt;% 
  mutate(income = str_remove_all(income, fixed(&amp;quot;.&amp;quot;)))

all_data %&amp;gt;% 
  count(income, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   income     n
##   &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;
## 1 &amp;lt;=50K  37155
## 2 &amp;gt;50K   11687&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;native-country&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Native Country&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data %&amp;gt;% 
  count(native_country, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 42 x 2
##    native_country     n
##    &amp;lt;chr&amp;gt;          &amp;lt;int&amp;gt;
##  1 United-States  43832
##  2 Mexico           951
##  3 ?                857
##  4 Philippines      295
##  5 Germany          206
##  6 Puerto-Rico      184
##  7 Canada           182
##  8 El-Salvador      155
##  9 India            151
## 10 Cuba             138
## # â€¦ with 32 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;country_table &amp;lt;- all_data %&amp;gt;% 
  count(native_country, sort = TRUE) %&amp;gt;% 
  mutate(
    code = countrycode::countrycode(native_country,
                                    origin = &amp;quot;country.name&amp;quot;,
                                    destination = &amp;quot;continent&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in countrycode::countrycode(native_country, origin = &amp;quot;country.name&amp;quot;, : Some values were not matched unambiguously: ?, Columbia, England, Hong, Scotland, South, Yugoslavia&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;country_table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 42 x 3
##    native_country     n code    
##    &amp;lt;chr&amp;gt;          &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;   
##  1 United-States  43832 Americas
##  2 Mexico           951 Americas
##  3 ?                857 &amp;lt;NA&amp;gt;    
##  4 Philippines      295 Asia    
##  5 Germany          206 Europe  
##  6 Puerto-Rico      184 Americas
##  7 Canada           182 Americas
##  8 El-Salvador      155 Americas
##  9 India            151 Asia    
## 10 Cuba             138 Americas
## # â€¦ with 32 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;country_table %&amp;gt;% 
  mutate(
    code2 = case_when(
      native_country == &amp;quot;?&amp;quot; ~ &amp;quot;Unknown&amp;quot;,
      native_country == &amp;quot;South&amp;quot; ~ &amp;quot;Unknown&amp;quot;,
      native_country == &amp;quot;England&amp;quot; ~ &amp;quot;Europe&amp;quot;,
      native_country == &amp;quot;Columbia&amp;quot; ~ &amp;quot;Americas&amp;quot;,
      native_country == &amp;quot;Hong&amp;quot; ~ &amp;quot;Asia&amp;quot;,
      native_country == &amp;quot;Yugoslavia&amp;quot; ~ &amp;quot;Europe&amp;quot;,
      native_country == &amp;quot;Scotland&amp;quot; ~ &amp;quot;Europe&amp;quot;,
      TRUE ~ code
    )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 42 x 4
##    native_country     n code     code2   
##    &amp;lt;chr&amp;gt;          &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;   
##  1 United-States  43832 Americas Americas
##  2 Mexico           951 Americas Americas
##  3 ?                857 &amp;lt;NA&amp;gt;     Unknown 
##  4 Philippines      295 Asia     Asia    
##  5 Germany          206 Europe   Europe  
##  6 Puerto-Rico      184 Americas Americas
##  7 Canada           182 Americas Americas
##  8 El-Salvador      155 Americas Americas
##  9 India            151 Asia     Asia    
## 10 Cuba             138 Americas Americas
## # â€¦ with 32 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data &amp;lt;- all_data %&amp;gt;% 
  mutate(
    code = countrycode(
      native_country,
      origin = &amp;quot;country.name&amp;quot;,
      destination = &amp;quot;continent&amp;quot;
      ),
    native_continent = case_when(
      native_country == &amp;quot;?&amp;quot; ~ &amp;quot;Unknown&amp;quot;,
      native_country == &amp;quot;South&amp;quot; ~ &amp;quot;Unknown&amp;quot;,
      native_country == &amp;quot;England&amp;quot; ~ &amp;quot;Europe&amp;quot;,
      native_country == &amp;quot;Columbia&amp;quot; ~ &amp;quot;Americas&amp;quot;,
      native_country == &amp;quot;Hong&amp;quot; ~ &amp;quot;Asia&amp;quot;,
      native_country == &amp;quot;Yugoslavia&amp;quot; ~ &amp;quot;Europe&amp;quot;,
      native_country == &amp;quot;Scotland&amp;quot; ~ &amp;quot;Europe&amp;quot;,
      TRUE ~ code
    )
  ) %&amp;gt;% 
  select(-native_country, -code)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in countrycode(native_country, origin = &amp;quot;country.name&amp;quot;, destination = &amp;quot;continent&amp;quot;): Some values were not matched unambiguously: ?, Columbia, England, Hong, Scotland, South, Yugoslavia&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data %&amp;gt;% 
  count(native_continent, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 2
##   native_continent     n
##   &amp;lt;chr&amp;gt;            &amp;lt;int&amp;gt;
## 1 Americas         46086
## 2 Asia               981
## 3 Unknown            972
## 4 Europe             780
## 5 Oceania             23&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;occupation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Occupation&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data %&amp;gt;% 
  count(occupation, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 15 x 2
##    occupation            n
##    &amp;lt;chr&amp;gt;             &amp;lt;int&amp;gt;
##  1 Prof-specialty     6172
##  2 Craft-repair       6112
##  3 Exec-managerial    6086
##  4 Adm-clerical       5611
##  5 Sales              5504
##  6 Other-service      4923
##  7 Machine-op-inspct  3022
##  8 ?                  2809
##  9 Transport-moving   2355
## 10 Handlers-cleaners  2072
## 11 Farming-fishing    1490
## 12 Tech-support       1446
## 13 Protective-serv     983
## 14 Priv-house-serv     242
## 15 Armed-Forces         15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data &amp;lt;- all_data %&amp;gt;% 
  mutate(occupation = ifelse(occupation == &amp;quot;?&amp;quot;, &amp;quot;Unknown&amp;quot;, occupation))&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;race&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Race&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data %&amp;gt;% 
  count(race, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 2
##   race                   n
##   &amp;lt;chr&amp;gt;              &amp;lt;int&amp;gt;
## 1 White              41762
## 2 Black               4685
## 3 Asian-Pac-Islander  1519
## 4 Amer-Indian-Eskimo   470
## 5 Other                406&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Relationship&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data %&amp;gt;% 
  count(relationship, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   relationship       n
##   &amp;lt;chr&amp;gt;          &amp;lt;int&amp;gt;
## 1 Husband        19716
## 2 Not-in-family  12583
## 3 Own-child       7581
## 4 Unmarried       5125
## 5 Wife            2331
## 6 Other-relative  1506&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;sex&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sex&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data %&amp;gt;% 
  count(sex, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   sex        n
##   &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;
## 1 Male   32650
## 2 Female 16192&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;workclass&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Workclass&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data %&amp;gt;% 
  count(workclass, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 2
##   workclass            n
##   &amp;lt;chr&amp;gt;            &amp;lt;int&amp;gt;
## 1 Private          33906
## 2 Self-emp-not-inc  3862
## 3 Local-gov         3136
## 4 ?                 2799
## 5 State-gov         1981
## 6 Self-emp-inc      1695
## 7 Federal-gov       1432
## 8 Without-pay         21
## 9 Never-worked        10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data %&amp;gt;% 
  count(workclass, sort = TRUE) %&amp;gt;% 
  mutate(
    workclass2 = fct_collapse(
      workclass,
      &amp;quot;Private&amp;quot; = &amp;quot;Private&amp;quot;,
      &amp;quot;Government&amp;quot; = c(&amp;quot;Local-gov&amp;quot;, &amp;quot;State-gov&amp;quot;, &amp;quot;Federal-gov&amp;quot;),
      &amp;quot;Self-Employed&amp;quot; = c(&amp;quot;Self-emp-not-inc&amp;quot;, &amp;quot;Self-emp-inc&amp;quot;),
      &amp;quot;Unknown&amp;quot; = &amp;quot;?&amp;quot;,
      &amp;quot;No Income&amp;quot; = c(&amp;quot;Without-pay&amp;quot;, &amp;quot;Never-worked&amp;quot;)
    )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 3
##   workclass            n workclass2   
##   &amp;lt;chr&amp;gt;            &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt;        
## 1 Private          33906 Private      
## 2 Self-emp-not-inc  3862 Self-Employed
## 3 Local-gov         3136 Government   
## 4 ?                 2799 Unknown      
## 5 State-gov         1981 Government   
## 6 Self-emp-inc      1695 Self-Employed
## 7 Federal-gov       1432 Government   
## 8 Without-pay         21 No Income    
## 9 Never-worked        10 No Income&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data &amp;lt;- all_data %&amp;gt;% 
  mutate(
    workclass = fct_collapse(
      workclass,
      &amp;quot;Private&amp;quot; = &amp;quot;Private&amp;quot;,
      &amp;quot;Government&amp;quot; = c(&amp;quot;Local-gov&amp;quot;, &amp;quot;State-gov&amp;quot;, &amp;quot;Federal-gov&amp;quot;),
      &amp;quot;Self-Employed&amp;quot; = c(&amp;quot;Self-emp-not-inc&amp;quot;, &amp;quot;Self-emp-inc&amp;quot;),
      &amp;quot;Unknown&amp;quot; = &amp;quot;?&amp;quot;,
      &amp;quot;No Income&amp;quot; = c(&amp;quot;Without-pay&amp;quot;, &amp;quot;Never-worked&amp;quot;)
    )
  )&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-em&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot Emâ€™!&lt;/h2&gt;
&lt;p&gt;I made a &lt;em&gt;mapping&lt;/em&gt; function to use with &lt;em&gt;purrr&lt;/em&gt;â€™s &lt;hc&gt;map()&lt;/hc&gt; that will automate the graphing of all variables against our &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; variable, in this case &lt;span class=&#34;math inline&#34;&gt;\(y = income\)&lt;/span&gt;. I will not go over this in detail, if there is any questions or you would like to discuss this or any future code contact me on social media (found on the homepage).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_em &amp;lt;- as_mapper(~ {
  main_var &amp;lt;- parse_expr(.y)
  if (is.numeric(.x)) {
    p &amp;lt;- ggplot(all_data, aes(x = income, y = !!main_var)) +
      geom_boxplot() +
      labs(x = .y)
    
    p %&amp;gt;% snake_to()
    
  } else {
    bar_data &amp;lt;- all_data %&amp;gt;% 
      count(income, !!main_var, name = &amp;quot;count&amp;quot;) %&amp;gt;% 
      group_by(income) %&amp;gt;% 
      mutate(proportion = count / sum(count)) %&amp;gt;% 
      ungroup() %&amp;gt;% 
      arrange(desc(proportion)) %&amp;gt;% 
      mutate(
        !!main_var := factor(!!main_var, levels = unique(!!main_var)),
        income = factor(income, levels = unique(income))
        )
    
    p &amp;lt;- ggplot(
      data = bar_data,
      mapping = aes(x = income, y = proportion, fill = !!main_var)
      ) +
      geom_col() +
      labs(x = .y)
    
    p %&amp;gt;% snake_to()
  }
})

all_plots &amp;lt;- map2(all_data, names(all_data), plot_em)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;hc&gt;all_plots&lt;/hc&gt; holds all plots in a &lt;em&gt;list&lt;/em&gt;. We will go through one by one so we can have a deeper understanding of our data. We could use a graphing function such as &lt;hc&gt;pairs()&lt;/hc&gt; or &lt;hc&gt;ggpairs()&lt;/hc&gt;, however, I really wanted to look at all variables one by one closely and I got to practice some dplyr programming ;)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_plots[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On average there are more people that have an income of &lt;em&gt;&amp;gt;50k&lt;/em&gt; who are older.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_plots[[2]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Higher Proportion per Outcome (&lt;span class=&#34;math inline&#34;&gt;\(y = income\)&lt;/span&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Private: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Government: &lt;em&gt;&amp;gt;50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Self-Employed: &lt;em&gt;&amp;gt;50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Unknown: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_plots[[3]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Higher Proportion per Outcome (&lt;span class=&#34;math inline&#34;&gt;\(y = income\)&lt;/span&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;High School: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Bachelors: &lt;em&gt;&amp;gt;50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Some College: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;No Diploma: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Graduate: &lt;em&gt;&amp;gt;50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Associates: &lt;em&gt;About Even&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Professional School: &lt;em&gt;&amp;gt;50k&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_plots[[4]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Higher Proportion per Outcome (&lt;span class=&#34;math inline&#34;&gt;\(y = income\)&lt;/span&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Married: &lt;em&gt;&amp;gt;50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Never Married: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Not Married: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_plots[[5]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are a lot of different occupations so I will point out the obvious points.&lt;/p&gt;
&lt;p&gt;Higher Proportion per Outcome (&lt;span class=&#34;math inline&#34;&gt;\(y = income\)&lt;/span&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exec-managerial: &lt;em&gt;&amp;gt;50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Prof-specialty: &lt;em&gt;&amp;gt;50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Adm-clerical: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Sales: &lt;em&gt;&amp;gt;50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Machine-op-inspct: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Unknown: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_plots[[6]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Higher Proportion per Outcome (&lt;span class=&#34;math inline&#34;&gt;\(y = income\)&lt;/span&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Husband: &lt;em&gt;&amp;gt;50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Not-in-family: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Own-child: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Unmarried: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Wife: &lt;em&gt;&amp;gt;50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Other-relative: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_plots[[7]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-35-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Higher Proportion per Outcome (&lt;span class=&#34;math inline&#34;&gt;\(y = income\)&lt;/span&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;White: &lt;em&gt;&amp;gt;50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Black: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Asian-Pac-Islander: &lt;em&gt;&amp;gt;50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Amer-Indian-Eskimo: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Other: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_plots[[8]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-36-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Higher Proportion per Outcome (&lt;span class=&#34;math inline&#34;&gt;\(y = income\)&lt;/span&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Male: &lt;em&gt;&amp;gt;50k&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Female: &lt;em&gt;&amp;lt;=50k&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_plots[[9]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-37-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;No comment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_plots[[10]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-38-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Also no comment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_plots[[11]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-39-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see the average between line is around 40 for both &lt;em&gt;&amp;lt;=50k&lt;/em&gt; and &lt;em&gt;&amp;gt;50k&lt;/em&gt;. It appears on average more people in the &lt;em&gt;&amp;gt;50k&lt;/em&gt; group work more hours.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_plots[[14]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-40-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Mostly equal, Americas has a high N so it is hard to see a signal.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modeling&lt;/h1&gt;
&lt;div id=&#34;split-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Split the Data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train &amp;lt;- all_data %&amp;gt;% 
  filter(group == &amp;quot;train&amp;quot;) %&amp;gt;% 
  select(-group)

test &amp;lt;- all_data %&amp;gt;% 
  filter(group == &amp;quot;test&amp;quot;) %&amp;gt;% 
  select(-group)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;classification-tree&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Classification Tree&lt;/h2&gt;
&lt;p&gt;A classification tree uses a decision tree to take features (columns) of our data and comes up with a final value, in our case this the final value is a label. Decision trees can also handle numerical outputs. Generally this method is reffered to as CART, &lt;strong&gt;C&lt;/strong&gt;lassifican &lt;strong&gt;A&lt;/strong&gt;nd &lt;strong&gt;R&lt;/strong&gt;egression &lt;strong&gt;T&lt;/strong&gt;rees.&lt;/p&gt;
&lt;p&gt;A decision tree has a hierarchrical structure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Top: Root Node. This is where all of the data from our features begin their journey&lt;/li&gt;
&lt;li&gt;Bottom: Leaf Nodes. This is the finish line of the tree. Once the data reaches this point we obtain the final value. Either a numeric &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; value or a label for &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Middle: Internal Nodes. These are the nodes between the root and leaf nodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Advantages of Decision Trees:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Easy to interprit
&lt;ul&gt;
&lt;li&gt;If you can read a flow chart you can read a decision tree&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Training and prediction flows are easy to explain&lt;/li&gt;
&lt;li&gt;Easier to explain than a linear model&lt;/li&gt;
&lt;li&gt;Following the path of the tree allows for full explanation of the data&lt;/li&gt;
&lt;li&gt;Easy to interprit and visualize&lt;/li&gt;
&lt;li&gt;Handles categorical and numerical data with ease
&lt;ul&gt;
&lt;li&gt;No dummy data for categorical data&lt;/li&gt;
&lt;li&gt;No need to normalize or transform numeric data&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Missing data? No problem!
&lt;ul&gt;
&lt;li&gt;One method to handle missing data is when going down the branch and the value is NA for that feature it will randomly choose left or right and continue onward&lt;/li&gt;
&lt;li&gt;Another method involves going down both branches at the split with missing data and when the leaf nodes are reached you average the leafs values for the final prediction&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Robust to outliers&lt;/li&gt;
&lt;li&gt;Requires little data prep&lt;/li&gt;
&lt;li&gt;Can model non-linearity in the data&lt;/li&gt;
&lt;li&gt;Trains quickly on large data sets&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Disadvantages of Decision Trees:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Large trees can be hard to interpret&lt;/li&gt;
&lt;li&gt;Trees can have high variance
&lt;ul&gt;
&lt;li&gt;Causes model performance to be poor&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Trees overfit easily&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;model-selection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model Selection&lt;/h3&gt;
&lt;p&gt;We will use grid search for hyperperameter searching. Hyper perameters are the different knobs and settings we can tune to get the best results from our data. Grid search is an exhaustive and iterative search through a manually defined set of model hyperperameters.&lt;/p&gt;
&lt;p&gt;The two parameters we will be iterating over are &lt;hc&gt;minsplit&lt;/hc&gt; and &lt;hc&gt;maxdepth&lt;/hc&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;hc&gt;minsplit&lt;/hc&gt;: Minumum number of data points requeired to attempt a split&lt;/li&gt;
&lt;li&gt;&lt;hc&gt;maxdepth&lt;/hc&gt;: Maximum depth of our tree&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The goal of the grid search is to train model per row of the grid and then evaluate which model is best. The best model, in this instance, is accuracy of correct predictions. Below we will use some &lt;em&gt;tidy&lt;/em&gt; methods for training the models. This brilliant flow is thanks to the post located &lt;a href=&#34;https://drsimonj.svbtle.com/grid-search-in-the-tidyverse&#34;&gt;here&lt;/a&gt;!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid_search &amp;lt;- list(
  minsplit = seq(10, 45, 15),
  maxdepth = c(1, 5, 10, 25)
) %&amp;gt;% 
  cross_df()

grid_search&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 x 2
##    minsplit maxdepth
##       &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1       10        1
##  2       25        1
##  3       40        1
##  4       10        5
##  5       25        5
##  6       40        5
##  7       10       10
##  8       25       10
##  9       40       10
## 10       10       25
## 11       25       25
## 12       40       25&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have our grid we can train a model per row. We expect there to be 12 models at the end. We can define our own function to simply pass the grid to the &lt;hc&gt;control&lt;/hc&gt; parameter of &lt;hc&gt;rpart()&lt;/hc&gt; using &lt;hc&gt;â€¦&lt;/hc&gt;. We will use mutate to add a model per row to the &lt;hc&gt;grid_search&lt;/hc&gt; data frame,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dt_mod &amp;lt;- function(...) {
  rpart(
    formula = income ~ .,
    data = train,
    control = rpart.control(...)
    )
}

grid_search &amp;lt;- grid_search %&amp;gt;% 
  mutate(fit = pmap(grid_search, dt_mod))

grid_search&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 x 3
##    minsplit maxdepth fit    
##       &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt; 
##  1       10        1 &amp;lt;rpart&amp;gt;
##  2       25        1 &amp;lt;rpart&amp;gt;
##  3       40        1 &amp;lt;rpart&amp;gt;
##  4       10        5 &amp;lt;rpart&amp;gt;
##  5       25        5 &amp;lt;rpart&amp;gt;
##  6       40        5 &amp;lt;rpart&amp;gt;
##  7       10       10 &amp;lt;rpart&amp;gt;
##  8       25       10 &amp;lt;rpart&amp;gt;
##  9       40       10 &amp;lt;rpart&amp;gt;
## 10       10       25 &amp;lt;rpart&amp;gt;
## 11       25       25 &amp;lt;rpart&amp;gt;
## 12       40       25 &amp;lt;rpart&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;hc&gt;fit&lt;/hc&gt; column now holds all 12 of our models. If we print the model column we can see the normal output of an &lt;em&gt;rpart&lt;/em&gt; model for all 12 models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid_search$fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;scroll&#34;&gt;&lt;code&gt;## [[1]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 32561 7841 &amp;lt;=50K (0.7591904 0.2408096) *
## 
## [[2]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 32561 7841 &amp;lt;=50K (0.7591904 0.2408096) *
## 
## [[3]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 32561 7841 &amp;lt;=50K (0.7591904 0.2408096) *
## 
## [[4]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &amp;lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &amp;lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&amp;lt; 7073.5 17482  872 &amp;lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&amp;gt;=7073.5 318   12 &amp;gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &amp;lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &amp;lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&amp;lt; 5095.5 9807 2944 &amp;lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&amp;gt;=5095.5 522   10 &amp;gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &amp;gt;50K (0.27639892 0.72360108) *
## 
## [[5]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &amp;lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &amp;lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&amp;lt; 7073.5 17482  872 &amp;lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&amp;gt;=7073.5 318   12 &amp;gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &amp;lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &amp;lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&amp;lt; 5095.5 9807 2944 &amp;lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&amp;gt;=5095.5 522   10 &amp;gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &amp;gt;50K (0.27639892 0.72360108) *
## 
## [[6]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &amp;lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &amp;lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&amp;lt; 7073.5 17482  872 &amp;lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&amp;gt;=7073.5 318   12 &amp;gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &amp;lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &amp;lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&amp;lt; 5095.5 9807 2944 &amp;lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&amp;gt;=5095.5 522   10 &amp;gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &amp;gt;50K (0.27639892 0.72360108) *
## 
## [[7]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &amp;lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &amp;lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&amp;lt; 7073.5 17482  872 &amp;lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&amp;gt;=7073.5 318   12 &amp;gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &amp;lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &amp;lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&amp;lt; 5095.5 9807 2944 &amp;lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&amp;gt;=5095.5 522   10 &amp;gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &amp;gt;50K (0.27639892 0.72360108) *
## 
## [[8]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &amp;lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &amp;lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&amp;lt; 7073.5 17482  872 &amp;lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&amp;gt;=7073.5 318   12 &amp;gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &amp;lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &amp;lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&amp;lt; 5095.5 9807 2944 &amp;lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&amp;gt;=5095.5 522   10 &amp;gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &amp;gt;50K (0.27639892 0.72360108) *
## 
## [[9]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &amp;lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &amp;lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&amp;lt; 7073.5 17482  872 &amp;lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&amp;gt;=7073.5 318   12 &amp;gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &amp;lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &amp;lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&amp;lt; 5095.5 9807 2944 &amp;lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&amp;gt;=5095.5 522   10 &amp;gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &amp;gt;50K (0.27639892 0.72360108) *
## 
## [[10]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &amp;lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &amp;lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&amp;lt; 7073.5 17482  872 &amp;lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&amp;gt;=7073.5 318   12 &amp;gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &amp;lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &amp;lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&amp;lt; 5095.5 9807 2944 &amp;lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&amp;gt;=5095.5 522   10 &amp;gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &amp;gt;50K (0.27639892 0.72360108) *
## 
## [[11]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &amp;lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &amp;lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&amp;lt; 7073.5 17482  872 &amp;lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&amp;gt;=7073.5 318   12 &amp;gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &amp;lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &amp;lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&amp;lt; 5095.5 9807 2944 &amp;lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&amp;gt;=5095.5 522   10 &amp;gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &amp;gt;50K (0.27639892 0.72360108) *
## 
## [[12]]
## n= 32561 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 32561 7841 &amp;lt;=50K (0.75919044 0.24080956)  
##    2) relationship=Not-in-family,Other-relative,Own-child,Unmarried 17800 1178 &amp;lt;=50K (0.93382022 0.06617978)  
##      4) capital_gain&amp;lt; 7073.5 17482  872 &amp;lt;=50K (0.95012012 0.04987988) *
##      5) capital_gain&amp;gt;=7073.5 318   12 &amp;gt;50K (0.03773585 0.96226415) *
##    3) relationship=Husband,Wife 14761 6663 &amp;lt;=50K (0.54860782 0.45139218)  
##      6) education=No Diploma,Associates,High School,Some College 10329 3456 &amp;lt;=50K (0.66540807 0.33459193)  
##       12) capital_gain&amp;lt; 5095.5 9807 2944 &amp;lt;=50K (0.69980626 0.30019374) *
##       13) capital_gain&amp;gt;=5095.5 522   10 &amp;gt;50K (0.01915709 0.98084291) *
##      7) education=Bachelors,Graduate,Professional School 4432 1225 &amp;gt;50K (0.27639892 0.72360108) *&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-evaluation-and-tuning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model Evaluation and Tuning&lt;/h3&gt;
&lt;p&gt;Following the flow from the post linked above we can easily make make predictions on the test data and check the accuracy. We will seperate the independant variables and the dependent variable from the test data to compute the accuracy per model. We will add the accuracy per model to the &lt;hc&gt;grid_search&lt;/hc&gt; data using &lt;hc&gt;mutate&lt;/hc&gt; and arrange to show the best and most simple model. Simplicity is always preferred!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_accuracy &amp;lt;- function(fit, test_features, test_labels) {
  predicted &amp;lt;- predict(
    fit,
    test_features,
    type = &amp;quot;class&amp;quot;
    )
  
  mean(predicted == test_labels)
}

test_features &amp;lt;- test %&amp;gt;%
  select(-income)
test_labels   &amp;lt;- test %&amp;gt;% 
  pull(income)

grid_search &amp;lt;- grid_search %&amp;gt;%
  mutate(
    test_accuracy = map_dbl(
      fit,
      compute_accuracy,
      test_features,
      test_labels)
    ) %&amp;gt;%
  arrange(desc(test_accuracy), minsplit, maxdepth)
  
grid_search&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 x 4
##    minsplit maxdepth fit     test_accuracy
##       &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;          &amp;lt;dbl&amp;gt;
##  1       10        5 &amp;lt;rpart&amp;gt;         0.845
##  2       10       10 &amp;lt;rpart&amp;gt;         0.845
##  3       10       25 &amp;lt;rpart&amp;gt;         0.845
##  4       25        5 &amp;lt;rpart&amp;gt;         0.845
##  5       25       10 &amp;lt;rpart&amp;gt;         0.845
##  6       25       25 &amp;lt;rpart&amp;gt;         0.845
##  7       40        5 &amp;lt;rpart&amp;gt;         0.845
##  8       40       10 &amp;lt;rpart&amp;gt;         0.845
##  9       40       25 &amp;lt;rpart&amp;gt;         0.845
## 10       10        1 &amp;lt;rpart&amp;gt;         0.764
## 11       25        1 &amp;lt;rpart&amp;gt;         0.764
## 12       40        1 &amp;lt;rpart&amp;gt;         0.764&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see the best and most simplistic model has a &lt;hc&gt;minsplit&lt;/hc&gt; of 10 and a &lt;hc&gt;maxdepth&lt;/hc&gt; of 5. A lot of these models appear to split and look at the data in the same way due to all of them having the same accuracy. 84% is a pretty great result for such a simple model! But is it as good as it seems? We will revisit this number at the end of this section. Now we can use the &lt;strong&gt;rpart.plot&lt;/strong&gt; library to produce a nice visualize of the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_rpart &amp;lt;- grid_search %&amp;gt;% 
  pull(fit) %&amp;gt;% 
  .[[1]]

rpart.plot(
  x = best_rpart,
  yesno = 2,
  type = 0,
  extra = 0
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-46-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Using this flow we can take a person and their information and make a prediction on how much income they make. This is the power of a tree, we can interpreit the model and how it works every step of the way with ease!&lt;/p&gt;
&lt;p&gt;There are a few more things we can do to our model. &lt;strong&gt;Pruning&lt;/strong&gt; can possibly reduce the size of the tree (simple is ideal!) without losing and predictive power on classifications. We can do this by looking at the &lt;hc&gt;cp&lt;/hc&gt;, aka &lt;em&gt;complexity parameter&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;CP is a penalty term that helps control tree size. The smaller the CP the more complex a tree will be. The &lt;hc&gt;rpart()&lt;/hc&gt; function computes the 10-fold cross validation error of the model over various values for CP and stores the results in a table inside the model. We can plot the cross validation error across different values of CP using &lt;hc&gt;plotcp()&lt;/hc&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plotcp(best_rpart)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-47-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we can quickly get an estimate for the omptimal value of CP. To retreieve the optimal CP value we can look at the CP table stored in the model and look where the &lt;em&gt;xerror&lt;/em&gt; is minimized.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_rpart$cptable&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           CP nsplit rel error    xerror        xstd
## 1 0.12638694      0 1.0000000 1.0000000 0.009839876
## 2 0.06402245      2 0.7472261 0.7472261 0.008840225
## 3 0.03749522      3 0.6832037 0.6832037 0.008532119
## 4 0.01000000      4 0.6457085 0.6457085 0.008339388&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;opt_index &amp;lt;- which.min(best_rpart$cptable[, &amp;quot;xerror&amp;quot;])
cp_opt &amp;lt;- best_rpart$cptable[opt_index, &amp;quot;CP&amp;quot;]
cp_opt&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this value we can use &lt;hc&gt;prune()&lt;/hc&gt; to possibly trim our model. &lt;hc&gt;prune()&lt;/hc&gt; will return the optimized model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_rpart_opt &amp;lt;- prune(
  tree = best_rpart,
  cp = cp_opt
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Did this change anything?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(
  x = best_rpart_opt,
  yesno = 2,
  type = 0,
  extra = 0
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-51-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It appears in this instance, pruning did not change the visual of our model. Now onto the last part of of the evaluation is to look at a confusion matrix. A confusion matrix will show us a more detailed breakdown of correct and incorrect classifications for each class. We will use the &lt;strong&gt;caret&lt;/strong&gt; package to produce the matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class_prediction &amp;lt;- predict(
  object = best_rpart_opt,
  newdata = test,
  type = &amp;quot;class&amp;quot;
)

caret::confusionMatrix(
  data = class_prediction,
  reference = as.factor(test$income)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction &amp;lt;=50K  &amp;gt;50K
##      &amp;lt;=50K 11805  1901
##      &amp;gt;50K    630  1945
##                                           
##                Accuracy : 0.8445          
##                  95% CI : (0.8389, 0.8501)
##     No Information Rate : 0.7638          
##     P-Value [Acc &amp;gt; NIR] : &amp;lt; 2.2e-16       
##                                           
##                   Kappa : 0.5137          
##                                           
##  Mcnemar&amp;#39;s Test P-Value : &amp;lt; 2.2e-16       
##                                           
##             Sensitivity : 0.9493          
##             Specificity : 0.5057          
##          Pos Pred Value : 0.8613          
##          Neg Pred Value : 0.7553          
##              Prevalence : 0.7638          
##          Detection Rate : 0.7251          
##    Detection Prevalence : 0.8418          
##       Balanced Accuracy : 0.7275          
##                                           
##        &amp;#39;Positive&amp;#39; Class : &amp;lt;=50K           
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a lot of output to this function. We will focus on the 2x2 table, accuracy, sensitivity, and specificity.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Accuracy&lt;/strong&gt;: Correct Prediction %&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sensitivity&lt;/strong&gt;: True Positive Rate
&lt;ul&gt;
&lt;li&gt;As noted on the last line of the output, the â€œpositiveâ€ factor is &lt;em&gt;&amp;lt;=50k&lt;/em&gt;. So the decision tree get 95% of the predictions correct when they are positive. Sounds great, right?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Specificity&lt;/strong&gt;: True Negative Rate
&lt;ul&gt;
&lt;li&gt;The decision tree only has a 50% accuracy when it comes to incomes &lt;em&gt;&amp;gt;50k&lt;/em&gt;. What gives? Lets look at the count in each group. It appears the tree model labels a strong majority of predictions as &lt;em&gt;&amp;lt;=50k&lt;/em&gt;. Since most of the data belongs to this group and three likes to predict this label, it will have a high accuracy overall. This is why it is important to look at sensitivity and specificity. Our tree is not as great once we look at accuracy per label!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;other-accuracy-metrics-wip&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other Accuracy Metrics [WIP]&lt;/h3&gt;
&lt;p&gt;If accuracy is deceiving at times, what can we use? Great question person reading this. We can use AUCâ€¦&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bagging-random-forests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bagging &amp;amp; Random Forests&lt;/h2&gt;
&lt;p&gt;The drawbacks of decision trees is the high variability. A small change in the data, introduction to new data, or changing the test / training groups can change the model drastically. How can we approach these issues drawbacks? Enter bagging. :)&lt;/p&gt;
&lt;p&gt;Bagged trees averages many trees to reduce the variance. Combing multiple models like this, in this instance it is multiple decision trees, is called an &lt;strong&gt;ensemble model&lt;/strong&gt;. Another issue bagging helps with is overfitting.&lt;/p&gt;
&lt;p&gt;Bagging is an ensemble method and the term &lt;em&gt;bagging&lt;/em&gt; is shorthand for &lt;strong&gt;B&lt;/strong&gt;ootstrap &lt;strong&gt;AGG&lt;/strong&gt;regat&lt;strong&gt;ING&lt;/strong&gt;. Bagging uses &lt;a href=&#34;https://en.wikipedia.org/wiki/Bootstrapping_(statistics)&#34;&gt;bootstrap sampling&lt;/a&gt; and aggregates the individual models by averaging. Bootstrap sampling means sampling rows at random from the training dataset with replacement. Bagging also starts with all available features of the data.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;https://i.imgur.com/JYBMwak.png&#34; /&gt;
&lt;figcaption&gt;
&lt;a href=&#34;https://www.datacamp.com/courses/machine-learning-with-tree-based-models-in-r&#34;&gt;Image Source&lt;/a&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;With replacement means there is a chance the bootstrap sample will have the same observation more than once. This can also lead some rows be absent. This allows us to have â€œnewâ€ data. By doing this we can fit many different, yet similar, models.&lt;/p&gt;
&lt;div id=&#34;bagging-steps&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bagging Steps:&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Draw &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; samples with replacement from the original training set, where B is a number less than or equal to the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; ($N = total training rows $). A common choice for &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\frac{N}{2}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Train a decision tree on the newly created bootstrapped sample.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Repeat steps 1 and 2 multiple times. 10, 50, 100, 1000, etc. Typically, the more trees the better.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;predicting-with-bagging&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Predicting with Bagging&lt;/h3&gt;
&lt;p&gt;If we have 1,000 bootstrapped trees that makes up our ensemble model, each bootstrap tree may have different terminal nodes compared to the other. To generate a prediction with this model, the model will make a prediction with all 1,000 trees and then average the predictions together to end up with the final prediciton. Due to bagging averaging the predictions, this will lower the variability and lead to a better performing model. I found a picture that repsents a decision trees prediction versus a bagging ensemble method. Hint: the decision tree is on the left ;)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cdn.imgbin.com/17/0/7/imgbin-the-wisdom-of-crowds-wisdom-of-the-crowd-crowdsourcing-others-0Ehm17am7cWP7TMhUGkA7wXjv.jpg&#34; class=&#34;uri&#34;&gt;https://cdn.imgbin.com/17/0/7/imgbin-the-wisdom-of-crowds-wisdom-of-the-crowd-crowdsourcing-others-0Ehm17am7cWP7TMhUGkA7wXjv.jpg&lt;/a&gt;
&lt;img src=&#34;https://cdn.imgbin.com/17/0/7/imgbin-the-wisdom-of-crowds-wisdom-of-the-crowd-crowdsourcing-others-0Ehm17am7cWP7TMhUGkA7wXjv.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-about-random-forests&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What about Random Forests?&lt;/h3&gt;
&lt;p&gt;The only difference between bagging and random forest is that random forests will use a random subset of the datas features to build the models while bagging uses all features. This can lead to random forest removing a random feature and possibly finding an pattern that was not noticable while using all features. This parameter in functions to fit the models is called &lt;hc&gt;mtry&lt;/hc&gt;. When &lt;hc&gt;mtry&lt;/hc&gt; is equal to the count of independant variables then it is bagging. Anything less than that is random forest. We will fit an example model using the randomForest package for demonstration and when we need to use grid search for parameter selection we will use the &lt;strong&gt;ranger&lt;/strong&gt; package (ranger is written in C# and is much faster when scaling up). The model will be fit using the default parameters, aside from mtry for bagging, and to go over the output of the model.&lt;/p&gt;
&lt;p&gt;To demonstrate the time difference between fitting models between randomForest and ranger I will use the package &lt;strong&gt;tictoc&lt;/strong&gt; to measure the amount of time it takes to run the code and compare them when we transfer to ranger. Note: &lt;code&gt;randomForest()&lt;/code&gt; requires character columns to be of the factor data type. We will transfrom this into a different data set so we dont change our original data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bagging &amp;lt;- ncol(train) - 1 # Do not include the Y variable
train_rf &amp;lt;- train %&amp;gt;% 
  mutate_if(is.character, as.factor)

tic()
rf_example &amp;lt;- randomForest(
  formula = income ~ .,
  data = train_rf,
  mtry = bagging,
  ntree = 1000
)
toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 94.728 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic()
ranger_example &amp;lt;- ranger(
  formula = income ~ .,
  data = train_rf,
  mtry = bagging,
  num.trees = 1000,
  verbose = FALSE
)
toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 67.176 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For 1,000 trees it saves us almost 1 minute! To explore random forests and bagging, the &lt;code&gt;randomForest()&lt;/code&gt; function has nice built in functions for visualizing the model. When we want to find an optimized model we will grid search with &lt;strong&gt;ranger&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;randomforest-output&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;randomForest Output&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_example&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
##  randomForest(formula = income ~ ., data = train_rf, mtry = bagging,      ntree = 1000) 
##                Type of random forest: classification
##                      Number of trees: 1000
## No. of variables tried at each split: 12
## 
##         OOB estimate of  error rate: 15.35%
## Confusion matrix:
##       &amp;lt;=50K &amp;gt;50K class.error
## &amp;lt;=50K 22677 2043  0.08264563
## &amp;gt;50K   2956 4885  0.37699273&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we &lt;em&gt;print&lt;/em&gt; the model we can see the original call to create the data, the type of random forest, tree count, number of variables per split, OOB estimate, and a confusion matrix. The OOB estimate takes the prediction for each tree. For example, we trained 1,000 trees. The OOB&lt;/p&gt;
&lt;p&gt;We can see the error rate as the tree count increases using the &lt;hc&gt;plot()&lt;/hc&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(rf_example)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-09-13-fitting-tree-models_files/figure-html/unnamed-chunk-55-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that around 175 trees the error stops decreasing. This meaning we used too many trees, which translate to too much time spent waiting in terms of computation time. This means we can train a model with a much smaller amount of trees. This will be covered when we do a grid search, but it is nice to visual a model and see how many trees optimizes the error.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest-grid-search&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random Forest Grid Search&lt;/h3&gt;
&lt;p&gt;Just like the decision tree we will grid search for the best random forest model. The two parameters we will be tuning are &lt;em&gt;mtry&lt;/em&gt;, total variables to use, and &lt;em&gt;num.tree&lt;/em&gt;, the tree count. The choice for &lt;em&gt;mtry&lt;/em&gt; is a standard way of checking when I was in college. Due to what we saw in the first random forest model, we shouldnt need anywhere near 1000 trees, but it doesnâ€™t hurt to check. Remember, more trees = more computationally expensive for us. By us I mean my computer :) So I will only check a few tree values. Idealy these models will be trained when there is idle time and/or on a computer that can handle training a lot of large models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# bagging = total number of independent variables
rf_grid_search &amp;lt;- list(
  mtry = unique(
    ceiling(
      c(
        bagging,
        sqrt(bagging),
        bagging / 2)
      )
    ),
  num.trees = c(100, 300, 500)
) %&amp;gt;% 
  cross_df() %&amp;gt;% 
  arrange(mtry, num.trees)

rf_grid_search&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 2
##    mtry num.trees
##   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     4       100
## 2     4       300
## 3     4       500
## 4     6       100
## 5     6       300
## 6     6       500
## 7    12       100
## 8    12       300
## 9    12       500&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;rf-model-fitting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;RF Model Fitting&lt;/h3&gt;
&lt;p&gt;Again we will create a model function. This time we will use &lt;strong&gt;ranger&lt;/strong&gt; to fit our RF models. The pmap handles passing in the parameter using â€¦! The other method we could go with is a for loop, however, this is shorter and elegant.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_mod &amp;lt;- function(...) {
  ranger(income ~ ., data = train, ...)
}

rf_grid_search &amp;lt;- rf_grid_search %&amp;gt;% 
  mutate(fit = pmap(rf_grid_search, rf_mod))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Growing trees.. Progress: 97%. Estimated remaining time: 1 seconds.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_grid_search&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 3
##    mtry num.trees fit     
##   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;  
## 1     4       100 &amp;lt;ranger&amp;gt;
## 2     4       300 &amp;lt;ranger&amp;gt;
## 3     4       500 &amp;lt;ranger&amp;gt;
## 4     6       100 &amp;lt;ranger&amp;gt;
## 5     6       300 &amp;lt;ranger&amp;gt;
## 6     6       500 &amp;lt;ranger&amp;gt;
## 7    12       100 &amp;lt;ranger&amp;gt;
## 8    12       300 &amp;lt;ranger&amp;gt;
## 9    12       500 &amp;lt;ranger&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have 9 ranger models. We will look at accuracy. To reiterate, in a real life setting it may be more applicable to use AUC. Below, notice how &lt;hc&gt;predicted&lt;/hc&gt; needs to be set up. Before when we used predict it returned a vector. &lt;strong&gt;ranger&lt;/strong&gt;â€™s predict function returns a list object with &lt;strong&gt;predictions&lt;/strong&gt; being one of the list elements. We can use the &lt;hc&gt;pluck()&lt;/hc&gt; function from &lt;strong&gt;purrr&lt;/strong&gt; to grab the named element from the list.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_accuracy &amp;lt;- function(fit, test_features, test_labels) {
  predicted &amp;lt;- predict(fit, test_features) %&amp;gt;% 
    pluck(predictions)
  
  mean(predicted == test_labels)
}

test_features &amp;lt;- test %&amp;gt;% select(-income)
test_labels   &amp;lt;- test$income

rf_grid_search &amp;lt;- rf_grid_search %&amp;gt;%
  mutate(
    test_accuracy = map_dbl(
      fit,
      compute_accuracy,
      test_features,
      test_labels
      )
    ) %&amp;gt;%
  arrange(
    desc(test_accuracy),
    mtry, 
    num.trees
    )
  
rf_grid_search&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 4
##    mtry num.trees fit      test_accuracy
##   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;           &amp;lt;dbl&amp;gt;
## 1     4       300 &amp;lt;ranger&amp;gt;         0.862
## 2     4       100 &amp;lt;ranger&amp;gt;         0.861
## 3     4       500 &amp;lt;ranger&amp;gt;         0.861
## 4     6       300 &amp;lt;ranger&amp;gt;         0.855
## 5     6       500 &amp;lt;ranger&amp;gt;         0.854
## 6     6       100 &amp;lt;ranger&amp;gt;         0.854
## 7    12       100 &amp;lt;ranger&amp;gt;         0.846
## 8    12       500 &amp;lt;ranger&amp;gt;         0.846
## 9    12       300 &amp;lt;ranger&amp;gt;         0.845&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;best-model-evaluation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Best Model Evaluation&lt;/h3&gt;
&lt;p&gt;In terms of overall accuracy the best model has a &lt;em&gt;mtry&lt;/em&gt; value of 4 and uses a tree count of 300. Much like what we did with the rpart model we can make a confusion matrix to check our best models performance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_rf_mod &amp;lt;- rf_grid_search %&amp;gt;% 
  pull(fit) %&amp;gt;% 
  .[[1]]

best_rf_mod&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Ranger result
## 
## Call:
##  ranger(income ~ ., data = train, ...) 
## 
## Type:                             Classification 
## Number of trees:                  300 
## Sample size:                      32561 
## Number of independent variables:  12 
## Mtry:                             4 
## Target node size:                 1 
## Variable importance mode:         none 
## Splitrule:                        gini 
## OOB prediction error:             13.89 %&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;features &amp;lt;- test %&amp;gt;%
  select(-income)
labels   &amp;lt;- test %&amp;gt;% 
  pull(income)

class_prediction &amp;lt;- predict(
  best_rf_mod,
  test_features
) %&amp;gt;% 
  pluck(predictions)

caret::confusionMatrix(
  data = class_prediction,
  reference = as.factor(labels)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction &amp;lt;=50K  &amp;gt;50K
##      &amp;lt;=50K 11619  1423
##      &amp;gt;50K    816  2423
##                                           
##                Accuracy : 0.8625          
##                  95% CI : (0.8571, 0.8677)
##     No Information Rate : 0.7638          
##     P-Value [Acc &amp;gt; NIR] : &amp;lt; 2.2e-16       
##                                           
##                   Kappa : 0.5969          
##                                           
##  Mcnemar&amp;#39;s Test P-Value : &amp;lt; 2.2e-16       
##                                           
##             Sensitivity : 0.9344          
##             Specificity : 0.6300          
##          Pos Pred Value : 0.8909          
##          Neg Pred Value : 0.7481          
##              Prevalence : 0.7638          
##          Detection Rate : 0.7137          
##    Detection Prevalence : 0.8011          
##       Balanced Accuracy : 0.7822          
##                                           
##        &amp;#39;Positive&amp;#39; Class : &amp;lt;=50K           
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our previous simple model had:
- &lt;strong&gt;Accuracy&lt;/strong&gt;: 84.45%
- &lt;strong&gt;Sensitivity&lt;/strong&gt;: 94.93%
- &lt;strong&gt;Specificity&lt;/strong&gt;: 50.57%&lt;/p&gt;
&lt;p&gt;Our best random forest model does better in every category besides sensitivity, where it dropp by ~ 1%. The RF model was able to catch a lot more (13% more!) people who made more than $50,000! Fantastic! This is the power of the ensemble method of random forest, it will build multiple trees with different data and use different features to find different patterns that are used in the final outcome! I find that so awesome.&lt;/p&gt;
&lt;p&gt;Now there is still one more tree method to go! This next method isnâ€™t too different from the RF methods. Onto boosted trees!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;boosted-trees&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Boosted Trees&lt;/h2&gt;
&lt;p&gt;Work in progress..&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Neural Networks</title>
      <link>/2019/2019-06-15-introduction-to-neural-networks/</link>
      <pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/2019-06-15-introduction-to-neural-networks/</guid>
      <description>


&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://use.fontawesome.com/releases/v5.8.1/css/all.css&#34; integrity=&#34;sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;follow-me-on&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Follow Me On&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/KoderKow&#34;&gt; &lt;i class=&#39;fab fa-twitter fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://www.instagram.com/koderkow/&#34;&gt; &lt;i class=&#39;fab fa-instagram fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KoderKow/&#34;&gt;&lt;i class=&#39;fab fa-github fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://koderkow.rbind.io&#34;&gt;&lt;i class=&#39;fas fa-globe fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-are-neural-networks&#34;&gt;What Are Neural Networks?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#structure-of-a-neuron&#34;&gt;Structure of a Neuron&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#visual-representation-of-neural-networks&#34;&gt;Visual Representation of Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#layers-of-a-neural-network&#34;&gt;Layers of a Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feedforward-and-feedback-neural-networks&#34;&gt;Feedforward and Feedback Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#neural-network-application---the-faraway-way&#34;&gt;Neural Network Application - The Faraway Way&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#examine-the-estimated-weights&#34;&gt;Examine the Estimated Weights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#drawbacks-of-a-neural-network-model&#34;&gt;Drawbacks of a Neural Network Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weight-interpretation&#34;&gt;Weight Interpretation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#improving-the-fit&#34;&gt;Improving the Fit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#demonstration-wrap-up&#34;&gt;Demonstration Wrap-Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#final-model-fit&#34;&gt;Final Model Fit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;goal&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Goal&lt;/h1&gt;
&lt;p&gt;This post was first built as a Xaringan presentation for the final in the Contemporary Regression course at IUPUI (Indiana University Purdue University Indianapolis) which is part of the amazing Health Data Science program. I enjoyed making the presentation (link below) so much that I wanted to transfer the information I gathered into a blog post. I hope this serves as a launching point for those who have not had an opportunity to work with basic neural networks.&lt;/p&gt;
&lt;p&gt;Slides available here: &lt;strong&gt;&lt;a href=&#34;http://bit.ly/intro-to-neural-networks&#34; class=&#34;uri&#34;&gt;http://bit.ly/intro-to-neural-networks&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-are-neural-networks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What Are Neural Networks?&lt;/h1&gt;
&lt;p&gt;A Neural network, or Artificial Neural Network, is a set of algorithms, modeled loosely after the human brain to help recognize patterns. The brain has about &lt;span class=&#34;math inline&#34;&gt;\(1.5 \times 10^{10}\)&lt;/span&gt; neurons each with 10 to 104 connections called synapses. The speed of messages between neurons is about 100 m/sec which is much slower than CPU speed. The human brainâ€™s fastest reaction time is around 100 ms. A neuron computation time is 1â€“10 ms. Computation (to no surprise) is 10 times faster! That is just for one simple task!&lt;/p&gt;
&lt;p&gt;The original idea behind neural networks was to use a computer-based model of the human brain. We can recognize people in fractions of a second, but computers find this task difficult. So why not make the software more like the human brain? The brain model of connected neurons, first suggested by &lt;a href=&#34;http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html&#34;&gt;McCulloch and Pitts (1943)&lt;/a&gt;, is too simplistic given more recent research.&lt;/p&gt;
&lt;p&gt;As with artificial intelligence and the sentient takeover, the promise of neural networks is not matched by the reality of their performance. At least for nowâ€¦&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img class=&#34;lazyload&#34; src=&#34;https://img.buzzfeed.com/buzzfeed-static/static/2015-04/1/17/enhanced/webdr07/anigif_enhanced-29933-1427925503-3.gif&#34; width=40% /&gt;
&lt;figcaption&gt;
&lt;a href=&#34;https://www.buzzfeed.com/norbertobriceno/01101010101001001&#34;&gt;Image Source&lt;/a&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;Neural networks have various purposes such as biological models, hardware implementation for adaptive control and many more! We are interested in the data analysis application of neural network; classification, clustering methods, regression methods.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-a-neuron&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Structure of a Neuron&lt;/h1&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img class=&#34;lazyload&#34; src=&#34;https://3c1703fe8d.site.internapcdn.net/newman/csz/news/800/2018/2-whyareneuron.jpg&#34; width=80% /&gt;
&lt;figcaption&gt;
&lt;a href=&#34;https://medicalxpress.com/news/2018-07-neuron-axons-spindly-theyre-optimizing.html&#34;&gt;Image Source&lt;/a&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Dendrites&lt;/em&gt; receive signals&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Cell body&lt;/em&gt; sums up the inputs of the signals to generate output&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Axon terminals&lt;/em&gt; is the final output&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;visual-representation-of-neural-networks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visual Representation of Neural Networks&lt;/h1&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img class=&#34;lazyload&#34; src=&#34;https://cdn-images-1.medium.com/max/1600/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg&#34; width=40% /&gt;
&lt;figcaption&gt;
&lt;a href=&#34;https://cdn-images-1.medium.com/max/1600/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg&#34;&gt;Image Source&lt;/a&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;Here we can see how a neural network resembles a neuron. Neural networks are collections of thousands of these simple processing units that together perform useful computations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Inputs &lt;span class=&#34;math inline&#34;&gt;\(x_1, x_2, \dots, x_n\)&lt;/span&gt;&lt;/strong&gt;: independent variables&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Weights &lt;span class=&#34;math inline&#34;&gt;\(w_1, w_2, \dots, w_n\)&lt;/span&gt;:&lt;/strong&gt; learns the weights from the data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bias &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;:&lt;/strong&gt; the intercept&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Activation Function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;:&lt;/strong&gt; defines the output of the neuron
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Identity Function:&lt;/em&gt; linear fit&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Sigmoid Function:&lt;/em&gt; logistic fit, where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is binary&lt;/li&gt;
&lt;li&gt;&lt;em&gt;ReLu (rectified linear fit):&lt;/em&gt; linear fit, outputs 0 for negative values&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;layers-of-a-neural-network&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Layers of a Neural Network&lt;/h1&gt;
&lt;center&gt;
&lt;img class=&#34;lazyload&#34; src=&#34;https://i.stack.imgur.com/Kc50L.jpg&#34; width = 40%/&gt;
&lt;/center&gt;
&lt;p&gt;&lt;strong&gt;Input Layer:&lt;/strong&gt; the raw data, think of each â€œnodeâ€ as a variable in our data
&lt;strong&gt;Hidden Layer:&lt;/strong&gt; this is where the â€œblack magicâ€ happens&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each layer is focused on learning about the data&lt;/li&gt;
&lt;li&gt;We can think about each layer is learning about an aspect of the data&lt;/li&gt;
&lt;li&gt;Larger and more complex data may require multiple hidden layers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Output Layer:&lt;/strong&gt; the final output. This is generally a single output of the input(s)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;feedforward-and-feedback-neural-networks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Feedforward and Feedback Neural Networks&lt;/h1&gt;
&lt;div class=&#34;column-left&#34;&gt;
&lt;h3&gt;
Feedforward
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Signal goes from input to output&lt;/li&gt;
&lt;li&gt;No loops
&lt;br&gt;&lt;br&gt;
&lt;center&gt;
&lt;img class=&#34;lazyload&#34; src=&#34;https://thumbs.gfycat.com/EnviousNiftyCorydorascatfish-size_restricted.gif&#34; width=&#34;450px&#34; height=&#34;250px&#34;/&gt;
&lt;/center&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;column-right&#34;&gt;
&lt;h3&gt;
Feedback
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The neural network is recursive&lt;/li&gt;
&lt;li&gt;Data loops; goes in both directions
&lt;br&gt;&lt;br&gt;
&lt;center&gt;
&lt;img class=&#34;lazyload&#34; src=&#34;https://thumbs.gfycat.com/MiniatureDependentCob-size_restricted.gif&#34; width=&#34;450px&#34; height=&#34;250px&#34;/&gt;
&lt;/center&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=aircAruvnKk&#34;&gt;Image Source&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;neural-network-application---the-faraway-way&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Neural Network Application - The Faraway Way&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# libaries
library(nnet)
library(tidyverse)
library(ggthemes)
library(glue)
library(plotly)
library(kowr)
data(ozone, package = &amp;quot;faraway&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will start with three variables from the &lt;a href=&#34;https://cran.r-project.org/web/packages/faraway/faraway.pdf&#34;&gt;ozone data set from the faraway package&lt;/a&gt; for demonstrative purposes. We fit a feed-forward neural network with one hidden layer containing two units with a linear output unit:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2019)
nnet_model &amp;lt;- nnet(
  formula = O3 ~ temp + ibh + ibt,
  data = ozone,
  size = 2,
  linout = TRUE,
  trace = FALSE
  )&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nnet()&lt;/code&gt; fits a single-hidden-layer neural network&lt;/li&gt;
&lt;li&gt;&lt;code&gt;formula = O3 ~ temp + ibh + ibt&lt;/code&gt;: formula interface&lt;/li&gt;
&lt;li&gt;&lt;code&gt;data = ozone&lt;/code&gt;: data where the formula variables reside&lt;/li&gt;
&lt;li&gt;&lt;code&gt;size = 2&lt;/code&gt;: number of neurons in the hidden layer (this can be optimized)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;linout = TRUE&lt;/code&gt;: tells the model that it will have lienar output units&lt;/li&gt;
&lt;li&gt;&lt;code&gt;trace = FALSE&lt;/code&gt;: hides the printed out optimization information&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;neural-network-application&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Neural Network Application&lt;/h1&gt;
&lt;p&gt;If you repeat this, your result may differ slightly because of the random starting point of the algorithm, but you will likely get a similar result.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## RSS Value: 21099.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The RSS of 21099.4 is almost equal to &lt;span class=&#34;math inline&#34;&gt;\(\sum_i(y_i - \hat{y})^2\)&lt;/span&gt;, so the fit is not any better than the null model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum((ozone$O3 - mean(ozone$O3))^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 21115.41&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem lies with the initial selection of weights. It is hard to do this well when the variables have very different scales.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scale_ozone &amp;lt;- scale(ozone)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Due to the random starting point, the algorithm uses it may not actually converge. We will fit the model 100 times and pick the one that has the lowest RSS. In theory, this will choose a random starting point that leads to the true minimum.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2019)

## fit 100 nn models
results &amp;lt;- 1:100 %&amp;gt;%
  map(~nnet(
    formula = O3 ~ temp + ibh + ibt,
    data = scale_ozone,
    size = 2,
    linout = TRUE,
    trace = FALSE))

## get the index of the model with the lowest RSS
best_model_index &amp;lt;- results %&amp;gt;%
  map_dbl(~.x$value) %&amp;gt;%
  which.min()

## select best model
best_nn &amp;lt;- results[[best_model_index]]&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;/center&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_nn&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## a 3-2-1 network with 11 weights
## inputs: temp ibh ibt 
## output(s): O3 
## options were - linear output units&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Best RSS Value: 89.0786&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our &lt;code&gt;best_nn&lt;/code&gt; model has 11 parameters or weights (The parameters are shown below). For each of the parameters, there is an optimization that occurs. The surface optimization problem has multiple peaks and valleys. The model can converge on one of these minimums. This is why we run our model 100 times to test out multiple random starting points for our model, to hopefully find the global minimum!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;examine-the-estimated-weights&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Examine the Estimated Weights&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(best_nn)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## a 3-2-1 network with 11 weights
## options were - linear output units 
##  b-&amp;gt;h1 i1-&amp;gt;h1 i2-&amp;gt;h1 i3-&amp;gt;h1 
##  -1.14   0.95  -0.83  -0.28 
##  b-&amp;gt;h2 i1-&amp;gt;h2 i2-&amp;gt;h2 i3-&amp;gt;h2 
##  35.90 -18.32  63.10  34.91 
##   b-&amp;gt;o  h1-&amp;gt;o  h2-&amp;gt;o 
##  -1.83   4.51   0.69&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;: bias&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(i_x\)&lt;/span&gt;: input where &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the index of the variable&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(h_y\)&lt;/span&gt;: hidden neuron where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the index of the hidden neuron&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(o\)&lt;/span&gt;: output&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(i_1 \rightarrow h_1\)&lt;/span&gt;: refers to the link between input 1 and the first hidden neuron&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b \rightarrow o\)&lt;/span&gt;: is a one skip-layer connection, from the bias to the output&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;drawbacks-of-a-neural-network-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Drawbacks of a Neural Network Model&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Parameters are uninterpretable&lt;/li&gt;
&lt;li&gt;Not based on a probability model that expresses the structure and variation
&lt;ul&gt;
&lt;li&gt;No standard errors&lt;/li&gt;
&lt;li&gt;Some inference is possible with bootstrapping&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can get an &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; estimate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 - best_nn$value / sum((scale_ozone[, 1] - mean(scale_ozone[, 1]))^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7292443&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is similar to the additive model fit for these predictors that Faraway fits in previous chapters of his &lt;a href=&#34;https://www.amazon.com/Extending-Linear-Model-Generalized-Nonparametric/dp/149872096X&#34;&gt;book&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weight-interpretation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Weight Interpretation&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-06-15-introduction-to-neural-networks_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although the neural network weights may be difficult to interpret, we can get some sense of the effect of the predictors by observing the marginal effect of changes in one or more predictor as other predictors are held fixed. Here, we vary each predictor individually while keeping the other predictors fixed at their mean values. Because the data has been centered and scaled for the neural network fitting, we need to restore the original scales. As seen in the plots there are large discontinuities in the lines plots. This does not follow the linear trend we are expecting. Looking back at the weights of &lt;code&gt;summary(best_nn)&lt;/code&gt; we can see that some weights have extremely large values despite the scaling of the data, &lt;span class=&#34;math inline&#34;&gt;\(i_2 \rightarrow h_2 = 63.10\)&lt;/span&gt;. This means there is a lot of variability in this neuron. This is analogous to the collinearity problem in linear regression. The neural network is choosing these large values to optimize the fit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;improving-the-fit&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Improving the Fit&lt;/h1&gt;
&lt;p&gt;We can use a penalty function, as with smoothing splines, to obtain a more stable fit. Instead of minimizing MSE, we minimize: &lt;span class=&#34;math inline&#34;&gt;\(MSE + \lambda \sum\limits_{i} w_i^2\)&lt;/span&gt;. We can introduce a &lt;em&gt;weight decay&lt;/em&gt; to our neural network, this is a similar approach we take with ridge regression. Lets set &lt;span class=&#34;math inline&#34;&gt;\(\lambda = 0.001\)&lt;/span&gt; and create 100 models again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2019)
## fit 100 nn models
results_decay &amp;lt;- 1:100 %&amp;gt;%
  map(~nnet(
    formula = O3 ~ temp + ibh + ibt,
    data = scale_ozone,
    size = 2,
    linout = TRUE,
    trace = FALSE,
    `decay = 0.001`))

## get the index of the model with the lowest RSS
best_decay_index &amp;lt;- results_decay %&amp;gt;%
  map_dbl(~.x$value) %&amp;gt;%
  which.min()

## select best model
best_decay &amp;lt;- results[[best_decay_index]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_decay$value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 91.8121&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our previous value was 89.0786, our new RSS is a little bit higher. This is expected because we are sacrificing some of the fit for a more stable result.&lt;/p&gt;
&lt;div class=&#34;column-left&#34;&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2019-06-15-introduction-to-neural-networks_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;column-right&#34;&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img class=&#34;lazyload&#34; src=&#34;https://media1.tenor.com/images/154e8427624e163c030970a795b6f169/tenor.gif?itemid=5143620&#34; /&gt;
&lt;figcaption&gt;
&lt;a href=&#34;https://tenor.com/&#34;&gt;Image Source&lt;/a&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;br&gt;&lt;br&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;demonstration-wrap-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Demonstration Wrap-Up&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(best_decay)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## a 3-2-1 network with 11 weights
## options were - linear output units 
##  b-&amp;gt;h1 i1-&amp;gt;h1 i2-&amp;gt;h1 i3-&amp;gt;h1 
##   1.16  -0.63   0.42  -0.44 
##  b-&amp;gt;h2 i1-&amp;gt;h2 i2-&amp;gt;h2 i3-&amp;gt;h2 
##  13.09   2.46   8.71  -3.30 
##  b-&amp;gt;o h1-&amp;gt;o h2-&amp;gt;o 
##  1.55 -3.93  1.28&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The weights of the second row are not as extreme now. There is not a way to assess the significance of any of the variables. Neural networks do have interactions built in and these can be observed by the method we used before by varying two variables in our model at a time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-model-fit&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Final Model Fit&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2019)

## fit 100 nn models
results &amp;lt;- 1:100 %&amp;gt;%
  map(~nnet(
    formula = O3 ~ .,
    data = scale_ozone,
    size = 4,
    linout = TRUE,
    trace = FALSE))

## get the index of the model with the lowest RSS
best_model_index &amp;lt;- results_decay %&amp;gt;%
  map_dbl(~.x$value) %&amp;gt;%
  which.min()

## select best model
best_model &amp;lt;- results[[best_model_index]]
best_model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## a 9-4-1 network with 45 weights
## inputs: vh wind humidity temp ibh dpg ibt vis doy 
## output(s): O3 
## options were - linear output units&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(best_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## a 9-4-1 network with 45 weights
## options were - linear output units 
##  b-&amp;gt;h1 i1-&amp;gt;h1 i2-&amp;gt;h1 i3-&amp;gt;h1 i4-&amp;gt;h1 i5-&amp;gt;h1 i6-&amp;gt;h1 i7-&amp;gt;h1 i8-&amp;gt;h1 i9-&amp;gt;h1 
##  -1.47   0.29  -0.36   0.40   0.35  -0.29   0.49   0.29   0.15  -0.02 
##  b-&amp;gt;h2 i1-&amp;gt;h2 i2-&amp;gt;h2 i3-&amp;gt;h2 i4-&amp;gt;h2 i5-&amp;gt;h2 i6-&amp;gt;h2 i7-&amp;gt;h2 i8-&amp;gt;h2 i9-&amp;gt;h2 
##  40.19   6.03  15.36 -17.61 -13.86  17.07 -22.75   7.20  -6.12 -22.51 
##  b-&amp;gt;h3 i1-&amp;gt;h3 i2-&amp;gt;h3 i3-&amp;gt;h3 i4-&amp;gt;h3 i5-&amp;gt;h3 i6-&amp;gt;h3 i7-&amp;gt;h3 i8-&amp;gt;h3 i9-&amp;gt;h3 
## -11.16  -8.76  11.96   3.34   8.17  -1.90 -11.21  17.59 -25.30  -5.20 
##  b-&amp;gt;h4 i1-&amp;gt;h4 i2-&amp;gt;h4 i3-&amp;gt;h4 i4-&amp;gt;h4 i5-&amp;gt;h4 i6-&amp;gt;h4 i7-&amp;gt;h4 i8-&amp;gt;h4 i9-&amp;gt;h4 
##  38.72   6.96  13.35 -22.89 -34.75   9.49   5.97   7.99  -4.86   1.02 
##   b-&amp;gt;o  h1-&amp;gt;o  h2-&amp;gt;o  h3-&amp;gt;o  h4-&amp;gt;o 
##  -1.47   3.60   1.03   0.53  -0.62&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; estimate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 - best_model$value / sum((scale_ozone[,1] - mean(scale_ozone))^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8314248&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Neural networks cannot be used for inference&lt;/li&gt;
&lt;li&gt;Flexible, Easy to fit large complex data&lt;/li&gt;
&lt;li&gt;Can be easily overfit&lt;/li&gt;
&lt;li&gt;Truly a â€œblack boxâ€, plots only give a rough idea of what is happening with our data&lt;/li&gt;
&lt;li&gt;Lacks the diagnostics, model selection, and theory&lt;/li&gt;
&lt;li&gt;Initially developed to address real-life issues, not statistical issues&lt;/li&gt;
&lt;li&gt;â€œNeural networks can outperform their statistical competitors for some problems provided they are carefully used. However, one should not be fooled by the evocative name, as neural networks are just another tool in the box.â€ (Faraway, 2016)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;thanks&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Thanks!&lt;/h1&gt;
&lt;div id=&#34;slides&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Slides&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Slides&lt;/strong&gt;: &lt;em&gt;&lt;a href=&#34;http://bit.ly/intro-to-neural-networks&#34; class=&#34;uri&#34;&gt;http://bit.ly/intro-to-neural-networks&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Source Code&lt;/strong&gt;: &lt;em&gt;&lt;a href=&#34;https://github.com/KoderKow/intro-to-neural-networks&#34; class=&#34;uri&#34;&gt;https://github.com/KoderKow/intro-to-neural-networks&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;All Posts Source Code&lt;/strong&gt;: &lt;em&gt;&lt;a href=&#34;https://github.com/KoderKow/personal-site/tree/master/content/posts&#34; class=&#34;uri&#34;&gt;https://github.com/KoderKow/personal-site/tree/master/content/posts&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;References&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html&#34;&gt;McCulloch and Pitts (1943)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/community/tutorials/neural-network-models-r&#34;&gt;DataCamp: Neural Network Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/35345191/what-is-a-layer-in-a-neural-network&#34;&gt;Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Extending-Linear-Model-Generalized-Nonparametric/dp/149872096X&#34;&gt;Faraway: Extending the Linear Model with R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/yihui/xaringan&#34;&gt;R Package Used for Slides: Xaringan&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Fri, 10 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>

&lt;h1 id=&#34;hello&#34;&gt;Hello!&lt;/h1&gt;

&lt;p&gt;My name is Kyle Harris. I am a data analytics associate at CLA (CliftonLarsonAllen). I have a double major in Health Data Science and Informatics. My educational interests include solving data related issues, R programming, and machine learning.&lt;/p&gt;

&lt;p&gt;My personal hobbies include spending time with my girlfriend Lexi, petting my cats, playing video games, studying/reading about data science and machine learning, and listening to audio books.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Indy Civic Food Security Hackathon</title>
      <link>/2018/2018-07-03-indy-civic-food-security-hackathon/</link>
      <pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/2018-07-03-indy-civic-food-security-hackathon/</guid>
      <description>


&lt;div id=&#34;follow-me-on&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Follow Me On&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/KoderKow&#34;&gt; &lt;i class=&#39;fab fa-twitter fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://www.instagram.com/koderkow/&#34;&gt; &lt;i class=&#39;fab fa-instagram fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KoderKow/&#34;&gt;&lt;i class=&#39;fab fa-github fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://koderkow.rbind.io&#34;&gt;&lt;i class=&#39;fas fa-globe fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;indy-civic-food-security-hackathon&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Indy Civic Food Security Hackathon&lt;/h1&gt;
&lt;p&gt;This will be a short post (Hopefully a header image soon. Ha!). This is more of a reference to show what my friends and I from Health Data Science were able to build in 24 hours at the Indy Civic Food Security Hackathon! Link to their site is below.&lt;/p&gt;
&lt;p&gt;Our App was made with Rshiny to help show nearby food options for those in need. It will show U-Picks (open gardens), SNAP certified grocery stores, and farmers markets. We also put bus stops in to help with transportation needs.&lt;/p&gt;
&lt;p&gt;Go to the website &lt;a href=&#34;https://sugarshoedev.shinyapps.io/foodhackathon/&#34;&gt;here&lt;/a&gt; and you can enable the markers on the map by clicking the top right box to bring up a menu. There is also an option to get your current location as well as a voice assistance for those who need help using the application. There is one command; â€˜ZIPâ€™. If you want you can enable microphone access and say a zipcode in indianapolis you are interested in. Ex: â€œZIP 46202â€. This will result in the mapview to change to that zip code.&lt;/p&gt;
&lt;p&gt;There is also a skeleton logic of snap questions on the other tab we made to show the power and multiple options that can be implemented with RShiny.&lt;/p&gt;
&lt;p&gt;Again, this is a quick write up to share this great app we made! Shout out to Dan, Becca, and Satirios on this amazing application, and thank you to Indy Chamber for hosting this event!&lt;/p&gt;
&lt;p&gt;Links:
&lt;a href=&#34;https://indychamber.com/hack/&#34; class=&#34;uri&#34;&gt;https://indychamber.com/hack/&lt;/a&gt;
&lt;a href=&#34;https://sugarshoedev.shinyapps.io/foodhackathon/&#34; class=&#34;uri&#34;&gt;https://sugarshoedev.shinyapps.io/foodhackathon/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Making the Transcribing Process a Bit Simpler</title>
      <link>/2018/2018-06-10-making-the-transcribing-process-a-bit-simpler/</link>
      <pubDate>Sun, 10 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/2018-06-10-making-the-transcribing-process-a-bit-simpler/</guid>
      <description>


&lt;div id=&#34;follow-me-on&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Follow Me On&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/KoderKow&#34;&gt; &lt;i class=&#39;fab fa-twitter fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://www.instagram.com/koderkow/&#34;&gt; &lt;i class=&#39;fab fa-instagram fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KoderKow/&#34;&gt;&lt;i class=&#39;fab fa-github fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://koderkow.rbind.io&#34;&gt;&lt;i class=&#39;fas fa-globe fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro&lt;/h1&gt;
&lt;p&gt;For the past year I have had the pleasure of working with the data visualization team at IUPUI. I approached Professor Reda looking to explore the realm of research. He gave me the opportunity to sit in their team meetings for the spring semester. As fall came around I joined the team.&lt;/p&gt;
&lt;p&gt;For those who have worked in research, you may have crossed paths with user testing and the art of transcription. This was my first encounter with transcription and as the few jokes I heard about it, it wasnâ€™t/isnâ€™t that great of a time. With each user test being close to a hour, the time to transribe can be cumbersome. After some failed attemptes of hacky audio to text methods I ran into something that worked fairly well, YouTube!&lt;/p&gt;
&lt;p&gt;YouTube makes auto closed captions for videos. Using their automation I was able to &lt;b&gt;privately&lt;/b&gt; upload the videos and have them be transcribed fairly accurately. YouTube has a great UI for editing closed captions, allowing me to watch the full video with the auto generated captions to ensure accuracy. After that all that is left to do is download the file and convert it to a csv file.&lt;/p&gt;
&lt;p&gt;During this process I added certain code words at the start of sentences, for example if any of the research members spoke during the test I started the line with â€œstaff:â€. Later on this will help seperate the text into different columns (user text and staff text). If anyone is reading this that wants to do this for their transcription, feel free to make your own code words, you will have to change the R code a bit for the sbv to csv converter below. Sbv is the file format you save the YouTube close captionings.&lt;/p&gt;
&lt;p&gt;There can be difficulties with this method. A big one is audio quality. Making sure the audio is clear with no background noise is important. I have had multiple videos be translated to spanish and german due to the service not being able to depict the language at the start. Another issue is two people talking at once. The service will get confused and try to choose a word that sounds like the words that both people are saying. Making sure one person speaks at a time ensures a easy transcription process. At worst YouTube does not auto transcribe for you. You can still make your own file on the site. There are convenient hotkeys to use while transcribing. Again, the UI is great for this type of work.&lt;/p&gt;
&lt;p&gt;All around this method has saved our team a lot of time during the transcribing process. I wanted to share this method in hopes to save others who do transcriptions by ear. Below is a write I did for my team and the R code to convert the sbv files.&lt;/p&gt;
&lt;p&gt;I want to emphasize about privacy. Videos are uploaded privately and deleted right after the process is complete. On top of that there is no way to identify any user in the videos due to not being able to see them.&lt;/p&gt;
&lt;p&gt;Here is a great article that summarizes the work our research team does at IUPUI:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://soic.iupui.edu/news/reda-nsf-crii-grant/&#34;&gt;HCC faculty member awarded NSF grant for data visualization research&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Download the video(s) from box of the necessary session&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Upload them to YouTube (you will need a YouTube/google account)
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Make sure you set publish setting to PRIVATE!&lt;/li&gt;
&lt;li&gt;Also make sure you delete the video after you are completely done&lt;br /&gt;
Â &lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;After the video has finished upload, it may take several hours for YouTube to automatically generate close captioningâ€™s.
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Close captioningâ€™s will be our transcription in this case&lt;br /&gt;
Â &lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;To see if the automatic translation is complete, start at the homepage of YouTube
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Click on the top right icon (your profile icon) and click â€˜Creator Studio.â€™&lt;/li&gt;
&lt;li&gt;On the next page click â€˜Video Managerâ€™ on the left side.&lt;/li&gt;
&lt;li&gt;Find the video you need on the list and click â€˜Editâ€™ on that video&lt;/li&gt;
&lt;li&gt;On the top menu, all the way to the right, click on â€˜Subtitles/CCâ€™&lt;/li&gt;
&lt;li&gt;To the right of the video underneath the blue bar that says â€˜Add new subtitles or CCâ€™ it will have a title that says â€˜Publishedâ€™&lt;/li&gt;
&lt;li&gt;If it has gone according to plan, it will say â€˜English (Automatic)â€™. Click this.
&lt;ul&gt;
&lt;li&gt;If it lists a different language, it means the audio quality was not clear and you will have to either start doing the transcription manually, or if you are efficient with video/audio editing you can try to clean up the audio&lt;/li&gt;
&lt;li&gt;If it does not show up at all after 6 hours, it will be due to the audio quality not being clear (from what I have read online)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Â 
5. After clicking â€˜English (automatic)â€™ click edit on the top right
Â 
6. Get familiar with this layout!
a. You can click the times on the left to go directly to that spot on the timeline
b. You can edit the text directly while listening
c. Hotkeys I use:
- Shift + Space = play/pause
- Shift + Left/Right arrow = go forward/back a few seconds&lt;/p&gt;
&lt;p&gt;Â 
7. Listen to the whole video and read along with what is said in the video&lt;/p&gt;
&lt;p&gt;Â 
8. Important: When a member of the research team speaks start that line with â€˜staff:â€™
a. This is very important for the format of the final csv, I use this exact string to separate user text and staff text into separate columns in the R script.&lt;/p&gt;
&lt;p&gt;Â 
9. I manually insert a few important parts
a. When a video starts; â€˜MVI 0001 Startsâ€™
b. When a video ends; â€˜MVI 0001 Endsâ€™
c. When a question is SUBMITTED; â€˜Q. I want to seeâ€¦â€™
d. When a graph is generated â€˜**Graph is Generated**â€™&lt;/p&gt;
&lt;p&gt;Â 
10. Once you are happy with the entirety of the proof-read script, click on â€˜Actionsâ€™ which is located right above the text editing area.&lt;/p&gt;
&lt;p&gt;Â 
11. Save it as a â€˜.sbvâ€™ file&lt;/p&gt;
&lt;p&gt;Â 
12. Once you have saved it you can message me and I can quickly do the rest, upload it to the box [Symbol] (I will add the R .sbv to .csv script soon)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# If you do not have tidyverse:
# install.packages(&amp;#39;tidyverse&amp;#39;)
#
# CTRL + ENTER to run lines. run library and function.
#
# The last line is where you need to edit the
# string to files you want to edit

library(tidyverse)

sbv_to_csv &amp;lt;- function(filepath, savename){
  rawData &amp;lt;- read.table(filepath,
                    header = F,
                    sep = &amp;quot;\n&amp;quot;,
                    quote=&amp;quot;\&amp;quot;&amp;quot;)
  
  rawData &amp;lt;- droplevels(rawData)
  rawData &amp;lt;- rawData %&amp;gt;% mutate(dont_keep = str_detect(rawData$V1, &amp;quot;0:&amp;quot;))
  
  # We decided we did not team time stamps (dont_keep)
  df &amp;lt;- data.frame(words = rawData %&amp;gt;%
                     filter(dont_keep == F) %&amp;gt;% select(V1))
  colnames(df) &amp;lt;- &amp;#39;words&amp;#39;
  
  df$words &amp;lt;- as.character(df$words)
  
  df &amp;lt;- df %&amp;gt;% mutate(test = str_detect(words, &amp;#39;staff:&amp;#39;) |
                        str_detect(words, &amp;#39;Staff:&amp;#39;),
                      interviewer = ifelse(test, words, &amp;quot;&amp;quot;),
                      text = ifelse(test, &amp;quot;&amp;quot;, words))
  
# this line is for start/end time colums. fix select on line 26 to include this.
#transcribedData &amp;lt;- df %&amp;gt;% separate(time, c(&amp;quot;startTime&amp;quot;, &amp;quot;endTime&amp;quot;), &amp;quot;,&amp;quot;)
  
  transcribedData &amp;lt;- df %&amp;gt;% select(text, interviewer)
  
  write.csv(transcribedData, file = savename)
}

# First param is path to file, if you are on windows
# you will need to escape the \, so &amp;quot;\\&amp;quot; as shown below
#
# Second param is the output, make sure it ends with
# .csv. this can also be a file path to save where you want it
#
# By default it is your documents I believe, or you
# can set your working directory with setwd(*FOLDERPATH*)

sbv_to_csv(&amp;#39;C:...\\MVI_0115.sbv&amp;#39;, &amp;#39;0115.csv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;!-- ```{r echo=FALSE} --&gt;
&lt;!-- blogdown::shortcode(&#39;googleAdsense&#39;) --&gt;
&lt;!-- ``` --&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Accidental Drug Related Deaths</title>
      <link>/2018/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut/</link>
      <pubDate>Sun, 27 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;follow-me-on&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Follow Me On&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/KoderKow&#34;&gt; &lt;i class=&#39;fab fa-twitter fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://www.instagram.com/koderkow/&#34;&gt; &lt;i class=&#39;fab fa-instagram fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KoderKow/&#34;&gt;&lt;i class=&#39;fab fa-github fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://koderkow.rbind.io&#34;&gt;&lt;i class=&#39;fas fa-globe fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro&lt;/h1&gt;
&lt;p&gt;The opioid epidemic is a nationwide issue for the United States. Among the states Connecticut takes second place in terms of deaths from opioids in the years 2014-2015. As the years go on death rates are doubling from the abuse of opioids. Heroin overdoses continue to rise and now fentanyl is joining its ranks. Fentanyl has been getting mixed with heroin to increase its potency and in return it increases the chance of overdosing (Rondinone, 2017). This report will be looking over the top 5 opioids that are related to overdosing. The data used is from &lt;a href=&#34;https://data.ct.gov/Health-and-Human-Services/Accidental-Drug-Related-Deaths-2012-2017/rybz-nyjw&#34;&gt;data.ct.gov&lt;/a&gt;. This project was done during a class at IUPUI in November 2017.&lt;/p&gt;
&lt;hr /&gt;
&lt;div id=&#34;data-exploration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Exploration&lt;/h2&gt;
&lt;p&gt;We loaded the data into a dataframe called â€˜odDatâ€™. Our first step is to make more columns according to the date.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;odDat$yearMon &amp;lt;- as.yearmon(odDat$Date,&amp;#39;%m/%d/%Y&amp;#39;)
odDat$yearMonNum &amp;lt;- as.numeric(odDat$yearMon)
odDat$year &amp;lt;- year(mdy(odDat$Date))
odDat$month &amp;lt;- month(mdy(odDat$Date))
odDat$day &amp;lt;- day(mdy(odDat$Date))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will be looking into the location type for the drug over doses. Lets see a count of them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;odDat %&amp;gt;%
  group_by(Location) %&amp;gt;%
  summarise(n = n()) %&amp;gt;%
  arrange(desc(n))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Location
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Residence
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1854
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Hospital
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1175
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Other
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
528
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Convalescent Home
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Hospice
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Most of our data is within 3 locations. So we will be aggragating and grouping our data based off these 3 locations. We will be working with the dataset renamed as â€˜odâ€™.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;od &amp;lt;- odDat %&amp;gt;%
  filter(Location == &amp;quot;Residence&amp;quot; |
           Location == &amp;quot;Hospital&amp;quot; |
           Location == &amp;quot;Other&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets look at an example of the data we are wanting to find out about. For this example we will be looking at heroin overdoses in locations marked as â€œOther.â€&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;od %&amp;gt;%
  filter(Location == &amp;quot;Other&amp;quot; &amp;amp; Heroin == &amp;quot;Y&amp;quot;) %&amp;gt;%
  summarise(n = n())&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
330
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There are 330 counts of heroin overdose in the location known as other. I want to clean the overdose data to perform better insights. Any case of â€˜Yâ€™ or â€˜yâ€™ will be marked as a 1, if it is not a Y then we will label it as a 0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;od[,16:27] &amp;lt;- ifelse(od[,16:27] == &amp;quot;Y&amp;quot; | od[,16:27] == &amp;#39;y&amp;#39;,1, 0)
od[,29] &amp;lt;- ifelse(od[,29] == &amp;quot;Y&amp;quot; | od[,29] == &amp;#39;y&amp;#39;, 1, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this project we are interested in the top 5 drugs by count of overdoses. We will use dplyr to select the columns, and then summarize them to find the top 5 most common overdoses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;drugs &amp;lt;- od %&amp;gt;%
  select_(.dots = c(&amp;quot;Location&amp;quot;, &amp;quot;Heroin&amp;quot;, &amp;quot;Cocaine&amp;quot;, &amp;quot;Fentanyl&amp;quot;,
                    &amp;quot;Oxycodone&amp;quot;, &amp;quot;Oxymorphone&amp;quot;, &amp;quot;EtOH&amp;quot;,
                    &amp;quot;Hydrocodone&amp;quot;, &amp;quot;Benzodiazepine&amp;quot;, &amp;quot;Methadone&amp;quot;,
                    &amp;quot;Amphet&amp;quot;, &amp;quot;Tramad&amp;quot;, &amp;quot;Morphine..not.heroin.&amp;quot;,
                    &amp;quot;Any.Opioid&amp;quot;, &amp;quot;yearMon&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: select_() is deprecated. 
## Please use select() instead
## 
## The &amp;#39;programming&amp;#39; vignette or the tidyeval book can help you
## to program with select() : https://tidyeval.tidyverse.org
## This warning is displayed once per session.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;drugCount &amp;lt;- drugs %&amp;gt;%
  summarise(Heroin = sum(Heroin),
            Cocaine = sum(Cocaine),
            Fenentanyl = sum(Fentanyl),
            Oxycodone = sum(Oxycodone),
            Oxymorphone = sum(Oxymorphone),
            Etoh = sum(EtOH),
            Hydrocodone = sum(Hydrocodone),
            Benzodiazepine = sum(Benzodiazepine),
            Methadone = sum(Methadone),
            Amphet = sum(Amphet),
            Tramad = sum(Tramad))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Display the top 10 opioids.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;drugCount %&amp;gt;%
  gather() %&amp;gt;%
  top_n(10) %&amp;gt;%
  arrange(desc(value))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
key
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
value
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Heroin
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1926
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fenentanyl
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1102
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Cocaine
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
990
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Benzodiazepine
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
877
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Etoh
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
768
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Oxycodone
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
489
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Methadone
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
333
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Hydrocodone
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
97
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Oxymorphone
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
95
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Amphet
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
80
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The top 5 in order of highest to lowest is heroin, fentanyl, cocaine, benzodiazepine, and etOh. It is no surprise to see heroin and fentanyl are on top. Looking at the information for these opioids we are able to look into where the most common area for overdosing is to happen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;drugs5 &amp;lt;- od %&amp;gt;%
  select_(.dots = c(&amp;quot;Location&amp;quot;, &amp;quot;Heroin&amp;quot;, &amp;quot;Cocaine&amp;quot;,
                    &amp;quot;Fentanyl&amp;quot;, &amp;quot;EtOH&amp;quot;, &amp;quot;Benzodiazepine&amp;quot;,
                    &amp;quot;yearMon&amp;quot;))

drugs5 %&amp;gt;% summarize(Residence = sum(Location == &amp;#39;Residence&amp;#39;),
                     Hospital = sum(Location == &amp;#39;Hospital&amp;#39;),
                     Other = sum(Location == &amp;#39;Other&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Residence
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Hospital
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Other
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1854
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1175
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
528
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Residence has the most occurances, followed by hospital then other. Now lets look at how location is distributed among the top 5 drugs. We will do this with two different bar plots. The first will be a normal bar plot, with each bar representing counts of location per drug.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;drugs5Loc &amp;lt;- drugs5 %&amp;gt;%
  group_by(Location) %&amp;gt;%
  summarize(nHeroin = sum(Heroin),
            nFentanyl = sum(Fentanyl),
            nCocaine = sum(Cocaine),
            nBenzodiazepine = sum(Benzodiazepine),
            nEtOh = sum(EtOH)) %&amp;gt;%
  gather(opioid, count, 2:6)

drugs5Loc$opioid &amp;lt;- str_replace(drugs5Loc$opioid, &amp;#39;n&amp;#39;, &amp;#39;&amp;#39;)

ggplot(data = drugs5Loc,
       aes(x = reorder(opioid, count), y = count, fill = Location)) +
  geom_bar(stat = &amp;#39;identity&amp;#39;, position=&amp;#39;dodge&amp;#39;,
           color = &amp;#39;black&amp;#39;) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  labs(x = &amp;#39;opioid&amp;#39;, y = &amp;#39;Count&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The other bar plot I will use is a proportion based bar plot so it is easier to see how location makes up each opiod.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = drugs5Loc, aes(x = opioid, fill = Location, y = count)) +
  geom_bar(position = &amp;#39;fill&amp;#39;, stat = &amp;#39;identity&amp;#39;) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  ylab(&amp;#39;Proportion&amp;#39;) +
  xlab(&amp;#39;opioid&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at these graphs we can confirm that residence makes up most cases for each drug. Generally hospital and other are lower counts, with hospital being the next common and then other. Now that we have an understanding of the top opioids and locations that they occur, it is time to look into overdose counts over the months and years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yearMonDrug &amp;lt;- od %&amp;gt;%
  group_by(yearMon, Location) %&amp;gt;%
  summarize(Heroin = sum(Heroin),
            Cocaine = sum(Cocaine),
            Fentanyl = sum(Fentanyl),
            Etoh = sum(EtOH),
            Benzo = sum(Benzodiazepine)) %&amp;gt;%
  gather(opioid, count, 3:7)

yearMonDrug %&amp;gt;%
  ggplot(aes(x = yearMon, y = count, fill = opioid)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  facet_grid(opioid ~ Location) +
  labs(y = &amp;#39;Count&amp;#39;, x = &amp;#39;Year&amp;#39;) +
  scale_colour_gdocs() + theme_gdocs()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that fentanyl has had a huge spike in overdoses in 2016. Before late 2015 fentanyl is one of the lowest overdosed drugs. This truly shines the light on how fentanyl is a major epidemic. There is a constant increase for all drugs in all locations with a very few observations of strong decreases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yearMonDrug %&amp;gt;%
  ggplot(aes(x = yearMon, y = count, fill = opioid)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;) + facet_wrap( ~ Location) +
  labs(x = &amp;quot;Year&amp;quot;, y = &amp;quot;Count&amp;quot;) +
  theme(axis.text.x  = element_text(angle=45)) + 
          scale_x_continuous(breaks=c(2012, 2013, 2014, 2015, 2016
                                      ,2017))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can clearly see the constant increase of overdoses. Next we will facet the plot to look at each opioid individually.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot to show count of top 5 drugs over 2012-2017
yearMonDrug %&amp;gt;%
  ggplot(aes(x = yearMon, y = count, fill = opioid)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;) + facet_wrap( ~ opioid) +
  labs(x = &amp;quot;Year&amp;quot;, y = &amp;quot;Count&amp;quot;) +
  theme(axis.text.x  = element_text(angle=45)) + 
          scale_x_continuous(breaks=c(2012, 2013, 2014, 2015, 2016
                                      ,2017))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at all of the data we can see that the primary location for overdosing is residence. In all locations we can see that the count is increasing. Heroin and fentanyl are the most common as well. It is interesting to see the huge spike in fentanyl count over the past few years. The large increase is so much it now has a higher count than heroin.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation&lt;/h2&gt;
&lt;p&gt;Moving on past visual representation and aggragation of our data we will now look into the last 6 months of our data set, more specifically the first 6 months of 2017. I will be using these 6 months to simulate the next few months that are not present in our data. Janurary overdose count will be used for July, Feburary for August, ect. I Will be simulated the first 3 months for each drug. Then I will break down the simulation by location for the first three months, then I am interested in doing heroin, the most popular opioid, by location for the last 6 months.&lt;/p&gt;
&lt;p&gt;This process may not make sense logically. This was a requirement by the project to implement simulation. It is moreso theory in how to apply simulation in this kind of setting and how to interpret it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)

od2 &amp;lt;- od %&amp;gt;%
  select(Heroin, Cocaine, Fentanyl, EtOH,
         Benzodiazepine, yearMon, year, month, Location)

# values for overdoses in Jan 2017
od20171 &amp;lt;- od2 %&amp;gt;%
  group_by(Location) %&amp;gt;%
  filter(year == 2017 &amp;amp; month == 1) %&amp;gt;%
  summarize(nHeroin = sum(Heroin),
            nCocaine = sum(Cocaine),
            nFentanyl = sum(Fentanyl),
            nEtoh = sum(EtOH),
            nBenzo = sum(Benzodiazepine))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Location
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
nHeroin
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
nCocaine
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
nFentanyl
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
nEtoh
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
nBenzo
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Hospital
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
19
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Other
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Residence
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This shows us the counts of opioids in Janurary 2017.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sim for july
set.seed(char2seed(&amp;#39;Kyle&amp;#39;))
# sims for each drug by total
july2017 &amp;lt;- data.frame(Heroin=rpois(1000,sum(od20171$nHeroin)),
                       Cocaine=rpois(1000,sum(od20171$nCocaine)),
                       Fent=rpois(1000,sum(od20171$nFentanyl)),
                       Etoh=rpois(1000,sum(od20171$nEtoh)),
                       Benzo=rpois(1000,
                                sum(od20171$nBenzo)))
july2017 %&amp;gt;%
  gather() %&amp;gt;% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = &amp;quot;free&amp;quot;) +
  geom_histogram(color=&amp;#39;black&amp;#39;,binwidth = 2,aes(fill=key)) +
  theme(legend.position=&amp;quot;none&amp;quot;) +
  labs(x=&amp;quot;opioid&amp;quot;, y=&amp;quot;Overdose Count&amp;quot;, title = &amp;quot;July 2017 Simulation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is five separate simulations for each drug to show, in a simulated manner, the count of overdoses for July 2017 based off the counts of Janurary 2017. Taking information from these simulations we can see that Benzo will have around 10 overdoses on the low end and we can expect a high of around 30 (10-30). Following this template, cocaine will have 20-42, Etoh 5-17, fentanyl will have 35 - 70, and heroin 31 - 65. Looking at summaries of our data before it is to be expected heroin would be higher. I will do a few more simulations for August and September to see if fentanyl is on top and to see if our simulated results show an increase/decrease to other.&lt;/p&gt;
&lt;p&gt;To avoid summarizing each set, I will add the results of observations into a data frame and print it out.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;od20172 &amp;lt;- od2 %&amp;gt;%
  group_by(Location) %&amp;gt;%
  filter(year == 2017 &amp;amp; month == 2) %&amp;gt;%
  summarize(nHeroin = sum(Heroin),
            nCocaine = sum(Cocaine),
            nFentanyl = sum(Fentanyl),
            nEtoh = sum(EtOH),
            nBenzodiazepine = sum(Benzodiazepine))

set.seed(char2seed(&amp;#39;Kyle&amp;#39;))
# sims for each drug by total
august2017 &amp;lt;- data.frame(Heroin=rpois(1000,sum(od20172$nHeroin)),
                       Cocaine=rpois(1000,sum(od20172$nCocaine)),
                       Fent=rpois(1000,sum(od20172$nFentanyl)),
                       Etoh=rpois(1000,sum(od20172$nEtoh)),
                       Benzo=rpois(1000,
                                sum(od20172$nBenzodiazepine)))
august2017 %&amp;gt;%
  gather() %&amp;gt;% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = &amp;quot;free&amp;quot;) +
  geom_histogram(color=&amp;#39;black&amp;#39;,binwidth = 2,aes(fill=key)) +
  theme(legend.position=&amp;quot;none&amp;quot;) +
  labs(x=&amp;quot;opioid&amp;quot;, y=&amp;quot;Overdose Count&amp;quot;, title = &amp;quot;August 2017 Simulation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;od20173 &amp;lt;- od2 %&amp;gt;%
  group_by(Location) %&amp;gt;%
  filter(year == 2017 &amp;amp; month == 3) %&amp;gt;%
  summarize(nHeroin = sum(Heroin),
            nCocaine = sum(Cocaine),
            nFentanyl = sum(Fentanyl),
            nEtoh = sum(EtOH),
            nBenzodiazepine = sum(Benzodiazepine))

set.seed(char2seed(&amp;#39;Kyle&amp;#39;))
# sims for each drug by total
sept2017 &amp;lt;- data.frame(Heroin=rpois(1000,sum(od20173$nHeroin)),
                       Cocaine=rpois(1000,sum(od20173$nCocaine)),
                       Fent=rpois(1000,sum(od20173$nFentanyl)),
                       Etoh=rpois(1000,sum(od20173$nEtoh)),
                       Benzo=rpois(1000,
                                sum(od20173$nBenzodiazepine)))
sept2017 %&amp;gt;%
  gather() %&amp;gt;% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = &amp;quot;free&amp;quot;) +
  geom_histogram(color=&amp;#39;black&amp;#39;,binwidth = 2,aes(fill=key)) +
  theme(legend.position=&amp;quot;none&amp;quot;) +
  labs(x=&amp;quot;opioid&amp;quot;, y=&amp;quot;Overdose Count&amp;quot;, title = &amp;quot;September 2017 Simulation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simRange %&amp;gt;%
  filter(opioid %in% c(&amp;quot;Fentanyl&amp;quot;, &amp;quot;Heroin&amp;quot;, &amp;quot;Benzo&amp;quot;,
                                   &amp;quot;Cocaine&amp;quot;, &amp;quot;Etoh&amp;quot;)) %&amp;gt;%
  arrange(opioid)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
opioid
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Month
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Range
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Benzo
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jul
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
10-30
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Benzo
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Aug
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
12-36
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Benzo
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sept
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
15-40
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Cocaine
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jul
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
20-42
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Cocaine
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Aug
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
15-42
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Cocaine
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sept
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
18-44
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Etoh
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jul
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5-17
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Etoh
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Aug
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0-17
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Etoh
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sept
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0-20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fentanyl
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jul
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
35-70
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fentanyl
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Aug
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
30-66
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fentanyl
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sept
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
37-76
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Heroin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jul
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
41-65
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Heroin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Aug
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
22-50
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Heroin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sept
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
27-56
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To conclude our simulations for the next three months we can see that benzo overdoses are on the rise. Cocaine is roughly constant. Etoh is also constant and is the lower overdose count. Fentanyl seems to be the most common overdose in 2017, it has a large range and the highest values. Heroin is now the second most common in these 2017 simulations, and it appears there is a big drop off from 41 to the 20s of overdose counts.&lt;/p&gt;
&lt;p&gt;To finalize the simulations I will simulate July 2017 for heroin based off of location.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;locationSim &amp;lt;- od2 %&amp;gt;%
  group_by(Location) %&amp;gt;%
  filter(year == 2017 &amp;amp; month == 1) %&amp;gt;%
  summarize(nHeroin = sum(Heroin))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Location
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
nHeroin
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Hospital
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Other
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Residence
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;julyLocSim &amp;lt;- data.frame(Hospital = rpois(1000,
                                        locationSim$nHeroin[1]),
                         Other = rpois(1000,
                                         locationSim$nHeroin[2]),
                         Residence = rpois(1000,
                                        locationSim$nHeroin[3]))

julyLocSim %&amp;gt;%
  gather() %&amp;gt;% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = &amp;quot;free&amp;quot;) +
  geom_histogram(color=&amp;#39;black&amp;#39;,binwidth = 2,aes(fill=key)) +
  theme(legend.position=&amp;quot;none&amp;quot;) +
  labs(x=&amp;quot;Location&amp;quot;, y=&amp;quot;Overdose Count&amp;quot;, title = &amp;quot;July 2017 Location Simulation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/posts/2018-05-27-accidental-drug-related-deaths-2012-2017-in-connecticut_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The distribution of these poisson simulations look normal. This is because they have a smaller lamba due to the total count for the month being split into three separate locations. For heroin count in the hospital in July 20717 is no less than 8 and expect a high of 26. For ther it is 4 to 23, and 10 to 37 for residence. From this breakdown we are able to confirm what was observed earlier with our data exploration. According to our simulations residence is the most common location, followed by hospital, then other. I wanted to show a simulation breakdown according to location for a particular drug. This can be applied to any drug of interest.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;predictive-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Predictive Modeling&lt;/h2&gt;
&lt;p&gt;Making a model for this dataset will require aggregating the data and getting the total counts by month and year. This is because we will be using a poisson model and it will require total counts. If we try to use the base data set we will not get the results we are expecting because opioids are composed of 0â€™s and 1â€™s. The training set, all data before 2017, will be used to train our model. For this example we will be looking specifically at the opioid fentanyl. Then I will test the model against a test set, all data after 2017. At the end the error will be calculated to show us the accuracy of the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trainod &amp;lt;- od2 %&amp;gt;%
  filter(year &amp;lt; 2017) %&amp;gt;%
  group_by(Location, year, month) %&amp;gt;%
  summarize(nHeroin = sum(Heroin),
            nCocaine = sum(Cocaine),
            nFentanyl = sum(Fentanyl),
            nEtoh = sum(EtOH),
            nBenzodiazepine = sum(Benzodiazepine))

model1 &amp;lt;- glm(nFentanyl ~ Location + year + month, data = trainod,
            family = &amp;#39;poisson&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
term
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
estimate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
std.error
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
statistic
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
p.value
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
(Intercept)
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1826.96
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
77.81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-23.48
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
LocationOther
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.86
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-7.48
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
LocationResidence
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.49
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
year
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.91
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23.50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
month
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.42
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The model, model1, will be used now predict the first month of 2017. Showing the actual data and the predicted data will show if the model is predicting close to the real data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new.predict1 &amp;lt;- predict(model1,newdata = data.frame(year=2016, Location = c(&amp;#39;Hospital&amp;#39;, &amp;#39;Other&amp;#39;, &amp;#39;Residence&amp;#39;), month=12), type = &amp;quot;response&amp;quot;)
names(new.predict1) &amp;lt;- c(&amp;#39;Hospital&amp;#39;,&amp;#39;Other&amp;#39;,&amp;#39;Residence&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# predicted values for Jan 2017 in all locations for fentanyl
new.predict1&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Hospital
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Other
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Residence
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.91
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
26.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Actual values for Jan 2017 in all locations for fentanyl.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;od2 %&amp;gt;%
  filter(year == 2017 &amp;amp; month == 1) %&amp;gt;%
  summarize(nFentanyl = sum(Fentanyl))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-condensed table-bordered&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
nFentanyl
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
52
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As shown after adding up the three individual predictions our predicted values are very close to the actual data. Exciting! To test the accuracy of the model I will now run the model against the test data, which is data in 2017. This way we will be able to gauge the accuracy of our model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;testod &amp;lt;- od2 %&amp;gt;%
  filter(year &amp;lt; 2017) %&amp;gt;%
  group_by(Location, year, month) %&amp;gt;%
  summarize(nHeroin = sum(Heroin),
            nCocaine = sum(Cocaine),
            nFentanyl = sum(Fentanyl),
            nEtoh = sum(EtOH),
            nBenzodiazepine = sum(Benzodiazepine))

predTest &amp;lt;- predict(model1, newdata = testod, type = &amp;quot;response&amp;quot;) 

errTest &amp;lt;- mean((testod$nFentanyl - predTest) ^ 2)

sqrt(errTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model has a 2.5 error, meaning our predications are potentially off by 2.5. I believe this is accurate for our model. This could be very useful to predict upcoming months for the overdoses. The model is for heroin, but it can easily be switched with any of the other drugs in the data set.&lt;/p&gt;
&lt;p&gt;To summarize our findings there is definite proof that overdose counts are on the rise. Residences are common locations for these accidents. Heroin is the most common in the data set, but it is slowly getting over taken by fentanyl. We are able to simulate ranges for expected overdoses in the data. This could be used for professionals to get an idea what they are looking for in the future. Finally we built a fairly accurate model. This model is able to take in year, month, and location and return a value that represents the count of overdoses. I believe this model could be useful to see if counts increase in the future. Furthermore after more data is acquired it would be great to see predicted counts go down because this would show overdose situations are under control.&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;sources-related-links&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sources + Related Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.courant.com/breaking-news/hc-cdc-fentanyl-overdoses-20170105-story.html&#34;&gt;Nicholas Rondinone, CDC: Connecticut Second In Percent Increase Of Synthetic Opioid Deaths Rate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.middletownpress.com/news/article/Sen-Murphy-in-Haddam-on-opioid-crisis-This-12548669.php&#34;&gt;Jeff Mill, Sen.Â Murphy in Haddam on opioid crisis: â€˜This is getting worse, not betterâ€™&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.law.com/ctlawtribune/sites/ctlawtribune/2018/01/24/attorneys-for-connecticut-cities-pledge-to-fight-any-move-to-opioid-mdl/&#34;&gt;Robert Storace, Attorneys for CT Cities Pledge Fight Against Move to Opioid MDL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.courant.com/politics/hc-pol-new-connecticut-laws-january-20171227-story.html&#34;&gt;Russel Blair, Eight New Laws That Take Effect in Connecticut on Jan.Â 1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- ```{r echo=FALSE} --&gt;
&lt;!-- blogdown::shortcode(&#39;googleAdsense&#39;) --&gt;
&lt;!-- ``` --&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Web Scraping World of Warcraft Weapons</title>
      <link>/2018/2018-05-24-web-scraping-world-of-warcraft-weapons/</link>
      <pubDate>Thu, 24 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/2018-05-24-web-scraping-world-of-warcraft-weapons/</guid>
      <description>


&lt;div id=&#34;follow-me-on&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Follow Me On&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/KoderKow&#34;&gt; &lt;i class=&#39;fab fa-twitter fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://www.instagram.com/koderkow/&#34;&gt; &lt;i class=&#39;fab fa-instagram fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KoderKow/&#34;&gt;&lt;i class=&#39;fab fa-github fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://koderkow.rbind.io&#34;&gt;&lt;i class=&#39;fas fa-globe fa-3x&#39;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro&lt;/h1&gt;
&lt;p&gt;The main goal of this project was to learn how to use the R libraries rvest and RSelenium. The web scraping would be taking place on the website wowhead.com. There is about 10,000 rows of data to collect.&lt;/p&gt;
&lt;div id=&#34;learning-points&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Learning Points&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Navigate the DOM to capture desired element values&lt;/li&gt;
&lt;li&gt;Used Râ€™s &lt;em&gt;RSelenium&lt;/em&gt; and &lt;em&gt;rvest&lt;/em&gt; library to capture data&lt;/li&gt;
&lt;li&gt;Construct a R script to automate the complete web scraping process&lt;/li&gt;
&lt;li&gt;Made a data frame with captured data to save to a csv file for data analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;web-scraping-process&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Web Scraping Process&lt;/h2&gt;
&lt;p&gt;The web scraper goes to the wowhead.com weapon page. There is about 10,000 weapons worth of data. All of the data we want to collect from these weapons are not presented in the rows of the table. For each page, there are 50 weapons.&lt;/p&gt;
&lt;div id=&#34;example-of-table-view-of-weapons&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Example of table view of weapons&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;/img/wow-scraper/table_view.png&#34; width=&#34;80%&#34; height=&#34;80%&#34;&gt;&lt;/p&gt;
&lt;p&gt;In order to capture all of the data we want we will need to click on each row, collect the data, go back to the previous page, and then click on the next item.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-of-weapon-view-for-all-attributes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Example of weapon view for all attributes&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;/img/wow-scraper/item_view.png&#34; width=&#34;80%&#34; height=&#34;80%&#34;&gt;&lt;/p&gt;
&lt;p&gt;At the start I was collecting the data directly from the elements shown on the top left box of the page. Issues started coming up when a weapon had a different amount of attributes. For example, if a weapon had two attributes such as strength and agility, that takes up 2 spaces on the DOM. Then if the next weapon had only one attribute, the script would return an error. After exploring the DOM I was able to find all of the data in a &amp;lt;noscript&amp;gt; tag. All of the attributes had different code tags (ie;â€˜!â€“stat3â€“&amp;gt;\+â€™ = agility). After sorting out all of the tags I was able to use string manipulation to return the information desired.&lt;/p&gt;
&lt;p&gt;First time running the script all the way through with no errors showed the next hurdle to jump. The original default weapon data only showed about 1,000 of the 10,000 total weapons. The next step was to automate a filtering process to collect all the weapons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;by-default-the-page-displays-around-1000-max&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;By default, the page displays around 1000 max&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;/img/wow-scraper/item_displayed_vs_total.png&#34; width=&#34;80%&#34; height=&#34;80%&#34;&gt;&lt;/p&gt;
&lt;p&gt;I decided to search for weapons based on their item level. For example, levels 1-25, 26-35, etc. This would return all the weapons in chunks. Once the first chunk of weapons was collected the script clears the filter and inputs the next number range.&lt;/p&gt;
&lt;p&gt;After thinking this project was complete I showed the data to my professor. We found out the weapon attributes were not correct. My string manipulation was not collecting stats correctly that had a comma in it (ex: 1,247). This was a simple fix, but then I noticed something completely different on the website. When you click a weapon and then go back, the order the weapons on the table were not static. The order changed! Eventually I found a solution, constantly sort the table by the weapon name everytime the script goes back to the main page.&lt;/p&gt;
&lt;p&gt;Unfortunately during this time, World of Warcraft introduced weapon scalability to their weapons. To put it simply and in terms I understand, a large amount of weapons in their game now â€˜upgradeâ€™ as you level up. For example, I am level one and have a level one sword that does 1-2 damage and has +1 strength. When I level up to level two the sword now does 2-3 damage and has +2 strength. World of Warcraft has 120 levels. This now means a lot of the weapons have different stats on 120 levels. This really put a thud on this web scraper.&lt;/p&gt;
&lt;p&gt;After talking to my professor, if I chose to continue with this project and wanted to do data analysis on the data, I could make an assumption that all weapons would be looked at in a maxed level view (player level = 120). Sadly, however, this will require going back and changing how a lot of the data is collected.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;reflection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reflection&lt;/h2&gt;
&lt;p&gt;It is unfortunate to get hit by that update, but the process of collecting the data and fixing all the issues that came up was an amazing learning experience. There is still possibility of data analysis with the old data I collected that had the comma issue.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;The data (.csv file) is on my &lt;a href=&#34;https://github.com/KoderKow/wow_scraper&#34;&gt;github&lt;/a&gt;. I wanted to share what the data looked like and that I had success in gathering a lot of different attributes for every weapon.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;I want to thank Professor Lourens for sharing his idea for this project. Professor Lourens assisted me on learning web scraping through rvest/RSelenium and addressed errors I came across.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# scraping libraries
library(RSelenium)
library(rvest)
# XML, allows access to xpath selector
library(XML)
# string splice library
library(stringr)

## create a remoteDriver class object
rD &amp;lt;- rsDriver()

## extract client from rD
remDr &amp;lt;- rD[[&amp;quot;client&amp;quot;]]

## Navigate to a page 
remDr$maxWindowSize()
remDr$navigate(&amp;quot;http://www.wowhead.com/weapons&amp;quot;)
## If the popup shows up, click it to get rid of it

popUp &amp;lt;- remDr$findElement(using =&amp;#39;css&amp;#39;, value = &amp;#39;#item-gallery-listview &amp;gt; div.walkthrough-details-wrapper &amp;gt; div &amp;gt; div &amp;gt; div &amp;gt; div.walkthrough-details-text.right &amp;gt; div &amp;gt; a&amp;#39;)
popUp$clickElement()

## ---- Loop ----
i &amp;lt;- 1 # for rows 1 - 50
j &amp;lt;- 1 # for data frame rows
f &amp;lt;- 1 # for filter seach count
rlvl90counter &amp;lt;- 0
filterHighValue &amp;lt;- &amp;#39;&amp;#39;
timeVec &amp;lt;- numeric()

# Total Weapons Amount
totalWeapons &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = &amp;#39;//*[@id=&amp;quot;tab-items&amp;quot;]/div[1]/div[2]&amp;#39;)
totalWeapons &amp;lt;- totalWeapons$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
totalWeapons &amp;lt;- as.numeric(str_split(totalWeapons, pattern = &amp;#39; &amp;#39;)[[1]][1])

# test for filter automation
reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;1&amp;#39;)) 
reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;25&amp;#39;)) 

# apply filter button
applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
applyFilter$clickElement()

# filtered weapon count
filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)

# sort table by name to avoid random order on back()s
sortByItem &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = &amp;#39;//*[@id=&amp;quot;tab-items&amp;quot;]/div[2]/table/thead/tr/th[2]/div/a/span&amp;#39;)
sortByItem$clickElement()

initialStart &amp;lt;- Sys.time()

while(j &amp;lt;= 10000){
  
  startTime &amp;lt;- Sys.time()
  # sort back to original name order
  sortByItem &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = &amp;#39;//*[@id=&amp;quot;tab-items&amp;quot;]/div[2]/table/thead/tr/th[2]/div/a/span&amp;#39;)
  sortLogic &amp;lt;- sortByItem$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
  if(sortLogic != &amp;quot;&amp;lt;span&amp;gt;Name&amp;lt;/span&amp;gt;&amp;quot;){
    sortByItem$clickElement()
    if(sortLogic != &amp;quot;&amp;lt;span&amp;gt;Name&amp;lt;/span&amp;gt;&amp;quot;){
      sortByItem$clickElement()
    }
  }
  
  URLData &amp;lt;- remDr$getCurrentUrl()
  
  # find all weapons ----
  print(paste(&amp;#39;select weapons table round&amp;#39;, j, &amp;#39;..&amp;#39;))
  weapons &amp;lt;- remDr$findElements(using = &amp;quot;css&amp;quot;, value = &amp;quot;#tab-items &amp;gt; div.listview-scroller &amp;gt; table &amp;gt; tbody &amp;gt; tr&amp;quot;)
  
  # select weapon i from list
  print(paste(&amp;#39;selecting weapon&amp;#39;, i, &amp;#39;..&amp;#39;))
  selectWeapon &amp;lt;- weapons[[i]]$findChildElement(using = &amp;quot;css&amp;quot;, value = &amp;quot;td:nth-child(3) &amp;gt; div &amp;gt; a&amp;quot;)
  
  # required lvl
  print(paste(&amp;#39;selecting weapon&amp;#39;, i, &amp;#39;required level...&amp;#39;))
  reqLvl &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = paste(&amp;quot;//*[@id=\&amp;quot;tab-items\&amp;quot;]/div[2]/table/tbody/tr[&amp;quot;, i, &amp;quot;]/td[5]&amp;quot;))
  reqLvl &amp;lt;- reqLvl$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
  reqLvl &amp;lt;- as.numeric(reqLvl)
  
  
  # click selected weapon
  print(paste(&amp;#39;clicking weapon&amp;#39;, i, &amp;#39;..&amp;#39;))
  selectWeapon$clickElement()
  
  # grab page&amp;#39;s url for item # (not sure if this is used anymore..)
  print(paste(&amp;#39;selecting URL&amp;#39;, i, &amp;#39;..&amp;#39;))
  URL &amp;lt;- remDr$getCurrentUrl()
  
  # split string for item number
  print(paste(&amp;#39;selecting item #&amp;#39;, i, &amp;#39;..&amp;#39;))
  itemNum &amp;lt;- str_split(str_split(URL, pattern = &amp;#39;=&amp;#39;)[[1]][2], pattern = &amp;#39;/&amp;#39;)[[1]][1]
  
  # Weapon Data from div[x]/noscript. x changes between 2 or 3 based on ads----
  weaponData &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = paste(&amp;quot;//*[@id=\&amp;quot;main-contents\&amp;quot;]/div[2]/noscript&amp;quot;, sep = &amp;quot;&amp;quot;))
  WeaponDataText &amp;lt;- weaponData$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
  
  # Weapon Name ----
  print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;name...&amp;#39;))
  weaponName &amp;lt;- remDr$findElement(using = &amp;quot;xpath&amp;quot;, value = paste(&amp;quot;//*[@id=&amp;#39;main-contents&amp;#39;]/div[2]/h1&amp;quot;, sep = &amp;quot;&amp;quot;))
  name &amp;lt;- weaponName$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
  name &amp;lt;- str_split(name, &amp;#39;&amp;lt;span&amp;#39;)[[1]][1]
  
  # Weapon Item Level ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;item level...&amp;#39;))
  weaponItemLevel &amp;lt;- remDr$findElement(using = &amp;quot;xpath&amp;quot;, value = paste(&amp;quot;/html/head/meta[5]&amp;quot;, sep = &amp;quot;&amp;quot;))
  itemLevel &amp;lt;- weaponItemLevel$getElementAttribute(&amp;#39;content&amp;#39;)
  itemLevel &amp;lt;- str_split(itemLevel,&amp;#39;item level of &amp;#39;)[[1]][2]
  itemLevel &amp;lt;- str_split(itemLevel,&amp;#39;. &amp;#39;)[[1]][1]
  
  # Upgrade ----
  weaponUpgrade &amp;lt;- str_split(WeaponDataText, &amp;#39;Level &amp;amp;lt;!--uindex--&amp;amp;gt;&amp;#39;)[[1]][2]
  weaponUpgrade &amp;lt;- str_trim(str_split(weaponUpgrade, &amp;#39;/&amp;#39;)[[1]][2])
  upgradeLvl &amp;lt;- str_split(weaponUpgrade, &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]
  
  # Weapon Bind on Pick Up ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i, &amp;#39;bind setting...&amp;#39;))
  bind &amp;lt;- str_split(WeaponDataText, &amp;#39;!--bo--&amp;amp;gt;&amp;#39;)
  bind &amp;lt;- str_split(bind[[1]][2], &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]
  
  # Weapon Hand Characteristic ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;hand value...&amp;#39;))
  hand &amp;lt;- str_split(WeaponDataText, &amp;#39;width=\&amp;quot;100%\&amp;quot;&amp;amp;gt;&amp;amp;lt;tr&amp;amp;gt;&amp;amp;lt;td&amp;amp;gt;&amp;#39;)[[1]][2]
  hand &amp;lt;- str_split(hand, &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]

  # Weapon Type ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;type...&amp;#39;))
  type &amp;lt;- str_split(WeaponDataText, &amp;#39;class=\&amp;quot;q1\&amp;quot;&amp;amp;gt;&amp;#39;)[[1]][2]
  type &amp;lt;- str_split(type, &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]
  
  # Weapon Damage Range (High + Low) ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i, &amp;#39;damage range...&amp;#39;))
  damageRange &amp;lt;- str_split(WeaponDataText, &amp;#39;!--dmg--&amp;amp;gt;&amp;#39;)[[1]][2]
  damageRange &amp;lt;- str_split(damageRange, &amp;#39; &amp;#39;)
  # Weapon Low Damage
  damageLow &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, damageRange[[1]][1]))
  # Weapon High Damage
  damageHigh &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, damageRange[[1]][3]))
  
  # Weapon Speed ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i, &amp;#39;speed...&amp;#39;))
  speed &amp;lt;- str_split(WeaponDataText, &amp;#39;!--spd--&amp;amp;gt;&amp;#39;)[[1]][2]
  speed &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_split(speed, &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]))
  
  # Weapon DPS ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i, &amp;#39;dps...&amp;#39;))
  dps &amp;lt;- str_split(WeaponDataText,&amp;#39;!--dps--&amp;amp;gt;\\(&amp;#39;)
  dps &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(dps[[1]][2], &amp;#39;damage&amp;#39;)[[1]][1])))
  
  # Weapon Or Option ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i, &amp;#39;Or Option...&amp;#39;))
  or &amp;lt;- str_split(WeaponDataText,&amp;#39;damage per second\\)&amp;amp;lt;br /&amp;amp;gt;&amp;amp;lt;span&amp;amp;gt;&amp;amp;lt;&amp;#39;)
  or &amp;lt;- str_split(dps[[1]][2], &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]
  if(!is.na(str_detect(or, &amp;#39;or&amp;#39;))){
    orOption &amp;lt;- 1
  }else{
    orOption &amp;lt;- NA
  }
  
  # Agility Test ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Agility level...&amp;#39;))
  weaponAgility &amp;lt;- str_split(WeaponDataText, &amp;#39;!--stat3--&amp;amp;gt;\\+&amp;#39;)[[1]][2]
  agility &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(weaponAgility, &amp;#39;Agility&amp;#39;)[[1]][1])))
  
  # Intellect Test ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Intellect level...&amp;#39;))
  weaponIntellect &amp;lt;- str_split(WeaponDataText, &amp;#39;!--stat5--&amp;amp;gt;\\+&amp;#39;)[[1]][2]
  intellect &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(weaponIntellect, &amp;#39;Intellect&amp;#39;)[[1]][1])))
  
  # Mastery Test ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Mastery level...&amp;#39;))
  weaponMastery &amp;lt;- str_split(WeaponDataText, &amp;#39;!--rtg49--&amp;amp;gt;&amp;#39;)[[1]][2]
  mastery &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(weaponMastery, &amp;#39;Mastery&amp;#39;)[[1]][1])))
  
  # Stamina Test ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Stanima level...&amp;#39;))
  weaponStamina &amp;lt;- str_split(WeaponDataText, &amp;#39;!--stat7--&amp;amp;gt;\\+&amp;#39;)[[1]][2]
  stamina &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(weaponStamina, &amp;#39;Stamina&amp;#39;)[[1]][1])))
  
  
  # Strength Test ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Stanima level...&amp;#39;))
  weaponStrength &amp;lt;- str_split(WeaponDataText, &amp;#39;!--stat4--&amp;amp;gt;\\+&amp;#39;)[[1]][2]
  strength &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(weaponStrength, &amp;#39;Strength&amp;#39;)[[1]][1])))
  
  # Critical Strike Test ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Critical Strike level...&amp;#39;))
  weaponCritStrike &amp;lt;- str_split(WeaponDataText, &amp;#39;!--rtg32--&amp;amp;gt;&amp;#39;)[[1]][2]
  critStrike &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(weaponCritStrike, &amp;#39;Critical&amp;#39;)[[1]][1])))
  
  # Durability Test ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Durability level...&amp;#39;))
  weaponDurability &amp;lt;- str_split(WeaponDataText, &amp;#39;Durability &amp;#39;)[[1]][2]
  durability &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_split(weaponDurability, &amp;#39; &amp;#39;)[[1]][1]))
  
  # Haste ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Haste level...&amp;#39;))
  weaponHaste &amp;lt;- str_split(WeaponDataText, &amp;#39;!--rtg36--&amp;amp;gt;&amp;#39;)[[1]][2]
  haste &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(weaponHaste, &amp;#39;Haste&amp;#39;)[[1]][1])))
  
  # Versatility ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Versatility level...&amp;#39;))
  weaponVersatility &amp;lt;- str_split(WeaponDataText, &amp;#39;!--rtg40--&amp;amp;gt;&amp;#39;)[[1]][2]
  versatility &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_trim(str_split(weaponVersatility, &amp;#39;Versatility&amp;#39;)[[1]][1])))
  
  # Classes ---- took out, unsure of best method
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i,&amp;#39;Class level...&amp;#39;))
  # weaponClasses &amp;lt;- str_split(WeaponDataText, &amp;#39;class=\&amp;quot;c7\&amp;quot;&amp;amp;gt;&amp;#39;)[[1]][2]
  # classes &amp;lt;- str_split(weaponClasses, &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]
  
  # Weapon Sell Price ----
  # print(paste(&amp;#39;selecting weapon&amp;#39;, i, &amp;#39;sell price...&amp;#39;))
  sellPriceGold &amp;lt;- str_split(WeaponDataText, &amp;#39;class=\&amp;quot;moneygold\&amp;quot;&amp;amp;gt;&amp;#39;)
  sellPriceGold &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_split(sellPriceGold[[1]][2], &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]))
  # Silver
  sellPriceSilver &amp;lt;- str_split(WeaponDataText, &amp;#39;class=\&amp;quot;moneysilver\&amp;quot;&amp;amp;gt;&amp;#39;)
  sellPriceSilver &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_split(sellPriceSilver[[1]][2], &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]))
  # Copper
  sellPriceCopper &amp;lt;- str_split(WeaponDataText, &amp;#39;class=\&amp;quot;moneycopper\&amp;quot;&amp;amp;gt;&amp;#39;)
  sellPriceCopper &amp;lt;- as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;&amp;#39;, str_split(sellPriceCopper[[1]][2], &amp;#39;&amp;amp;lt;&amp;#39;)[[1]][1]))
  
  # Weapon Patch / Expansion ----
  weaponPatch &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = paste(&amp;quot;/html/head/meta[6]&amp;quot;, sep = &amp;quot;&amp;quot;))
  patch &amp;lt;- weaponPatch$getElementAttribute(&amp;#39;content&amp;#39;)
  patch &amp;lt;- str_split(patch, &amp;#39;Item, &amp;#39;)[[1]][2]
  patch &amp;lt;- str_split(patch, &amp;#39;, &amp;#39;)
  patchNum &amp;lt;- patch[[1]][1]
  if(patch[[1]][2] == &amp;quot;Alliance&amp;quot; | patch[[1]][2] == &amp;quot;Horde&amp;quot;){
    side &amp;lt;- patch[[1]][2]
    expansion &amp;lt;- patch[[1]][3]
  }else{
    expansion &amp;lt;- patch[[1]][2]
    side &amp;lt;- NA
  }
  
  # Create Date Frame ----
  print(paste(&amp;#39;Creating data frame row&amp;#39;, j))
  if(!exists(&amp;#39;wowWeapons&amp;#39;)){
    wowWeapons &amp;lt;- data.frame(name, itemLevel, upgradeLvl, bind, hand, type, damageLow, damageHigh, speed, dps, orOption, agility, critStrike, haste, intellect, mastery, stamina, strength, versatility, durability, reqLvl, sellPriceGold, sellPriceSilver, sellPriceCopper, side, expansion, patchNum)
    colnames(wowWeapons)[1] &amp;lt;- &amp;#39;name&amp;#39;
    colnames(wowWeapons)[5] &amp;lt;- &amp;#39;hand&amp;#39;
    colnames(wowWeapons)[6] &amp;lt;- &amp;#39;type&amp;#39;
    
  }else{
    newRow &amp;lt;- data.frame(name, itemLevel, upgradeLvl, bind, hand, type, damageLow, damageHigh, speed, dps, orOption, agility, critStrike, haste, intellect, mastery, stamina, strength, versatility, durability, reqLvl, sellPriceGold, sellPriceSilver, sellPriceCopper, side, expansion, patchNum)
    names(newRow) &amp;lt;- names(wowWeapons)
    wowWeapons &amp;lt;- rbind(wowWeapons, newRow)
  }
  
  print(paste(&amp;#39;Weapon&amp;#39;, j, &amp;#39;complete! Going Back..&amp;#39;))
  timeVec[j] &amp;lt;- endTime - startTime
  remDr$goBack()

  # logic for going to next page
  if(i == 50){
    print(&amp;#39;Going to the next page&amp;#39;)
    nextPage &amp;lt;- remDr$findElement(using =&amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; a:nth-child(4)&amp;#39;)
    nextPage$clickElement()
    i &amp;lt;- 0
  }
  
  # filter automation start (up to rlvl 89) ----
  if(f == filterWeaponCount){
    print(&amp;#39;f == filterWeaponCount&amp;#39;)
    filterHighValue &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = &amp;#39;//*[@id=&amp;quot;filter-facet-max-req-level&amp;quot;]&amp;#39;)
    filterHighValue &amp;lt;- filterHighValue$getElementAttribute(&amp;#39;value&amp;#39;)
    
    # 26-55
    if(filterHighValue == &amp;#39;25&amp;#39;){
      print(&amp;#39;f == filterWeaponCount&amp;#39;)
      
      # test for filter automation
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;26&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;55&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      #56-65
    }else if(filterHighValue == &amp;#39;55&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;56&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;65&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      # 66 - 70
    }else if(filterHighValue == &amp;#39;65&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;66&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;70&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      # 71-79
    }else if(filterHighValue == &amp;#39;70&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;71&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;79&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      # 80-80
    }else if(filterHighValue == &amp;#39;79&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;80&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;80&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      #81-85
    }else if(filterHighValue == &amp;#39;80&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;81&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;85&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      #86-90
    }else if(filterHighValue == &amp;#39;85&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;86&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;89&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      # 91-99
    }else if(filterHighValue == &amp;#39;89&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;90&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;90&amp;#39;))
      
      # rlvl 90 filter 1
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(1)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(2)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(3)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(4)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(5)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(6)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(7)&amp;#39;)
      f90$clickElement()
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      rlvl90filter1 &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = &amp;#39;//*[@id=&amp;quot;tab-items&amp;quot;]/div[1]/div[1]/span/b[3]&amp;#39;)
      rlvl90filter1 &amp;lt;- rlvl90filter1$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      rlvl90filter1 &amp;lt;- as.numeric(rlvl90filter1)
      
      filterHighValue &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = &amp;#39;//*[@id=&amp;quot;filter-facet-max-req-level&amp;quot;]&amp;#39;)
      filterHighValue &amp;lt;- filterHighValue$getElementAttribute(&amp;#39;value&amp;#39;)
      
      
    }
    
    else if(rlvl90counter == rlvl90filter1){
      # clear filter 1 button
      clearfilterlist &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type-clear-link&amp;#39;)
      clearfilterlist$clickElement()
      
      # rlvl 90 filter 2
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(8)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(9)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(10)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(11)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(12)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(13)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(14)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(15)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(16)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(17)&amp;#39;)
      f90$clickElement()
      f90 &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type &amp;gt; option:nth-child(18)&amp;#39;)
      f90$clickElement()
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      # 100
    }else if(filterHighValue == &amp;#39;90&amp;#39;){
      clearfilterlist &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-type-clear-link&amp;#39;)
      clearfilterlist$clickElement()
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;91&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;99&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      # 100
    }
    
    else if(filterHighValue == &amp;#39;99&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;100&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;100&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
      # 101-110
    }else if(filterHighValue == &amp;#39;100&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;101&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;110&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
    }else if(filterHighValue == &amp;#39;110&amp;#39;){
      reqLvlLow &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-min-req-level&amp;#39;)
      reqLvlLow$clearElement()
      reqLvlLowText &amp;lt;- reqLvlLow$sendKeysToElement(list(&amp;#39;0&amp;#39;)) 
      reqLvlHigh &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#filter-facet-max-req-level&amp;#39;)
      reqLvlHigh$clearElement()
      reqLvlHighText &amp;lt;- reqLvlHigh$sendKeysToElement(list(&amp;#39;0&amp;#39;)) 
      
      # apply filter button
      applyFilter &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#fi &amp;gt; form &amp;gt; div.filter-row &amp;gt; button&amp;#39;)
      applyFilter$clickElement()
      sortByItem$clickElement()
      # filtered weapon count
      filterWeaponCount &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value = &amp;#39;#tab-items &amp;gt; div.listview-band-top &amp;gt; div.listview-nav &amp;gt; span &amp;gt; b:nth-child(3)&amp;#39;)
      filterWeaponCount &amp;lt;- filterWeaponCount$getElementAttribute(&amp;#39;innerHTML&amp;#39;)
      filterWeaponCount &amp;lt;- as.numeric(filterWeaponCount)
      f &amp;lt;- 0
      i &amp;lt;- 0
      
    }else if(filterHighValue == &amp;#39;0&amp;#39;){
      print(&amp;#39;testing breaks&amp;#39;)
      break
    }
  }
  # end filter logic 
  
  i &amp;lt;- i + 1
  j &amp;lt;- j + 1
  f &amp;lt;- f + 1
  if(filterHighValue == &amp;#39;90&amp;#39;){
    rlvl90counter &amp;lt;- rlvl90counter + 1
  }
  
}
finalTime &amp;lt;- Sys.time()
# Loop Finish ----&lt;/code&gt;&lt;/pre&gt;
&lt;!-- ```{r echo=FALSE} --&gt;
&lt;!-- blogdown::shortcode(&#39;googleAdsense&#39;) --&gt;
&lt;!-- ``` --&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
